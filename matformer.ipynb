{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS7VAVkD0UmjIr8fc87il4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayisharya/Ayisharya/blob/main/matformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nctZKIKHmV8",
        "outputId": "13f286d1-bf66-4699-f177-f1297d3c6be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install networkx\n",
        "!pip3 install pandas\n",
        "!pip3 install plotly\n",
        "!pip install pymatgen==2023.7.11\n",
        "!pip3 install alignn\n",
        "!pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html\n",
        "!pip3 install pytz==2021.1\n",
        "!pip3 install scipy==1.7.1\n",
        "!pip3 install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip3 install jarvis-tools==2022.9.16\n",
        "!pip3 install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip3 install torch_geometric\n",
        "!pip install gensim\n",
        "!pip install periodictable\n",
        "!pip install pandarallel==1.6.3\n",
        "!pip install pytorch-ignite\n",
        "\n",
        "!pip install pydantic==1.10.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pydantic\n",
        "print(f\"Pydantic: {pydantic.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jviw1SCIE84",
        "outputId": "0a8fceeb-d812-4712-f765-a5d52ea0f434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pydantic: 1.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YKQ98/Matformer.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY4WXM5aKDBS",
        "outputId": "191f6788-582b-4d44-fb71-bd176a994ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Matformer' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "t16Bqtm-KHzv",
        "outputId": "25236e98-6915-4dde-a1bd-88c6bc55251c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Matformer/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7Jaoh-tKosf",
        "outputId": "0c5fc835-e20b-483d-a8f1-240a93aa588e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Matformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sJOvP0odK7nZ",
        "outputId": "87a5a049-0bd2-4f18-e24e-ad3204f6c7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Matformer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Matformer.matformer.models import pyg_att"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AGEtfBOK_AW",
        "outputId": "efec4880-4550-467d-ba11-280fe4a8f5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/ALIGNN\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "from jarvis.db.figshare import data as jdata\n",
        "from jarvis.core.atoms import Atoms\n",
        "from jarvis.core.graphs import Graph\n",
        "import dgl\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.metrics import MeanSquaredError\n",
        "\n",
        "# Import Matformer components\n",
        "from Matformer.matformer.models.pyg_att import Matformer, MatformerConfig\n",
        "\n",
        "# -------------------------\n",
        "# Data Preparation Functions\n",
        "# -------------------------\n",
        "def collate_batch(batch):\n",
        "    \"\"\"Custom collate function for Matformer input\"\"\"\n",
        "    graphs = []\n",
        "    line_graphs = []\n",
        "    lattices = []\n",
        "    labels = []\n",
        "    for item in batch:\n",
        "        graphs.append(item[0][0])\n",
        "        line_graphs.append(item[0][1])\n",
        "        lattices.append(item[0][2])\n",
        "        labels.append(item[1])\n",
        "    batched_graphs = dgl.batch(graphs)\n",
        "    batched_line_graphs = dgl.batch(line_graphs)\n",
        "    batched_lattices = torch.stack(lattices)\n",
        "    labels = torch.stack(labels)\n",
        "    return (batched_graphs, batched_line_graphs, batched_lattices), labels\n",
        "\n",
        "def load_dataset(name=\"dft_3d\", target=\"formation_energy_peratom\", limit=None):\n",
        "    \"\"\"Load dataset from JARVIS\"\"\"\n",
        "    d = jdata(name)\n",
        "    data = []\n",
        "    for i in d:\n",
        "        if target in i and i[target] != \"na\" and not math.isnan(i[target]):\n",
        "            data.append(i)\n",
        "    if limit is not None:\n",
        "        data = data[:limit]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def create_graphs(df, target):\n",
        "    \"\"\"Create DGL graphs from atomic structures\"\"\"\n",
        "    data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        try:\n",
        "            atoms = Atoms.from_dict(row[\"atoms\"])\n",
        "            graph, line_graph = Graph.atom_dgl_multigraph(atoms)\n",
        "            lattice = torch.tensor(atoms.lattice_mat, dtype=torch.float)\n",
        "            label = torch.tensor([row[target]], dtype=torch.float)\n",
        "            data.append(((graph, line_graph, lattice), label))\n",
        "        except Exception as e:\n",
        "            print(\"Skipping graph due to error:\", e)\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "# -------------------------\n",
        "# Model Configuration\n",
        "# -------------------------\n",
        "def load_matformer_config():\n",
        "    \"\"\"Load Matformer configuration similar to pyg_att.py implementation\"\"\"\n",
        "    config = {\n",
        "        \"name\": \"matformer\",\n",
        "        \"node_dim\": 64,\n",
        "        \"edge_dim\": 64,\n",
        "        \"output_dim\": 1,\n",
        "        \"link\": \"identity\",\n",
        "        \"n_heads\": 4,\n",
        "        \"n_layers\": 3,\n",
        "        \"dropout\": 0.0,\n",
        "        \"attention_dropout\": 0.0,\n",
        "        \"use_bn\": False,\n",
        "        \"use_ffn\": False,\n",
        "        \"use_ffn_output\": True,\n",
        "        \"use_residual\": True,\n",
        "        \"use_attention_output\": True,\n",
        "        \"hidden_dim\": 128,\n",
        "        \"graph_norm\": True,\n",
        "        \"edge_embed\": False,\n",
        "        \"use_edge_attr\": False,\n",
        "        \"pos_embed\": False,\n",
        "        \"use_global_pool\": True,\n",
        "        \"global_pool\": \"mean\",\n",
        "        \"ffn_act\": \"relu\",\n",
        "        \"ffn_norm\": \"batch\",\n",
        "        \"attn_type\": \"multihead\",\n",
        "        \"attn_kernel\": \"softmax\",\n",
        "        \"attn_norm\": \"layer\",\n",
        "        \"attn_act\": \"relu\",\n",
        "        \"attn_mask\": False,\n",
        "        \"attn_dropout\": 0.0,\n",
        "        \"attn_residual\": True,\n",
        "        \"attn_bias\": True,\n",
        "        \"attn_ffn\": False,\n",
        "        \"attn_ffn_act\": \"relu\",\n",
        "        \"attn_ffn_norm\": \"batch\",\n",
        "        \"attn_ffn_dropout\": 0.0,\n",
        "        \"attn_ffn_residual\": True,\n",
        "        \"attn_ffn_bias\": True,\n",
        "    }\n",
        "    return MatformerConfig(**config)\n",
        "\n",
        "# -------------------------\n",
        "# Main Training Setup\n",
        "# -------------------------\n",
        "# Configuration\n",
        "dataset_name = \"dft_3d\"\n",
        "target_property = \"formation_energy_peratom\"\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load and prepare data\n",
        "print(\"Loading dataset...\")\n",
        "df = load_dataset(name=dataset_name, target=target_property, limit=1000)\n",
        "print(f\"Loaded {len(df)} samples.\")\n",
        "\n",
        "data = create_graphs(df, target_property)\n",
        "print(f\"Graphs created: {len(data)}\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(data))\n",
        "val_size = int(0.1 * len(data))\n",
        "test_size = len(data) - train_size - val_size\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size + val_size]\n",
        "test_data = data[train_size + val_size:]\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Initialize Matformer model\n",
        "config = load_matformer_config()\n",
        "model = Matformer(config).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# -------------------------\n",
        "# Training Functions\n",
        "# -------------------------\n",
        "def train_step(engine, batch):\n",
        "    \"\"\"Training step function\"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    (g, lg, lat), labels = batch\n",
        "    g, lg, lat, labels = g.to(device), lg.to(device), lat.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass with Matformer model\n",
        "    outputs = model(g, lg, lat).view(-1)\n",
        "    loss = criterion(outputs, labels.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def val_step(engine, batch):\n",
        "    \"\"\"Validation step function\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        (g, lg, lat), labels = batch\n",
        "        g, lg, lat, labels = g.to(device), lg.to(device), lat.to(device), labels.to(device)\n",
        "        outputs = model(g, lg, lat).view(-1)\n",
        "        return outputs, labels.view(-1)\n",
        "\n",
        "# Create engines\n",
        "trainer = Engine(train_step)\n",
        "evaluator = Engine(val_step)\n",
        "MeanSquaredError().attach(evaluator, \"mse\")\n",
        "\n",
        "# -------------------------\n",
        "# Logging and Checkpointing\n",
        "# -------------------------\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_validation_results(engine):\n",
        "    \"\"\"Log validation metrics after each epoch\"\"\"\n",
        "    evaluator.run(val_loader)\n",
        "    metrics = evaluator.state.metrics\n",
        "    print(f\"Validation Results - Epoch: {engine.state.epoch}  MSE: {metrics['mse']:.4f}\")\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def save_checkpoint(engine):\n",
        "    \"\"\"Save model checkpoint after each epoch\"\"\"\n",
        "    epoch = engine.state.epoch\n",
        "    checkpoint_path = f\"checkpoints/matformer_model_{epoch}.pt\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'config': config,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "# -------------------------\n",
        "# Run Training\n",
        "# -------------------------\n",
        "print(\"Starting training...\")\n",
        "trainer.run(train_loader, max_epochs=epochs)\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2ERxkpulLY_5",
        "outputId": "b3b319fa-0f73-4a5d-f50b-462cd34b3a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Obtaining 3D dataset 55k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Loading the zipfile...\n",
            "Loading completed.\n",
            "Loaded 1000 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:33<00:00, 29.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graphs created: 1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "35 validation errors for MatformerConfig\nattention_dropout\n  extra fields not permitted (type=value_error.extra)\nattn_act\n  extra fields not permitted (type=value_error.extra)\nattn_bias\n  extra fields not permitted (type=value_error.extra)\nattn_dropout\n  extra fields not permitted (type=value_error.extra)\nattn_ffn\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_act\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_bias\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_dropout\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_norm\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_residual\n  extra fields not permitted (type=value_error.extra)\nattn_kernel\n  extra fields not permitted (type=value_error.extra)\nattn_mask\n  extra fields not permitted (type=value_error.extra)\nattn_norm\n  extra fields not permitted (type=value_error.extra)\nattn_residual\n  extra fields not permitted (type=value_error.extra)\nattn_type\n  extra fields not permitted (type=value_error.extra)\ndropout\n  extra fields not permitted (type=value_error.extra)\nedge_dim\n  extra fields not permitted (type=value_error.extra)\nedge_embed\n  extra fields not permitted (type=value_error.extra)\nffn_act\n  extra fields not permitted (type=value_error.extra)\nffn_norm\n  extra fields not permitted (type=value_error.extra)\nglobal_pool\n  extra fields not permitted (type=value_error.extra)\ngraph_norm\n  extra fields not permitted (type=value_error.extra)\nhidden_dim\n  extra fields not permitted (type=value_error.extra)\nn_heads\n  extra fields not permitted (type=value_error.extra)\nn_layers\n  extra fields not permitted (type=value_error.extra)\nnode_dim\n  extra fields not permitted (type=value_error.extra)\noutput_dim\n  extra fields not permitted (type=value_error.extra)\npos_embed\n  extra fields not permitted (type=value_error.extra)\nuse_attention_output\n  extra fields not permitted (type=value_error.extra)\nuse_bn\n  extra fields not permitted (type=value_error.extra)\nuse_edge_attr\n  extra fields not permitted (type=value_error.extra)\nuse_ffn\n  extra fields not permitted (type=value_error.extra)\nuse_ffn_output\n  extra fields not permitted (type=value_error.extra)\nuse_global_pool\n  extra fields not permitted (type=value_error.extra)\nuse_residual\n  extra fields not permitted (type=value_error.extra)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5b6870397011>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# Initialize Matformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_matformer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-5b6870397011>\u001b[0m in \u001b[0;36mload_matformer_config\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;34m\"attn_ffn_bias\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     }\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMatformerConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# -------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/env_settings.cpython-311-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.env_settings.BaseSettings.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.cpython-311-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 35 validation errors for MatformerConfig\nattention_dropout\n  extra fields not permitted (type=value_error.extra)\nattn_act\n  extra fields not permitted (type=value_error.extra)\nattn_bias\n  extra fields not permitted (type=value_error.extra)\nattn_dropout\n  extra fields not permitted (type=value_error.extra)\nattn_ffn\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_act\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_bias\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_dropout\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_norm\n  extra fields not permitted (type=value_error.extra)\nattn_ffn_residual\n  extra fields not permitted (type=value_error.extra)\nattn_kernel\n  extra fields not permitted (type=value_error.extra)\nattn_mask\n  extra fields not permitted (type=value_error.extra)\nattn_norm\n  extra fields not permitted (type=value_error.extra)\nattn_residual\n  extra fields not permitted (type=value_error.extra)\nattn_type\n  extra fields not permitted (type=value_error.extra)\ndropout\n  extra fields not permitted (type=value_error.extra)\nedge_dim\n  extra fields not permitted (type=value_error.extra)\nedge_embed\n  extra fields not permitted (type=value_error.extra)\nffn_act\n  extra fields not permitted (type=value_error.extra)\nffn_norm\n  extra fields not permitted (type=value_error.extra)\nglobal_pool\n  extra fields not permitted (type=value_error.extra)\ngraph_norm\n  extra fields not permitted (type=value_error.extra)\nhidden_dim\n  extra fields not permitted (type=value_error.extra)\nn_heads\n  extra fields not permitted (type=value_error.extra)\nn_layers\n  extra fields not permitted (type=value_error.extra)\nnode_dim\n  extra fields not permitted (type=value_error.extra)\noutput_dim\n  extra fields not permitted (type=value_error.extra)\npos_embed\n  extra fields not permitted (type=value_error.extra)\nuse_attention_output\n  extra fields not permitted (type=value_error.extra)\nuse_bn\n  extra fields not permitted (type=value_error.extra)\nuse_edge_attr\n  extra fields not permitted (type=value_error.extra)\nuse_ffn\n  extra fields not permitted (type=value_error.extra)\nuse_ffn_output\n  extra fields not permitted (type=value_error.extra)\nuse_global_pool\n  extra fields not permitted (type=value_error.extra)\nuse_residual\n  extra fields not permitted (type=value_error.extra)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DGLMatformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(DGLMatformer, self).__init__()\n",
        "        self.node_dim = config[\"node_features\"]\n",
        "        self.edge_dim = config[\"edge_features\"]\n",
        "        self.output_dim = config[\"output_features\"]\n",
        "\n",
        "        # Simple linear layers to embed nodes and edges\n",
        "        self.node_emb = nn.Linear(1, self.node_dim)\n",
        "        self.edge_emb = nn.Linear(1, self.edge_dim)\n",
        "\n",
        "        # GNN Layers (simplified Transformer-like blocks)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=self.node_dim,\n",
        "                nhead=config[\"node_layer_head\"],\n",
        "                dim_feedforward=self.node_dim*2,\n",
        "                batch_first=True\n",
        "            ) for _ in range(config[\"conv_layers\"])\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(self.node_dim + 9, self.node_dim),  # 9 for flattened 3x3 lattice\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.node_dim, self.output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        g, lg, lattice = inputs  # lg is unused but passed for compatibility\n",
        "\n",
        "        # Node and edge features\n",
        "        h = self.node_emb(g.ndata['atom_features'].float())\n",
        "        e = self.edge_emb(g.edata['edge_features'].float())\n",
        "\n",
        "        # Assign to graph\n",
        "        g.ndata['h'] = h\n",
        "        g.edata['e'] = e\n",
        "\n",
        "        # Aggregate node features with mean (can be replaced with attention or message passing)\n",
        "        hg = dgl.mean_nodes(g, 'h')\n",
        "\n",
        "        # Flatten lattice and concatenate\n",
        "        flat_lat = lattice.view(lattice.size(0), -1)\n",
        "        out = torch.cat([hg, flat_lat], dim=1)\n",
        "\n",
        "        return self.output_layer(out)\n"
      ],
      "metadata": {
        "id": "Ar8xdtgPbLo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/ALIGNN\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "\n",
        "from jarvis.db.figshare import data as jdata\n",
        "from jarvis.core.atoms import Atoms\n",
        "from jarvis.core.graphs import Graph\n",
        "from Matformer.matformer.models.pyg_att import MatformerConfig  # For config template\n",
        "import dgl\n",
        "import numpy as np\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.metrics import MeanSquaredError\n",
        "\n",
        "# -------------------------\n",
        "# DGLMatformer Model Definition\n",
        "# -------------------------\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DGLMatformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(DGLMatformer, self).__init__()\n",
        "        self.node_dim = config[\"node_features\"]\n",
        "        self.edge_dim = config[\"edge_features\"]\n",
        "        self.output_dim = config[\"output_features\"]\n",
        "\n",
        "        self.node_emb = nn.Linear(1, self.node_dim)\n",
        "        self.edge_emb = nn.Linear(1, self.edge_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=self.node_dim,\n",
        "                nhead=config[\"node_layer_head\"],\n",
        "                dim_feedforward=self.node_dim * 2,\n",
        "                batch_first=True\n",
        "            ) for _ in range(config[\"conv_layers\"])\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(self.node_dim + 9, self.node_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.node_dim, self.output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        g, lg, lattice = inputs\n",
        "\n",
        "        h = self.node_emb(g.ndata['atom_features'].float())\n",
        "        e = self.edge_emb(g.edata['edge_features'].float())\n",
        "\n",
        "        g.ndata['h'] = h\n",
        "        g.edata['e'] = e\n",
        "\n",
        "        hg = dgl.mean_nodes(g, 'h')\n",
        "        flat_lat = lattice.view(lattice.size(0), -1)\n",
        "        out = torch.cat([hg, flat_lat], dim=1)\n",
        "\n",
        "        return self.output_layer(out)\n",
        "\n",
        "# -------------------------\n",
        "# Data Preparation\n",
        "# -------------------------\n",
        "def collate_batch(batch):\n",
        "    graphs, line_graphs, lattices, labels = [], [], [], []\n",
        "    for item in batch:\n",
        "        graphs.append(item[0][0])\n",
        "        line_graphs.append(item[0][1])\n",
        "        lattices.append(item[0][2])\n",
        "        labels.append(item[1])\n",
        "    return (\n",
        "        dgl.batch(graphs),\n",
        "        dgl.batch(line_graphs),\n",
        "        torch.stack(lattices),\n",
        "    ), torch.stack(labels)\n",
        "\n",
        "def load_dataset(name=\"dft_3d\", target=\"formation_energy_peratom\", limit=None):\n",
        "    d = jdata(name)\n",
        "    data = []\n",
        "    for i in d:\n",
        "        if target in i and i[target] != \"na\" and not math.isnan(i[target]):\n",
        "            data.append(i)\n",
        "    if limit:\n",
        "        data = data[:limit]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def create_graphs(df, target):\n",
        "    data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        try:\n",
        "            atoms = Atoms.from_dict(row[\"atoms\"])\n",
        "            graph, line_graph = Graph.atom_dgl_multigraph(atoms)\n",
        "\n",
        "            graph.ndata['atom_features'] = torch.tensor(atoms.atomic_numbers, dtype=torch.long).unsqueeze(-1)\n",
        "\n",
        "            if 'pos' not in graph.ndata:\n",
        "                graph.ndata['pos'] = torch.zeros(graph.num_nodes(), 3)\n",
        "\n",
        "            if 'bond_dist' not in graph.edata:\n",
        "                src, dst = graph.edges()\n",
        "                src_pos = graph.ndata['pos'][src]\n",
        "                dst_pos = graph.ndata['pos'][dst]\n",
        "                dist = torch.norm(src_pos - dst_pos, dim=1)\n",
        "                graph.edata['bond_dist'] = dist\n",
        "\n",
        "            graph.edata['edge_features'] = graph.edata['bond_dist'].float().unsqueeze(-1)\n",
        "            lattice = torch.tensor(atoms.lattice_mat, dtype=torch.float)\n",
        "            label = torch.tensor([row[target]], dtype=torch.float)\n",
        "\n",
        "            data.append(((graph, line_graph, lattice), label))\n",
        "        except Exception as e:\n",
        "            print(\"Skipping sample due to error:\", e)\n",
        "    return data\n",
        "\n",
        "# -------------------------\n",
        "# Main Training Setup\n",
        "# -------------------------\n",
        "dataset_name = \"dft_3d\"\n",
        "target_property = \"formation_energy_peratom\"\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "df = load_dataset(name=dataset_name, target=target_property, limit=1000)\n",
        "print(f\"Loaded {len(df)} samples.\")\n",
        "\n",
        "data = create_graphs(df, target_property)\n",
        "print(f\"Graphs created: {len(data)}\")\n",
        "if len(data) == 0:\n",
        "    raise ValueError(\"No valid graphs created.\")\n",
        "\n",
        "train_size = int(0.8 * len(data))\n",
        "val_size = int(0.1 * len(data))\n",
        "test_size = len(data) - train_size - val_size\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size + val_size]\n",
        "test_data = data[train_size + val_size:]\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# -------------------------\n",
        "# Model & Training Engine\n",
        "# -------------------------\n",
        "config_dict = {\n",
        "    \"name\": \"matformer\",\n",
        "    \"node_features\": 128,\n",
        "    \"edge_features\": 128,\n",
        "    \"output_features\": 1,\n",
        "    \"conv_layers\": 5,\n",
        "    \"node_layer_head\": 4,\n",
        "}\n",
        "\n",
        "model = DGLMatformer(config_dict).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "def train_step(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    (g, lg, lat), labels = batch\n",
        "    g, lg, lat, labels = g.to(device), lg.to(device), lat.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model((g, lg, lat)).view(-1)\n",
        "    loss = criterion(outputs, labels.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def val_step(engine, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        (g, lg, lat), labels = batch\n",
        "        g, lg, lat, labels = g.to(device), lg.to(device), lat.to(device), labels.to(device)\n",
        "        outputs = model((g, lg, lat)).view(-1)\n",
        "        return outputs, labels.view(-1)\n",
        "\n",
        "trainer = Engine(train_step)\n",
        "evaluator = Engine(val_step)\n",
        "MeanSquaredError().attach(evaluator, \"mse\")\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_validation_results(engine):\n",
        "    evaluator.run(val_loader)\n",
        "    metrics = evaluator.state.metrics\n",
        "    print(f\"Validation - Epoch {engine.state.epoch} | MSE: {metrics['mse']:.4f}\")\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def save_checkpoint(engine):\n",
        "    epoch = engine.state.epoch\n",
        "    path = f\"checkpoints/matformer_model_{epoch}.pt\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'config': config_dict,\n",
        "    }, path)\n",
        "    print(f\"Saved checkpoint to {path}\")\n",
        "\n",
        "# -------------------------\n",
        "# Train\n",
        "# -------------------------\n",
        "print(\"Starting training...\")\n",
        "trainer.run(train_loader, max_epochs=epochs)\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "029gGFz6LtZc",
        "outputId": "c943b288-e65f-4e3e-f2ea-8808db55f994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Obtaining 3D dataset 55k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Loading the zipfile...\n",
            "Loading completed.\n",
            "Loaded 1000 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:36<00:00, 27.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graphs created: 1000\n",
            "Starting training...\n",
            "Validation - Epoch 1 | MSE: 1.1486\n",
            "Saved checkpoint to checkpoints/matformer_model_1.pt\n",
            "Validation - Epoch 2 | MSE: 1.1309\n",
            "Saved checkpoint to checkpoints/matformer_model_2.pt\n",
            "Validation - Epoch 3 | MSE: 0.9725\n",
            "Saved checkpoint to checkpoints/matformer_model_3.pt\n",
            "Validation - Epoch 4 | MSE: 1.0108\n",
            "Saved checkpoint to checkpoints/matformer_model_4.pt\n",
            "Validation - Epoch 5 | MSE: 0.9432\n",
            "Saved checkpoint to checkpoints/matformer_model_5.pt\n",
            "Validation - Epoch 6 | MSE: 1.0081\n",
            "Saved checkpoint to checkpoints/matformer_model_6.pt\n",
            "Validation - Epoch 7 | MSE: 1.5548\n",
            "Saved checkpoint to checkpoints/matformer_model_7.pt\n",
            "Validation - Epoch 8 | MSE: 0.8910\n",
            "Saved checkpoint to checkpoints/matformer_model_8.pt\n",
            "Validation - Epoch 9 | MSE: 0.8671\n",
            "Saved checkpoint to checkpoints/matformer_model_9.pt\n",
            "Validation - Epoch 10 | MSE: 0.8624\n",
            "Saved checkpoint to checkpoints/matformer_model_10.pt\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
        "\n",
        "import imp\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "# from typing import Dict, List, Optional, Set, Tuple\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from jarvis.core.atoms import Atoms\n",
        "from matformer.graphs import PygGraph, PygStructureDataset\n",
        "#\n",
        "from jarvis.db.figshare import data as jdata\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from jarvis.db.jsonutils import dumpjson\n",
        "\n",
        "# from sklearn.pipeline import Pipeline\n",
        "import pickle as pk\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# use pandas progress_apply\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "def load_dataset(\n",
        "    name: str = \"dft_3d\",\n",
        "    target=None,\n",
        "    limit: Optional[int] = None,\n",
        "    classification_threshold: Optional[float] = None,\n",
        "):\n",
        "    \"\"\"Load jarvis data.\"\"\"\n",
        "    d = jdata(name)\n",
        "    data = []\n",
        "    for i in d:\n",
        "        if i[target] != \"na\" and not math.isnan(i[target]):\n",
        "            if classification_threshold is not None:\n",
        "                if i[target] <= classification_threshold:\n",
        "                    i[target] = 0\n",
        "                elif i[target] > classification_threshold:\n",
        "                    i[target] = 1\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        \"Check classification data type.\",\n",
        "                        i[target],\n",
        "                        type(i[target]),\n",
        "                    )\n",
        "            data.append(i)\n",
        "    d = data\n",
        "    if limit is not None:\n",
        "        d = d[:limit]\n",
        "    d = pd.DataFrame(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "def mean_absolute_deviation(data, axis=None):\n",
        "    \"\"\"Get Mean absolute deviation.\"\"\"\n",
        "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
        "\n",
        "\n",
        "\n",
        "def load_pyg_graphs(\n",
        "    df: pd.DataFrame,\n",
        "    name: str = \"dft_3d\",\n",
        "    neighbor_strategy: str = \"k-nearest\",\n",
        "    cutoff: float = 8,\n",
        "    max_neighbors: int = 12,\n",
        "    cachedir: Optional[Path] = None,\n",
        "    use_canonize: bool = False,\n",
        "    use_lattice: bool = False,\n",
        "    use_angle: bool = False,\n",
        "):\n",
        "    \"\"\"Construct crystal graphs.\n",
        "\n",
        "    Load only atomic number node features\n",
        "    and bond displacement vector edge features.\n",
        "\n",
        "    Resulting graphs have scheme e.g.\n",
        "    ```\n",
        "    Graph(num_nodes=12, num_edges=156,\n",
        "          ndata_schemes={'atom_features': Scheme(shape=(1,)}\n",
        "          edata_schemes={'r': Scheme(shape=(3,)})\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def atoms_to_graph(atoms):\n",
        "        \"\"\"Convert structure dict to DGLGraph.\"\"\"\n",
        "        structure = Atoms.from_dict(atoms)\n",
        "        return PygGraph.atom_dgl_multigraph(\n",
        "            structure,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            cutoff=cutoff,\n",
        "            atom_features=\"atomic_number\",\n",
        "            max_neighbors=max_neighbors,\n",
        "            compute_line_graph=False,\n",
        "            use_canonize=use_canonize,\n",
        "            use_lattice=use_lattice,\n",
        "            use_angle=use_angle,\n",
        "        )\n",
        "\n",
        "    graphs = df[\"atoms\"].progress_apply(atoms_to_graph).values\n",
        "\n",
        "    return graphs\n",
        "\n",
        "\n",
        "def get_id_train_val_test(\n",
        "    total_size=1000,\n",
        "    split_seed=123,\n",
        "    train_ratio=None,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    n_train=None,\n",
        "    n_test=None,\n",
        "    n_val=None,\n",
        "    keep_data_order=False,\n",
        "):\n",
        "    \"\"\"Get train, val, test IDs.\"\"\"\n",
        "    if (\n",
        "        train_ratio is None\n",
        "        and val_ratio is not None\n",
        "        and test_ratio is not None\n",
        "    ):\n",
        "        if train_ratio is None:\n",
        "            assert val_ratio + test_ratio < 1\n",
        "            train_ratio = 1 - val_ratio - test_ratio\n",
        "            print(\"Using rest of the dataset except the test and val sets.\")\n",
        "        else:\n",
        "            assert train_ratio + val_ratio + test_ratio <= 1\n",
        "    # indices = list(range(total_size))\n",
        "    if n_train is None:\n",
        "        n_train = int(train_ratio * total_size)\n",
        "    if n_test is None:\n",
        "        n_test = int(test_ratio * total_size)\n",
        "    if n_val is None:\n",
        "        n_val = int(val_ratio * total_size)\n",
        "    ids = list(np.arange(total_size))\n",
        "    if not keep_data_order:\n",
        "        random.seed(split_seed)\n",
        "        random.shuffle(ids)\n",
        "    if n_train + n_val + n_test > total_size:\n",
        "        raise ValueError(\n",
        "            \"Check total number of samples.\",\n",
        "            n_train + n_val + n_test,\n",
        "            \">\",\n",
        "            total_size,\n",
        "        )\n",
        "\n",
        "    id_train = ids[:n_train]\n",
        "    id_val = ids[-(n_val + n_test) : -n_test]  # noqa:E203\n",
        "    id_test = ids[-n_test:]\n",
        "    return id_train, id_val, id_test\n",
        "\n",
        "\n",
        "def get_torch_dataset(\n",
        "    dataset=[],\n",
        "    id_tag=\"jid\",\n",
        "    target=\"\",\n",
        "    neighbor_strategy=\"\",\n",
        "    atom_features=\"\",\n",
        "    use_canonize=\"\",\n",
        "    name=\"\",\n",
        "    line_graph=\"\",\n",
        "    cutoff=8.0,\n",
        "    max_neighbors=12,\n",
        "    classification=False,\n",
        "    output_dir=\".\",\n",
        "    tmp_name=\"dataset\",\n",
        "):\n",
        "    \"\"\"Get Torch Dataset.\"\"\"\n",
        "    df = pd.DataFrame(dataset)\n",
        "    # print(\"df\", df)\n",
        "    vals = df[target].values\n",
        "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
        "        val_list = [vals[i].item() for i in range(len(vals))]\n",
        "        vals = val_list\n",
        "    print(\"data range\", np.max(vals), np.min(vals))\n",
        "    print(\"data mean and std\", np.mean(vals), np.std(vals))\n",
        "    f = open(os.path.join(output_dir, tmp_name + \"_data_range\"), \"w\")\n",
        "    line = \"Max=\" + str(np.max(vals)) + \"\\n\"\n",
        "    f.write(line)\n",
        "    line = \"Min=\" + str(np.min(vals)) + \"\\n\"\n",
        "    f.write(line)\n",
        "    f.close()\n",
        "\n",
        "    graphs = load_graphs(\n",
        "        df,\n",
        "        name=name,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "    )\n",
        "\n",
        "    data = StructureDataset(\n",
        "        df,\n",
        "        graphs,\n",
        "        target=target,\n",
        "        atom_features=atom_features,\n",
        "        line_graph=line_graph,\n",
        "        id_tag=id_tag,\n",
        "        classification=classification,\n",
        "    )\n",
        "    return data\n",
        "\n",
        "def get_pyg_dataset(\n",
        "    dataset=[],\n",
        "    id_tag=\"jid\",\n",
        "    target=\"\",\n",
        "    neighbor_strategy=\"\",\n",
        "    atom_features=\"\",\n",
        "    use_canonize=\"\",\n",
        "    name=\"\",\n",
        "    line_graph=\"\",\n",
        "    cutoff=8.0,\n",
        "    max_neighbors=12,\n",
        "    classification=False,\n",
        "    output_dir=\".\",\n",
        "    tmp_name=\"dataset\",\n",
        "    use_lattice=False,\n",
        "    use_angle=False,\n",
        "    data_from='Jarvis',\n",
        "    use_save=False,\n",
        "    mean_train=None,\n",
        "    std_train=None,\n",
        "    now=False, # for test\n",
        "):\n",
        "    \"\"\"Get pyg Dataset.\"\"\"\n",
        "    df = pd.DataFrame(dataset)\n",
        "    # print(\"df\", df)\n",
        "    # neighbor_strategy = \"pairwise-k-nearest\"\n",
        "\n",
        "    vals = df[target].values\n",
        "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
        "        val_list = [vals[i].item() for i in range(len(vals))]\n",
        "        vals = val_list\n",
        "    output_dir = \"./saved_data/\" + tmp_name + \"test_graph_angle.pkl\" # for fast test use\n",
        "    print(\"data range\", np.max(vals), np.min(vals))\n",
        "    print(output_dir)\n",
        "    if now:\n",
        "        if not os.path.exists(output_dir):\n",
        "            graphs = load_pyg_graphs(\n",
        "                df,\n",
        "                name=name,\n",
        "                neighbor_strategy=neighbor_strategy,\n",
        "                use_canonize=use_canonize,\n",
        "                cutoff=cutoff,\n",
        "                max_neighbors=max_neighbors,\n",
        "                use_lattice=use_lattice,\n",
        "                use_angle=use_angle,\n",
        "            )\n",
        "            with open(output_dir, 'wb') as pf:\n",
        "                pk.dump(graphs, pf)\n",
        "            print('save graphs to ', output_dir)\n",
        "        else:\n",
        "            print('loading graphs from ', output_dir)\n",
        "            with open(output_dir, 'rb') as pf:\n",
        "                graphs = pk.load(pf)\n",
        "    else:\n",
        "        print('graphs not saved')\n",
        "        graphs = load_pyg_graphs(\n",
        "            df,\n",
        "            name=name,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            use_canonize=use_canonize,\n",
        "            cutoff=cutoff,\n",
        "            max_neighbors=max_neighbors,\n",
        "            use_lattice=use_lattice,\n",
        "            use_angle=use_angle,\n",
        "        )\n",
        "    if mean_train == None:\n",
        "        mean_train = np.mean(vals)\n",
        "        std_train = np.std(vals)\n",
        "        data = PygStructureDataset(\n",
        "            df,\n",
        "            graphs,\n",
        "            target=target,\n",
        "            atom_features=atom_features,\n",
        "            line_graph=line_graph,\n",
        "            id_tag=id_tag,\n",
        "            classification=classification,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            mean_train=mean_train,\n",
        "            std_train=std_train,\n",
        "        )\n",
        "    else:\n",
        "        data = PygStructureDataset(\n",
        "            df,\n",
        "            graphs,\n",
        "            target=target,\n",
        "            atom_features=atom_features,\n",
        "            line_graph=line_graph,\n",
        "            id_tag=id_tag,\n",
        "            classification=classification,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            mean_train=mean_train,\n",
        "            std_train=std_train,\n",
        "        )\n",
        "    return data, mean_train, std_train\n",
        "\n",
        "\n",
        "def get_train_val_loaders(\n",
        "    dataset: str = \"dft_3d\",\n",
        "    dataset_array=[],\n",
        "    target: str = \"formation_energy_peratom\",\n",
        "    atom_features: str = \"cgcnn\",\n",
        "    neighbor_strategy: str = \"k-nearest\",\n",
        "    n_train=None,\n",
        "    n_val=None,\n",
        "    n_test=None,\n",
        "    train_ratio=None,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    batch_size: int = 5,\n",
        "    standardize: bool = False,\n",
        "    line_graph: bool = True,\n",
        "    split_seed: int = 123,\n",
        "    workers: int = 0,\n",
        "    pin_memory: bool = True,\n",
        "    save_dataloader: bool = False,\n",
        "    filename: str = \"sample\",\n",
        "    id_tag: str = \"jid\",\n",
        "    use_canonize: bool = False,\n",
        "    cutoff: float = 8.0,\n",
        "    max_neighbors: int = 12,\n",
        "    classification_threshold: Optional[float] = None,\n",
        "    target_multiplication_factor: Optional[float] = None,\n",
        "    standard_scalar_and_pca=False,\n",
        "    keep_data_order=False,\n",
        "    output_features=1,\n",
        "    output_dir=None,\n",
        "    matrix_input=False,\n",
        "    pyg_input=False,\n",
        "    use_lattice=False,\n",
        "    use_angle=False,\n",
        "    use_save=True,\n",
        "    mp_id_list=None,\n",
        "):\n",
        "    \"\"\"Help function to set up JARVIS train and val dataloaders.\"\"\"\n",
        "    # data loading\n",
        "    mean_train=None\n",
        "    std_train=None\n",
        "    assert (matrix_input and pyg_input) == False\n",
        "\n",
        "    train_sample = filename + \"_train.data\"\n",
        "    val_sample = filename + \"_val.data\"\n",
        "    test_sample = filename + \"_test.data\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    if (\n",
        "        os.path.exists(train_sample)\n",
        "        and os.path.exists(val_sample)\n",
        "        and os.path.exists(test_sample)\n",
        "        and save_dataloader\n",
        "    ):\n",
        "        print(\"Loading from saved file...\")\n",
        "        print(\"Make sure all the DataLoader params are same.\")\n",
        "        print(\"This module is made for debugging only.\")\n",
        "        train_loader = torch.load(train_sample)\n",
        "        val_loader = torch.load(val_sample)\n",
        "        test_loader = torch.load(test_sample)\n",
        "        if train_loader.pin_memory != pin_memory:\n",
        "            train_loader.pin_memory = pin_memory\n",
        "        if test_loader.pin_memory != pin_memory:\n",
        "            test_loader.pin_memory = pin_memory\n",
        "        if val_loader.pin_memory != pin_memory:\n",
        "            val_loader.pin_memory = pin_memory\n",
        "        if train_loader.num_workers != workers:\n",
        "            train_loader.num_workers = workers\n",
        "        if test_loader.num_workers != workers:\n",
        "            test_loader.num_workers = workers\n",
        "        if val_loader.num_workers != workers:\n",
        "            val_loader.num_workers = workers\n",
        "        print(\"train\", len(train_loader.dataset))\n",
        "        print(\"val\", len(val_loader.dataset))\n",
        "        print(\"test\", len(test_loader.dataset))\n",
        "        return (\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            test_loader,\n",
        "            train_loader.dataset.prepare_batch,\n",
        "        )\n",
        "    else:\n",
        "        if not dataset_array:\n",
        "            d = jdata(dataset)\n",
        "        else:\n",
        "            d = dataset_array\n",
        "            # for ii, i in enumerate(pc_y):\n",
        "            #    d[ii][target] = pc_y[ii].tolist()\n",
        "\n",
        "        dat = []\n",
        "        if classification_threshold is not None:\n",
        "            print(\n",
        "                \"Using \",\n",
        "                classification_threshold,\n",
        "                \" for classifying \",\n",
        "                target,\n",
        "                \" data.\",\n",
        "            )\n",
        "            print(\"Converting target data into 1 and 0.\")\n",
        "        all_targets = []\n",
        "\n",
        "        # TODO:make an all key in qm9_dgl\n",
        "        if dataset == \"qm9_dgl\" and target == \"all\":\n",
        "            print(\"Making all qm9_dgl\")\n",
        "            tmp = []\n",
        "            for ii in d:\n",
        "                ii[\"all\"] = [\n",
        "                    ii[\"mu\"],\n",
        "                    ii[\"alpha\"],\n",
        "                    ii[\"homo\"],\n",
        "                    ii[\"lumo\"],\n",
        "                    ii[\"gap\"],\n",
        "                    ii[\"r2\"],\n",
        "                    ii[\"zpve\"],\n",
        "                    ii[\"U0\"],\n",
        "                    ii[\"U\"],\n",
        "                    ii[\"H\"],\n",
        "                    ii[\"G\"],\n",
        "                    ii[\"Cv\"],\n",
        "                ]\n",
        "                tmp.append(ii)\n",
        "            print(\"Made all qm9_dgl\")\n",
        "            d = tmp\n",
        "        for i in d:\n",
        "            if isinstance(i[target], list):  # multioutput target\n",
        "                all_targets.append(torch.tensor(i[target]))\n",
        "                dat.append(i)\n",
        "\n",
        "            elif (\n",
        "                i[target] is not None\n",
        "                and i[target] != \"na\"\n",
        "                and not math.isnan(i[target])\n",
        "            ):\n",
        "                if target_multiplication_factor is not None:\n",
        "                    i[target] = i[target] * target_multiplication_factor\n",
        "                if classification_threshold is not None:\n",
        "                    if i[target] <= classification_threshold:\n",
        "                        i[target] = 0\n",
        "                    elif i[target] > classification_threshold:\n",
        "                        i[target] = 1\n",
        "                    else:\n",
        "                        raise ValueError(\n",
        "                            \"Check classification data type.\",\n",
        "                            i[target],\n",
        "                            type(i[target]),\n",
        "                        )\n",
        "                dat.append(i)\n",
        "                all_targets.append(i[target])\n",
        "\n",
        "\n",
        "    if mp_id_list is not None:\n",
        "        if mp_id_list == 'bulk':\n",
        "            print('using mp bulk dataset')\n",
        "            with open('./data/bulk_megnet_train.pkl', 'rb') as f:\n",
        "                dataset_train = pk.load(f)\n",
        "            with open('./data/bulk_megnet_val.pkl', 'rb') as f:\n",
        "                dataset_val = pk.load(f)\n",
        "            with open('./data/bulk_megnet_test.pkl', 'rb') as f:\n",
        "                dataset_test = pk.load(f)\n",
        "\n",
        "        if mp_id_list == 'shear':\n",
        "            print('using mp shear dataset')\n",
        "            with open('./data/shear_megnet_train.pkl', 'rb') as f:\n",
        "                dataset_train = pk.load(f)\n",
        "            with open('./data/shear_megnet_val.pkl', 'rb') as f:\n",
        "                dataset_val = pk.load(f)\n",
        "            with open('./data/shear_megnet_test.pkl', 'rb') as f:\n",
        "                dataset_test = pk.load(f)\n",
        "\n",
        "    else:\n",
        "        id_train, id_val, id_test = get_id_train_val_test(\n",
        "            total_size=len(dat),\n",
        "            split_seed=split_seed,\n",
        "            train_ratio=train_ratio,\n",
        "            val_ratio=val_ratio,\n",
        "            test_ratio=test_ratio,\n",
        "            n_train=n_train,\n",
        "            n_test=n_test,\n",
        "            n_val=n_val,\n",
        "            keep_data_order=keep_data_order,\n",
        "        )\n",
        "        ids_train_val_test = {}\n",
        "        ids_train_val_test[\"id_train\"] = [dat[i][id_tag] for i in id_train]\n",
        "        ids_train_val_test[\"id_val\"] = [dat[i][id_tag] for i in id_val]\n",
        "        ids_train_val_test[\"id_test\"] = [dat[i][id_tag] for i in id_test]\n",
        "        dumpjson(\n",
        "            data=ids_train_val_test,\n",
        "            filename=os.path.join(output_dir, \"ids_train_val_test.json\"),\n",
        "        )\n",
        "        dataset_train = [dat[x] for x in id_train]\n",
        "        dataset_val = [dat[x] for x in id_val]\n",
        "        dataset_test = [dat[x] for x in id_test]\n",
        "\n",
        "    if standard_scalar_and_pca:\n",
        "        y_data = [i[target] for i in dataset_train]\n",
        "        # pipe = Pipeline([('scale', StandardScaler())])\n",
        "        if not isinstance(y_data[0], list):\n",
        "            print(\"Running StandardScalar\")\n",
        "            y_data = np.array(y_data).reshape(-1, 1)\n",
        "        sc = StandardScaler()\n",
        "\n",
        "        sc.fit(y_data)\n",
        "        print(\"Mean\", sc.mean_)\n",
        "        print(\"Variance\", sc.var_)\n",
        "        try:\n",
        "            print(\"New max\", max(y_data))\n",
        "            print(\"New min\", min(y_data))\n",
        "        except Exception as exp:\n",
        "            print(exp)\n",
        "            pass\n",
        "\n",
        "        pk.dump(sc, open(os.path.join(output_dir, \"sc.pkl\"), \"wb\"))\n",
        "\n",
        "    if classification_threshold is None:\n",
        "        try:\n",
        "            from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "            print(\"MAX val:\", max(all_targets))\n",
        "            print(\"MIN val:\", min(all_targets))\n",
        "            print(\"MAD:\", mean_absolute_deviation(all_targets))\n",
        "            try:\n",
        "                f = open(os.path.join(output_dir, \"mad\"), \"w\")\n",
        "                line = \"MAX val:\" + str(max(all_targets)) + \"\\n\"\n",
        "                line += \"MIN val:\" + str(min(all_targets)) + \"\\n\"\n",
        "                line += (\n",
        "                    \"MAD val:\"\n",
        "                    + str(mean_absolute_deviation(all_targets))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "                f.write(line)\n",
        "                f.close()\n",
        "            except Exception as exp:\n",
        "                print(\"Cannot write mad\", exp)\n",
        "                pass\n",
        "            # Random model precited value\n",
        "            x_bar = np.mean(np.array([i[target] for i in dataset_train]))\n",
        "            baseline_mae = mean_absolute_error(\n",
        "                np.array([i[target] for i in dataset_test]),\n",
        "                np.array([x_bar for i in dataset_test]),\n",
        "            )\n",
        "            print(\"Baseline MAE:\", baseline_mae)\n",
        "        except Exception as exp:\n",
        "            print(\"Data error\", exp)\n",
        "            pass\n",
        "\n",
        "    train_data, mean_train, std_train = get_pyg_dataset(\n",
        "        dataset=dataset_train,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"train_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "    )\n",
        "    val_data,_,_ = get_pyg_dataset(\n",
        "        dataset=dataset_val,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"val_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "        mean_train=mean_train,\n",
        "        std_train=std_train,\n",
        "    )\n",
        "    test_data,_,_ = get_pyg_dataset(\n",
        "        dataset=dataset_test,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"test_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "        mean_train=mean_train,\n",
        "        std_train=std_train,\n",
        "    )\n",
        "\n",
        "    collate_fn = train_data.collate\n",
        "    if line_graph:\n",
        "        collate_fn = train_data.collate_line_graph\n",
        "\n",
        "    # use a regular pytorch dataloader\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=True,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=True,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=False,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "    if save_dataloader:\n",
        "        torch.save(train_loader, train_sample)\n",
        "        torch.save(val_loader, val_sample)\n",
        "        torch.save(test_loader, test_sample)\n",
        "\n",
        "    print(\"n_train:\", len(train_loader.dataset))\n",
        "    print(\"n_val:\", len(val_loader.dataset))\n",
        "    print(\"n_test:\", len(test_loader.dataset))\n",
        "    return (\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        test_loader,\n",
        "        train_loader.dataset.prepare_batch,\n",
        "        mean_train,\n",
        "        std_train,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "j58PmpYdPWtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Matformer(nn.Module):\n",
        "    def __init__(self, config: MatformerConfig = MatformerConfig(name=\"matformer\")):\n",
        "        super().__init__()\n",
        "        self.classification = config.classification\n",
        "        self.use_angle = config.use_angle\n",
        "        self.atom_embedding = nn.Linear(config.atom_input_features, config.node_features)\n",
        "\n",
        "        self.rbf = nn.Sequential(\n",
        "            RBFExpansion(vmin=0, vmax=8.0, bins=config.edge_features),\n",
        "            nn.Linear(config.edge_features, config.node_features),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(config.node_features, config.node_features),\n",
        "        )\n",
        "\n",
        "        self.angle_lattice = config.angle_lattice\n",
        "\n",
        "        self.att_layers = nn.ModuleList([\n",
        "            MatformerConv(\n",
        "                in_channels=config.node_features,\n",
        "                out_channels=config.node_features,\n",
        "                heads=config.node_layer_head,\n",
        "                edge_dim=config.node_features\n",
        "            )\n",
        "            for _ in range(config.conv_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(config.node_features, config.fc_features),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "        if self.classification:\n",
        "            self.fc_out = nn.Linear(config.fc_features, 2)\n",
        "            self.softmax = nn.LogSoftmax(dim=1)\n",
        "        else:\n",
        "            self.fc_out = nn.Linear(config.fc_features, config.output_features)\n",
        "\n",
        "        self.link_name = config.link\n",
        "        self.link = self.get_link_fn(config.link, config.zero_inflated)\n",
        "\n",
        "    def get_link_fn(self, link: str, zero_inflated: bool):\n",
        "        if link == \"identity\":\n",
        "            return lambda x: x\n",
        "        elif link == \"log\":\n",
        "            avg_gap = 0.7\n",
        "            if not zero_inflated:\n",
        "                self.fc_out.bias.data = torch.tensor(np.log(avg_gap), dtype=torch.float)\n",
        "            return torch.exp\n",
        "        elif link == \"logit\":\n",
        "            return torch.sigmoid\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown link function: {link}\")\n",
        "\n",
        "    def forward(self, data_tuple):\n",
        "      data, ldata, lattice = data_tuple\n",
        "      act_list_x = []\n",
        "      act_list_y = []\n",
        "\n",
        "      # ---------- Atom Features ----------\n",
        "      x = self.atom_embedding(data.x)\n",
        "      atom_feature_avg = torch.mean(x, dim=0, keepdim=True)\n",
        "      act_list_x.append(atom_feature_avg)\n",
        "      print(\"\\n[Atom Features] shape:\", x.shape)\n",
        "      print(\"[Atom Features] sample:\\n\", x[:5])\n",
        "\n",
        "      # ---------- Bond Lengths ----------\n",
        "      bondlength = torch.norm(data.edge_attr, dim=1)\n",
        "      print(\"\\n[Bond Lengths] shape:\", bondlength.shape)\n",
        "      print(\"[Bond Lengths] sample:\\n\", bondlength[:10])\n",
        "\n",
        "      # ---------- Bond Features ----------\n",
        "      y = self.rbf(bondlength)\n",
        "      bond_feature_avg = torch.mean(y, dim=0, keepdim=True)\n",
        "      act_list_y.append(bond_feature_avg)\n",
        "      print(\"\\n[Bond Features] shape:\", y.shape)\n",
        "      print(\"[Bond Features] sample:\\n\", y[:5])\n",
        "\n",
        "      # ---------- Attention Layers ----------\n",
        "      for i, layer in enumerate(self.att_layers):\n",
        "          print(f\"\\n[Attention Layer {i + 1}]\")\n",
        "          x = layer(x, data.edge_index, y)\n",
        "          act_list_x.append(torch.mean(x, dim=0, keepdim=True))\n",
        "          act_list_y.append(torch.mean(y, dim=0, keepdim=True))\n",
        "\n",
        "      # ---------- Graph-level Output ----------\n",
        "      features = scatter(x, data.batch, dim=0, reduce=\"mean\")\n",
        "      features = self.fc(features)\n",
        "      out = self.fc_out(features)\n",
        "\n",
        "      if self.link:\n",
        "          out = self.link(out)\n",
        "\n",
        "      if self.classification:\n",
        "          out = self.softmax(out)\n",
        "\n",
        "      return torch.squeeze(out), act_list_x, act_list_y\n",
        "\n"
      ],
      "metadata": {
        "id": "z3oeYfSSyStI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = MatformerConfig(name=\"matformer\", conv_layers=4)\n",
        "model = Matformer(config)\n",
        "\n",
        "outputs, act_list_x, act_list_y = model((data, None, None))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzWV8pmweVO7",
        "outputId": "96646fce-dce8-47f5-cee1-ebfdc67269be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Atom Features] shape: torch.Size([2, 128])\n",
            "[Atom Features] sample:\n",
            " tensor([[-0.2976, -0.5521,  0.3740,  0.1117,  0.5190, -1.5470, -0.7916, -0.2023,\n",
            "         -0.1375,  0.1367,  0.4065, -0.0855,  0.4956, -0.3873,  0.4732, -0.4463,\n",
            "          0.1207,  0.6740,  0.4839,  0.2251, -0.1911,  0.3847, -0.2233,  0.4441,\n",
            "          0.9409, -0.5192, -0.1019,  0.9338, -0.2131, -0.1071,  0.3965,  0.2450,\n",
            "          0.4077, -0.6257,  1.0619, -0.2559, -0.3063,  0.0253,  0.3726,  0.3966,\n",
            "         -0.7921, -0.0502,  0.6373,  0.3175,  0.0231, -0.1748, -0.7618, -0.1238,\n",
            "         -0.7776,  0.1801, -0.9898,  0.7548,  0.6404,  0.8304,  0.2705, -0.4908,\n",
            "         -0.0456,  0.2351,  0.6243, -0.0221, -0.1272,  0.7097,  0.7599,  0.7663,\n",
            "         -0.2221, -0.7154,  0.0112,  0.0707, -0.7113,  0.7705, -0.3521, -0.3462,\n",
            "          0.4180, -1.1179,  0.1312, -0.3735,  0.6422, -0.4409, -0.1850,  0.8141,\n",
            "          0.0146, -0.6391, -0.0435, -0.4133,  0.8307, -0.6684,  0.9834, -0.0545,\n",
            "         -0.0526,  0.2228,  0.4522,  0.6352, -0.4935,  0.6039, -0.5780,  0.1187,\n",
            "          0.1119, -0.5742, -0.8127, -0.4767, -1.0627, -0.1604, -0.7938,  0.6776,\n",
            "         -0.4841, -1.0212, -0.5659,  0.1557,  0.0465, -0.4871,  1.0698,  0.1241,\n",
            "         -0.3139,  1.2241,  1.3882, -0.1128, -0.6385, -0.2916, -0.6309, -1.0407,\n",
            "          0.0191, -1.3064,  0.0545,  0.7256,  0.5478, -0.7577, -0.5042, -0.9842],\n",
            "        [ 1.0741,  0.1562,  0.0116, -0.2044, -0.4025, -0.1786, -1.2306, -1.4194,\n",
            "         -0.3057,  0.1789,  0.1775,  0.9535,  0.0800, -1.0128,  0.6979, -0.6016,\n",
            "         -0.2112, -0.6489,  0.6419,  1.1913,  0.0514, -0.2633, -0.2308,  0.7679,\n",
            "         -0.0388, -1.0560, -1.0556, -0.5021, -0.2676, -0.3084,  0.4332, -1.0973,\n",
            "          0.7809,  0.1365, -0.2215,  0.2752,  0.5624,  0.5757, -0.6819,  0.1506,\n",
            "         -0.2714,  0.9084, -0.6803,  0.1204, -0.1994,  1.0134, -0.3849,  1.4621,\n",
            "          0.0433,  0.1835,  0.0220, -1.1154,  0.3903,  0.7731, -0.0557,  0.0550,\n",
            "         -0.7772, -0.5284,  1.3438, -0.8345, -0.6660,  0.0473,  0.0054, -0.1346,\n",
            "         -0.0633,  0.0106,  0.5146, -0.4133,  0.0760, -0.5310,  0.6615,  0.0702,\n",
            "          0.1295,  0.3268, -0.4121,  0.2106, -0.5903, -0.6247,  0.6147,  0.0332,\n",
            "          0.5878, -0.4328, -0.8194, -0.3068, -0.6431,  0.1939,  0.2343, -0.3948,\n",
            "         -0.0799, -0.4062, -0.0234, -0.6863,  0.3628,  0.5220,  0.8600,  1.0190,\n",
            "          0.2321, -0.0242, -0.2293, -0.2332,  0.1335,  0.3057, -0.0056,  0.2307,\n",
            "          1.1658,  0.4131,  0.0085, -0.4469, -0.1088, -0.7946,  0.2127, -0.9462,\n",
            "          0.3956, -0.0114, -0.0704, -0.0808,  0.0016, -0.0316, -0.1987, -0.6671,\n",
            "         -1.2419,  0.5763, -0.1787, -0.8499, -0.3009, -0.2342,  0.5368,  0.3903]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "[Bond Lengths] shape: torch.Size([2])\n",
            "[Bond Lengths] sample:\n",
            " tensor([1., 1.])\n",
            "\n",
            "[Bond Features] shape: torch.Size([2, 128])\n",
            "[Bond Features] sample:\n",
            " tensor([[ 0.2690,  0.1089,  0.3910, -0.2940, -0.3001,  1.0337,  0.1252,  1.3159,\n",
            "         -0.1445, -0.4185, -0.1697,  0.3081,  0.5597,  0.0953, -0.3264, -0.1715,\n",
            "          0.0178,  0.3236,  0.8968, -0.2927, -0.6878, -0.2810,  0.5116,  0.2484,\n",
            "          0.0395, -0.0577, -0.4708,  0.2484, -0.8760, -0.6604,  0.0497,  0.4079,\n",
            "         -0.0089, -0.5269,  0.1427,  0.0018, -0.1832,  0.3709, -0.1819, -0.2449,\n",
            "          0.2114, -0.1083,  0.0191, -1.1353,  0.1807, -0.1848,  0.3430,  0.0593,\n",
            "          0.1303,  0.4671, -0.6254,  0.5068, -0.0884, -0.1549, -0.6523,  0.0580,\n",
            "         -0.1902, -0.2406,  0.5026, -0.3217, -0.1819,  0.0831,  0.3249,  0.6955,\n",
            "          0.2766,  0.0262,  0.3233,  0.1214, -0.4374, -0.0503, -0.0743, -0.2309,\n",
            "         -0.3134,  0.3838,  0.3154,  0.1255, -0.1907,  0.1486, -0.0100, -0.5023,\n",
            "          0.0669, -0.2397,  0.2259,  0.0486, -0.2769,  0.3730, -0.3406, -0.4452,\n",
            "         -0.0072, -0.7949, -0.3394, -0.5789, -0.1266,  0.1195, -0.3378,  0.1169,\n",
            "          0.6685, -0.2244, -0.1532, -0.7300, -0.0620, -0.2713, -0.0639, -0.3713,\n",
            "          0.2039,  0.0494, -0.3569, -0.6873,  0.1066,  0.1267,  0.1041, -0.1514,\n",
            "         -0.5487, -0.1007, -0.2508,  0.4243,  0.3186, -0.4152,  0.2244,  0.1014,\n",
            "          0.1414, -0.2540, -0.2233, -0.6833, -0.4749,  0.1267, -0.1711,  0.2362],\n",
            "        [ 0.2690,  0.1089,  0.3910, -0.2940, -0.3001,  1.0337,  0.1252,  1.3159,\n",
            "         -0.1445, -0.4185, -0.1697,  0.3081,  0.5597,  0.0953, -0.3264, -0.1715,\n",
            "          0.0178,  0.3236,  0.8968, -0.2927, -0.6878, -0.2810,  0.5116,  0.2484,\n",
            "          0.0395, -0.0577, -0.4708,  0.2484, -0.8760, -0.6604,  0.0497,  0.4079,\n",
            "         -0.0089, -0.5269,  0.1427,  0.0018, -0.1832,  0.3709, -0.1819, -0.2449,\n",
            "          0.2114, -0.1083,  0.0191, -1.1353,  0.1807, -0.1848,  0.3430,  0.0593,\n",
            "          0.1303,  0.4671, -0.6254,  0.5068, -0.0884, -0.1549, -0.6523,  0.0580,\n",
            "         -0.1902, -0.2406,  0.5026, -0.3217, -0.1819,  0.0831,  0.3249,  0.6955,\n",
            "          0.2766,  0.0262,  0.3233,  0.1214, -0.4374, -0.0503, -0.0743, -0.2309,\n",
            "         -0.3134,  0.3838,  0.3154,  0.1255, -0.1907,  0.1486, -0.0100, -0.5023,\n",
            "          0.0669, -0.2397,  0.2259,  0.0486, -0.2769,  0.3730, -0.3406, -0.4452,\n",
            "         -0.0072, -0.7949, -0.3394, -0.5789, -0.1266,  0.1195, -0.3378,  0.1169,\n",
            "          0.6685, -0.2244, -0.1532, -0.7300, -0.0620, -0.2713, -0.0639, -0.3713,\n",
            "          0.2039,  0.0494, -0.3569, -0.6873,  0.1066,  0.1267,  0.1041, -0.1514,\n",
            "         -0.5487, -0.1007, -0.2508,  0.4243,  0.3186, -0.4152,  0.2244,  0.1014,\n",
            "          0.1414, -0.2540, -0.2233, -0.6833, -0.4749,  0.1267, -0.1711,  0.2362]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "[Attention Layer 1]\n",
            "\n",
            "[Attention Layer 2]\n",
            "\n",
            "[Attention Layer 3]\n",
            "\n",
            "[Attention Layer 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "df = load_dataset(name=dataset_name, target=target_property, limit=1000)\n",
        "print(f\"Loaded {len(df)} samples.\")\n",
        "\n",
        "data = create_graphs(df, target_property)\n",
        "print(f\"Graphs created: {len(data)}\")\n",
        "if len(data) == 0:\n",
        "    raise ValueError(\"No valid graphs created.\")\n",
        "\n",
        "# Print node feature dimension of first sample\n",
        "print(\"Example node features shape:\", data[0].x.shape)  # e.g. torch.Size([N, F])\n",
        "print(\"Example edge_attr shape:\", data[0].edge_attr.shape)\n",
        "\n",
        "# Extract actual node feature size\n",
        "node_feature_dim = data[0].x.shape[1]\n",
        "\n",
        "# Update config with actual node feature size\n",
        "config = MatformerConfig(\n",
        "    name=\"matformer\",\n",
        "    conv_layers=5,\n",
        "    node_features=128,\n",
        "    edge_features=128,\n",
        "    output_features=1,\n",
        "    node_layer_head=4,\n",
        "    atom_input_features=node_feature_dim,  # fix here\n",
        ")\n",
        "model = Matformer(config).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0mvlkPkyVL3",
        "outputId": "83214b47-6d8b-488f-d043-dbd1ea920ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Obtaining 3D dataset 55k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Loading the zipfile...\n",
            "Loading completed.\n",
            "Loaded 1000 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 1119.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graphs created: 1000\n",
            "Example node features shape: torch.Size([8, 1])\n",
            "Example edge_attr shape: torch.Size([56, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Friday\n",
        "  "
      ],
      "metadata": {
        "id": "XhEq7AO7DLMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zjuKeLiu/DPF.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLnSoV_ODI96",
        "outputId": "246523a1-20a1-4a2d-ae94-42cab4bb34d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DPF' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6oajJGkGD-pS",
        "outputId": "3a421077-5ad2-4c74-c1eb-b84bcd298935"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DPF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwY1lDJ3EAPO",
        "outputId": "bc7b03c0-4ddb-4168-e73d-5a31ac44ba06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DPF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "graph.py\n"
      ],
      "metadata": {
        "id": "GmYj-04RFtFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Module to generate networkx graphs.\"\"\"\n",
        "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
        "from multiprocessing.context import ForkContext\n",
        "from re import X\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from jarvis.core.specie import chem_data, get_node_attributes\n",
        "\n",
        "# from jarvis.core.atoms import Atoms\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Sequence, Optional\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import LineGraph\n",
        "from torch_geometric.data.batch import Batch\n",
        "import itertools\n",
        "from jarvis.core.specie import chem_data, get_node_attributes\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    from tqdm import tqdm\n",
        "except Exception as exp:\n",
        "    print(\"torch/tqdm is not installed.\", exp)\n",
        "    pass\n",
        "\n",
        "chemical_symbols = [\n",
        "    # 0\n",
        "    'X',\n",
        "    # 1\n",
        "    'H', 'He',\n",
        "    # 2\n",
        "    'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',\n",
        "    # 3\n",
        "    'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar',\n",
        "    # 4\n",
        "    'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
        "    'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr',\n",
        "    # 5\n",
        "    'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd',\n",
        "    'In', 'Sn', 'Sb', 'Te', 'I', 'Xe',\n",
        "    # 6\n",
        "    'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy',\n",
        "    'Ho', 'Er', 'Tm', 'Yb', 'Lu',\n",
        "    'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi',\n",
        "    'Po', 'At', 'Rn',\n",
        "    # 7\n",
        "    'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk',\n",
        "    'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr',\n",
        "    'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc',\n",
        "    'Lv', 'Ts', 'Og']\n",
        "\n",
        "# pyg dataset\n",
        "class PygStructureDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset of crystal DGLGraphs.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        graphs: Sequence[Data],\n",
        "        target: str,\n",
        "        atom_features=\"atomic_number\",\n",
        "        transform=None,\n",
        "        line_graph=False,\n",
        "        classification=False,\n",
        "        id_tag=\"jid\",\n",
        "        neighbor_strategy=\"\",\n",
        "        lineControl=True,\n",
        "        mean_train=None,\n",
        "        std_train=None,\n",
        "        pre_train=False,\n",
        "        masks=None,\n",
        "        targets_mlm=None,\n",
        "        targets_lattice=None,\n",
        "        targets_position=None,\n",
        "    ):\n",
        "        \"\"\"Pytorch Dataset for atomistic graphs.\n",
        "\n",
        "        `df`: pandas dataframe from e.g. jarvis.db.figshare.data\n",
        "        `graphs`: DGLGraph representations corresponding to rows in `df`\n",
        "        `target`: key for label column in `df`\n",
        "        \"\"\"\n",
        "        self.masks = masks\n",
        "        self.targets_mlm = targets_mlm,\n",
        "        self.targets_lattice=targets_lattice,\n",
        "        self.targets_position=targets_position,\n",
        "        self.df = df\n",
        "        self.graphs = graphs\n",
        "        self.target = target\n",
        "        self.line_graph = line_graph\n",
        "        self.pre_train = pre_train\n",
        "        self.ids = self.df[id_tag]\n",
        "        self.atoms = self.df['atoms']\n",
        "        #print(self.df.head(2))\n",
        "        #print(\"##########################################\")\n",
        "        #print(target)\n",
        "        #print(\"##########################################\")\n",
        "        self.labels = torch.tensor(np.array(self.df[target].values)).type(\n",
        "            torch.get_default_dtype()\n",
        "        )\n",
        "        print(\"mean %f std %f\"%(self.labels.mean(), self.labels.std()))\n",
        "        if mean_train == None:\n",
        "            mean = self.labels.mean()\n",
        "            std = self.labels.std()\n",
        "            self.labels = (self.labels - mean) / std\n",
        "            print(\"normalize using training mean but shall not be used here %f and std %f\" % (mean, std))\n",
        "        else:\n",
        "            self.labels = (self.labels - mean_train) / std_train\n",
        "            print(\"normalize using training mean %f and std %f\" % (mean_train, std_train))\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        features = self._get_attribute_lookup(atom_features)\n",
        "\n",
        "        # load selected node representation\n",
        "        # assume graphs contain atomic number in g.ndata[\"atom_features\"]\n",
        "\n",
        "        '''\n",
        "        for g in graphs:\n",
        "            z = g.x\n",
        "            g.atomic_number = z\n",
        "            z = z.type(torch.IntTensor).squeeze()\n",
        "            f = torch.tensor(features[z]).type(torch.FloatTensor)\n",
        "            if g.x.size(0) == 1:\n",
        "                f = f.unsqueeze(0)\n",
        "            g.x = f\n",
        "        '''\n",
        "        self.prepare_batch = prepare_pyg_batch\n",
        "        if line_graph and pre_train:\n",
        "            self.prepare_batch = prepare_pyg_line_graph_batch_pre_train\n",
        "            print(\"pre_train\")\n",
        "            if lineControl == False:\n",
        "                self.line_graphs = []\n",
        "                self.graphs = []\n",
        "                for g in tqdm(graphs):\n",
        "                    linegraph_trans = LineGraph(force_directed=True)\n",
        "                    g_new = Data()\n",
        "                    g_new.x, g_new.edge_index, g_new.edge_attr = g.x, g.edge_index, g.edge_attr\n",
        "                    try:\n",
        "                        lg = linegraph_trans(g)\n",
        "                    except Exception as exp:\n",
        "                        print(g.x, g.edge_attr, exp)\n",
        "                        pass\n",
        "                    lg.edge_attr = pyg_compute_bond_cosines(lg) # old cosine emb\n",
        "                    # lg.edge_attr = pyg_compute_bond_angle(lg)\n",
        "                    self.graphs.append(g_new)\n",
        "                    self.line_graphs.append(lg)\n",
        "            else:\n",
        "                if neighbor_strategy == \"pairwise-k-nearest\":\n",
        "                    self.graphs = []\n",
        "                    labels = []\n",
        "                    idx_t = 0\n",
        "                    filter_out = 0\n",
        "                    max_size = 0\n",
        "                    for g in tqdm(graphs):\n",
        "                        g.edge_attr = g.edge_attr.float()\n",
        "                        if g.x.size(0) > max_size:\n",
        "                            max_size = g.x.size(0)\n",
        "                        if g.x.size(0) < 200:\n",
        "                            self.graphs.append(g)\n",
        "                            labels.append(self.labels[idx_t])\n",
        "                        else:\n",
        "                            filter_out += 1\n",
        "                        idx_t += 1\n",
        "                    print(\"filter out %d samples because of exceeding threshold of 200 for nn based method\" % filter_out)\n",
        "                    print(\"dataset max atom number %d\" % max_size)\n",
        "                    self.line_graphs = self.graphs\n",
        "                    self.labels = labels\n",
        "                    self.labels = torch.tensor(self.labels).type(\n",
        "                                    torch.get_default_dtype()\n",
        "                                )\n",
        "                else:\n",
        "                    self.graphs = []\n",
        "                    for g in tqdm(graphs):\n",
        "                        g.edge_attr = g.edge_attr.float()\n",
        "                        self.graphs.append(g)\n",
        "                    self.line_graphs = self.graphs\n",
        "        elif line_graph:\n",
        "            self.prepare_batch = prepare_pyg_line_graph_batch\n",
        "            print(\"building line graphs\")\n",
        "            if lineControl == False:\n",
        "                self.line_graphs = []\n",
        "                self.graphs = []\n",
        "                for g in tqdm(graphs):\n",
        "                    linegraph_trans = LineGraph(force_directed=True)\n",
        "                    g_new = Data()\n",
        "                    g_new.x, g_new.edge_index, g_new.edge_attr = g.x, g.edge_index, g.edge_attr\n",
        "                    try:\n",
        "                        lg = linegraph_trans(g)\n",
        "                    except Exception as exp:\n",
        "                        print(g.x, g.edge_attr, exp)\n",
        "                        pass\n",
        "                    lg.edge_attr = pyg_compute_bond_cosines(lg) # old cosine emb\n",
        "                    # lg.edge_attr = pyg_compute_bond_angle(lg)\n",
        "                    self.graphs.append(g_new)\n",
        "                    self.line_graphs.append(lg)\n",
        "            else:\n",
        "                if neighbor_strategy == \"pairwise-k-nearest\":\n",
        "                    self.graphs = []\n",
        "                    labels = []\n",
        "                    idx_t = 0\n",
        "                    filter_out = 0\n",
        "                    max_size = 0\n",
        "                    for g in tqdm(graphs):\n",
        "                        g.edge_attr = g.edge_attr.float()\n",
        "                        if g.x.size(0) > max_size:\n",
        "                            max_size = g.x.size(0)\n",
        "                        if g.x.size(0) < 200:\n",
        "                            self.graphs.append(g)\n",
        "                            labels.append(self.labels[idx_t])\n",
        "                        else:\n",
        "                            filter_out += 1\n",
        "                        idx_t += 1\n",
        "                    print(\"filter out %d samples because of exceeding threshold of 200 for nn based method\" % filter_out)\n",
        "                    print(\"dataset max atom number %d\" % max_size)\n",
        "                    self.line_graphs = self.graphs\n",
        "                    self.labels = labels\n",
        "                    self.labels = torch.tensor(self.labels).type(\n",
        "                                    torch.get_default_dtype()\n",
        "                                )\n",
        "                else:\n",
        "                    self.graphs = []\n",
        "                    for g in tqdm(graphs):\n",
        "                        g.edge_attr = g.edge_attr.float()\n",
        "                        self.graphs.append(g)\n",
        "                    self.line_graphs = self.graphs\n",
        "\n",
        "\n",
        "        if classification:\n",
        "            self.labels = self.labels.view(-1).long()\n",
        "            print(\"Classification dataset.\", self.labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_attribute_lookup(atom_features: str = \"cgcnn\"):\n",
        "        \"\"\"Build a lookup array indexed by atomic number.\"\"\"\n",
        "        max_z = max(v[\"Z\"] for v in chem_data.values())\n",
        "\n",
        "        # get feature shape (referencing Carbon)\n",
        "        template = get_node_attributes(\"C\", atom_features)\n",
        "\n",
        "        features = np.zeros((1 + max_z, len(template)))\n",
        "\n",
        "        for element, v in chem_data.items():\n",
        "            z = v[\"Z\"]\n",
        "            x = get_node_attributes(element, atom_features)\n",
        "\n",
        "            if x is not None:\n",
        "                features[z, :] = x\n",
        "\n",
        "        return features\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Get length.\"\"\"\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get StructureDataset sample.\"\"\"\n",
        "        g = self.graphs[idx]\n",
        "        label = self.labels[idx]\n",
        "        return_dict={}\n",
        "        if self.targets_mlm[0] is not None:\n",
        "            return_dict[\"mask\"] = self.masks[idx]\n",
        "            return_dict[\"atoms\"] = self.targets_mlm[0][idx]\n",
        "        if self.targets_lattice[0] is not None:\n",
        "            return_dict[\"lattice\"] = self.targets_lattice[0][idx]\n",
        "        if self.targets_position[0] is not None:\n",
        "            return_dict[\"positions\"] = self.targets_position[0][idx].t()\n",
        "\n",
        "        if self.transform:\n",
        "            g = self.transform(g)\n",
        "\n",
        "        if self.pre_train and self.line_graph:\n",
        "            return g, self.line_graphs[idx], return_dict\n",
        "        elif self.pre_train:\n",
        "            return g, self.masks[idx], self.targets_mlm[0][idx]\n",
        "\n",
        "        if self.line_graph:\n",
        "            return g, self.line_graphs[idx], label, label\n",
        "\n",
        "        return g, label\n",
        "\n",
        "    def setup_standardizer(self, ids):\n",
        "        \"\"\"Atom-wise feature standardization transform.\"\"\"\n",
        "        x = torch.cat(\n",
        "            [\n",
        "                g.x\n",
        "                for idx, g in enumerate(self.graphs)\n",
        "                if idx in ids\n",
        "            ]\n",
        "        )\n",
        "        self.atom_feature_mean = x.mean(0)\n",
        "        self.atom_feature_std = x.std(0)\n",
        "\n",
        "        self.transform = PygStandardize(\n",
        "            self.atom_feature_mean, self.atom_feature_std\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(samples: List[Tuple[Data, torch.Tensor]]):\n",
        "        \"\"\"Dataloader helper to batch graphs cross `samples`.\"\"\"\n",
        "        graphs, labels = map(list, zip(*samples))\n",
        "        batched_graph = Batch.from_data_list(graphs)\n",
        "        return batched_graph, torch.tensor(labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_line_graph(\n",
        "        samples: List[Tuple[Data, Data, torch.Tensor, torch.Tensor]]\n",
        "    ):\n",
        "        \"\"\"Dataloader helper to batch graphs cross `samples`.\"\"\"\n",
        "        graphs, line_graphs, lattice, labels = map(list, zip(*samples))\n",
        "        batched_graph = Batch.from_data_list(graphs)\n",
        "        batched_line_graph = Batch.from_data_list(line_graphs)\n",
        "        if len(labels[0].size()) > 0:\n",
        "            return batched_graph, batched_line_graph, torch.cat([i.unsqueeze(0) for i in lattice]), torch.stack(labels)\n",
        "        else:\n",
        "            return batched_graph, batched_line_graph, torch.cat([i.unsqueeze(0) for i in lattice]), torch.tensor(labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_line_graph_pretrain(\n",
        "        samples: List[Tuple[Data, Data, dict]]\n",
        "    ):\n",
        "        graphs, line_graphs, return_dict = map(list, zip(*samples))\n",
        "        batched_graph = Batch.from_data_list(graphs)\n",
        "        batched_line_graph = Batch.from_data_list(line_graphs)\n",
        "        target_dict = {}\n",
        "        if \"mask\" in return_dict[0]:\n",
        "            target_mask = []\n",
        "            target_atom = []\n",
        "            target_dict[\"mask\"] = target_mask\n",
        "            target_dict[\"atoms\"] = target_atom\n",
        "        if \"positions\" in return_dict[0]:\n",
        "            target_position = []\n",
        "            target_dict[\"positions\"] = target_position\n",
        "        if \"lattice\" in return_dict[0]:\n",
        "            target_lattice = []\n",
        "            target_dict[\"lattice\"] = target_lattice\n",
        "        for key in target_dict.keys():\n",
        "            for item in return_dict:\n",
        "                #print(key, type(item[key]))\n",
        "                target_dict[key].append(item[key])\n",
        "            target_dict[key] = torch.hstack(target_dict[key])\n",
        "        return batched_graph, batched_line_graph, target_dict\n",
        "\n",
        "\n",
        "def canonize_edge(\n",
        "    src_id,\n",
        "    dst_id,\n",
        "    src_image,\n",
        "    dst_image,\n",
        "):\n",
        "    \"\"\"Compute canonical edge representation.\n",
        "\n",
        "    Sort vertex ids\n",
        "    shift periodic images so the first vertex is in (0,0,0) image\n",
        "    \"\"\"\n",
        "    # store directed edges src_id <= dst_id\n",
        "    if dst_id < src_id:\n",
        "        src_id, dst_id = dst_id, src_id\n",
        "        src_image, dst_image = dst_image, src_image\n",
        "\n",
        "    # shift periodic images so that src is in (0,0,0) image\n",
        "    if not np.array_equal(src_image, (0, 0, 0)):\n",
        "        shift = src_image\n",
        "        src_image = tuple(np.subtract(src_image, shift))\n",
        "        dst_image = tuple(np.subtract(dst_image, shift))\n",
        "\n",
        "    assert src_image == (0, 0, 0)\n",
        "\n",
        "    return src_id, dst_id, src_image, dst_image\n",
        "\n",
        "\n",
        "def nearest_neighbor_edges_submit(\n",
        "    atoms=None,\n",
        "    cutoff=8,\n",
        "    max_neighbors=12,\n",
        "    id=None,\n",
        "    use_canonize=False,\n",
        "    use_lattice=False,\n",
        "    use_angle=False,\n",
        "):\n",
        "    \"\"\"Construct k-NN edge list.\"\"\"\n",
        "    # returns List[List[Tuple[site, distance, index, image]]]\n",
        "    lat = atoms.lattice\n",
        "    all_neighbors = atoms.get_all_neighbors(r=cutoff)\n",
        "    min_nbrs = min(len(neighborlist) for neighborlist in all_neighbors)\n",
        "\n",
        "    attempt = 0\n",
        "    if min_nbrs < max_neighbors:\n",
        "        lat = atoms.lattice\n",
        "        if cutoff < max(lat.a, lat.b, lat.c):\n",
        "            r_cut = max(lat.a, lat.b, lat.c)\n",
        "        else:\n",
        "            r_cut = 2 * cutoff\n",
        "        attempt += 1\n",
        "        return nearest_neighbor_edges_submit(\n",
        "            atoms=atoms,\n",
        "            use_canonize=use_canonize,\n",
        "            cutoff=r_cut,\n",
        "            max_neighbors=max_neighbors,\n",
        "            id=id,\n",
        "        )\n",
        "\n",
        "    edges = defaultdict(set)\n",
        "    for site_idx, neighborlist in enumerate(all_neighbors):\n",
        "\n",
        "        # sort on distance\n",
        "        neighborlist = sorted(neighborlist, key=lambda x: x[2])\n",
        "        distances = np.array([nbr[2] for nbr in neighborlist])\n",
        "        ids = np.array([nbr[1] for nbr in neighborlist])\n",
        "        images = np.array([nbr[3] for nbr in neighborlist])\n",
        "\n",
        "        # find the distance to the k-th nearest neighbor\n",
        "        max_dist = distances[max_neighbors - 1]\n",
        "        ids = ids[distances <= max_dist]\n",
        "        images = images[distances <= max_dist]\n",
        "        distances = distances[distances <= max_dist]\n",
        "        for dst, image in zip(ids, images):\n",
        "            src_id, dst_id, src_image, dst_image = canonize_edge(\n",
        "                site_idx, dst, (0, 0, 0), tuple(image)\n",
        "            )\n",
        "            if use_canonize:\n",
        "                edges[(src_id, dst_id)].add(dst_image)\n",
        "            else:\n",
        "                edges[(site_idx, dst)].add(tuple(image))\n",
        "\n",
        "        if use_lattice:\n",
        "            edges[(site_idx, site_idx)].add(tuple(np.array([0, 0, 1])))\n",
        "            edges[(site_idx, site_idx)].add(tuple(np.array([0, 1, 0])))\n",
        "            edges[(site_idx, site_idx)].add(tuple(np.array([1, 0, 0])))\n",
        "            edges[(site_idx, site_idx)].add(tuple(np.array([0, 1, 1])))\n",
        "            edges[(site_idx, site_idx)].add(tuple(np.array([1, 0, 1])))\n",
        "            edges[(site_idx, site_idx)].add(tuple(np.array([1, 1, 0])))\n",
        "\n",
        "    return edges\n",
        "\n",
        "\n",
        "\n",
        "def pair_nearest_neighbor_edges(\n",
        "        atoms=None,\n",
        "        pair_wise_distances=6,\n",
        "        use_lattice=False,\n",
        "        use_angle=False,\n",
        "):\n",
        "    \"\"\"Construct pairwise k-fully connected edge list.\"\"\"\n",
        "    smallest = pair_wise_distances\n",
        "    lattice_list = torch.as_tensor(\n",
        "        [[0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 0], [1, 0, 1], [0, 1, 1]]).float()\n",
        "\n",
        "    lattice = torch.as_tensor(atoms.lattice_mat).float()\n",
        "    pos = torch.as_tensor(atoms.cart_coords)\n",
        "    atom_num = pos.size(0)\n",
        "    lat = atoms.lattice\n",
        "    radius_needed = min(lat.a, lat.b, lat.c) * (smallest / 2 - 1e-9)\n",
        "    r_a = (np.floor(radius_needed / lat.a) + 1).astype(np.int)\n",
        "    r_b = (np.floor(radius_needed / lat.b) + 1).astype(np.int)\n",
        "    r_c = (np.floor(radius_needed / lat.c) + 1).astype(np.int)\n",
        "    period_list = np.array([l for l in itertools.product(*[list(range(-r_a, r_a + 1)), list(range(-r_b, r_b + 1)), list(range(-r_c, r_c + 1))])])\n",
        "    period_list = torch.as_tensor(period_list).float()\n",
        "    n_cells = period_list.size(0)\n",
        "    offset = torch.matmul(period_list, lattice).view(n_cells, 1, 3)\n",
        "    expand_pos = (pos.unsqueeze(0).expand(n_cells, -1, -1) + offset).transpose(0, 1).contiguous()\n",
        "    dist = (pos.unsqueeze(1).unsqueeze(1) - expand_pos.unsqueeze(0))  # [n, 1, 1, 3] - [1, n, n_cell, 3] -> [n, n, n_cell, 3]\n",
        "    dist2, index = torch.sort(dist.norm(dim=-1), dim=-1, stable=True)\n",
        "    max_value = dist2[:, :, smallest - 1]  # [n, n]\n",
        "    mask = (dist.norm(dim=-1) <= max_value.unsqueeze(-1))  # [n, n, n_cell]\n",
        "    shift = torch.matmul(lattice_list, lattice).repeat(atom_num, 1)\n",
        "    shift_src = torch.arange(atom_num).unsqueeze(-1).repeat(1, lattice_list.size(0))\n",
        "    shift_src = torch.cat([shift_src[i,:] for i in range(shift_src.size(0))])\n",
        "\n",
        "    indices = torch.where(mask)\n",
        "    dist_target = dist[indices]\n",
        "    u, v, _ = indices\n",
        "    if use_lattice:\n",
        "        u = torch.cat((u, shift_src), dim=0)\n",
        "        v = torch.cat((v, shift_src), dim=0)\n",
        "        dist_target = torch.cat((dist_target, shift), dim=0)\n",
        "        assert u.size(0) == dist_target.size(0)\n",
        "\n",
        "    return u, v, dist_target\n",
        "\n",
        "def build_undirected_edgedata(\n",
        "    atoms=None,\n",
        "    edges={},\n",
        "):\n",
        "    \"\"\"Build undirected graph data from edge set.\n",
        "\n",
        "    edges: dictionary mapping (src_id, dst_id) to set of dst_image\n",
        "    r: cartesian displacement vector from src -> dst\n",
        "    \"\"\"\n",
        "    # second pass: construct *undirected* graph\n",
        "    # import pprint\n",
        "    u, v, r = [], [], []\n",
        "    for (src_id, dst_id), images in edges.items():\n",
        "\n",
        "        for dst_image in images:\n",
        "            # fractional coordinate for periodic image of dst\n",
        "            dst_coord = atoms.frac_coords[dst_id] + dst_image\n",
        "            # cartesian displacement vector pointing from src -> dst\n",
        "            d = atoms.lattice.cart_coords(\n",
        "                dst_coord - atoms.frac_coords[src_id]\n",
        "            )\n",
        "            # if np.linalg.norm(d)!=0:\n",
        "            # print ('jv',dst_image,d)\n",
        "            # add edges for both directions\n",
        "            for uu, vv, dd in [(src_id, dst_id, d), (dst_id, src_id, -d)]:\n",
        "                u.append(uu)\n",
        "                v.append(vv)\n",
        "                r.append(dd)\n",
        "\n",
        "    u = torch.tensor(u)\n",
        "    v = torch.tensor(v)\n",
        "    if not isinstance(r, list):\n",
        "        r = torch.tensor(r).type(torch.get_default_dtype())\n",
        "    else:\n",
        "        r = torch.tensor(np.array(r)).type(torch.get_default_dtype())\n",
        "    return u, v, r\n",
        "\n",
        "\n",
        "class PygGraph(object):\n",
        "    \"\"\"Generate a graph object.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        nodes=[],\n",
        "        node_attributes=[],\n",
        "        edges=[],\n",
        "        edge_attributes=[],\n",
        "        color_map=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the graph object.\n",
        "\n",
        "        Args:\n",
        "            nodes: IDs of the graph nodes as integer array.\n",
        "\n",
        "            node_attributes: node features as multi-dimensional array.\n",
        "\n",
        "            edges: connectivity as a (u,v) pair where u is\n",
        "                   the source index and v the destination ID.\n",
        "\n",
        "            edge_attributes: attributes for each connectivity.\n",
        "                             as simple as euclidean distances.\n",
        "        \"\"\"\n",
        "        self.nodes = nodes\n",
        "        self.node_attributes = node_attributes\n",
        "        self.edges = edges\n",
        "        self.edge_attributes = edge_attributes\n",
        "        self.color_map = color_map\n",
        "        self.labels = labels\n",
        "\n",
        "    @staticmethod\n",
        "    def atom_dgl_multigraph(\n",
        "        atoms=None,\n",
        "        neighbor_strategy=\"k-nearest\",\n",
        "        cutoff=8.0,\n",
        "        max_neighbors=12,\n",
        "        atom_features=\"cgcnn\",\n",
        "        max_attempts=3,\n",
        "        id: Optional[str] = None,\n",
        "        compute_line_graph: bool = True,\n",
        "        use_canonize: bool = False,\n",
        "        use_lattice: bool = False,\n",
        "        use_angle: bool = False,\n",
        "    ):\n",
        "        if neighbor_strategy == \"k-nearest\":\n",
        "            edges = nearest_neighbor_edges_submit(\n",
        "                atoms=atoms,\n",
        "                cutoff=cutoff,\n",
        "                max_neighbors=max_neighbors,\n",
        "                id=id,\n",
        "                use_canonize=use_canonize,\n",
        "                use_lattice=use_lattice,\n",
        "                use_angle=use_angle,\n",
        "            )\n",
        "            u, v, r = build_undirected_edgedata(atoms, edges)\n",
        "        elif neighbor_strategy == \"pairwise-k-nearest\":\n",
        "            u, v, r = pair_nearest_neighbor_edges(\n",
        "                atoms=atoms,\n",
        "                pair_wise_distances=2,\n",
        "                use_lattice=use_lattice,\n",
        "                use_angle=use_angle,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Not implemented yet\", neighbor_strategy)\n",
        "\n",
        "\n",
        "        # build up atom attribute tensor\n",
        "        sps_features = []\n",
        "        '''\n",
        "        for ii, s in enumerate(atoms.elements):\n",
        "            feat = list(get_node_attributes(s, atom_features=atom_features))\n",
        "            sps_features.append(feat)\n",
        "        '''\n",
        "        '''\n",
        "        for ii, s in enumerate(atoms.elements):\n",
        "            one_hot = np.zeros(119)\n",
        "            one_hot[chemical_symbols.index(s)] = 1\n",
        "            feat = list(one_hot)\n",
        "            sps_features.append(feat)\n",
        "\n",
        "        sps_features = np.array(sps_features)\n",
        "\n",
        "        node_features = torch.tensor(sps_features).type(\n",
        "            torch.get_default_dtype()\n",
        "        )\n",
        "        '''\n",
        "        sps_features = []\n",
        "        for ii, s in enumerate(atoms.elements):\n",
        "            if s == 'X':\n",
        "\n",
        "                feat = [0.0] * 92\n",
        "            else:\n",
        "\n",
        "                feat = list(get_node_attributes(s, atom_features=atom_features))\n",
        "\n",
        "                if len(feat) < 92:\n",
        "                    feat += [0.0] * (92 - len(feat))\n",
        "                elif len(feat) > 92:\n",
        "                    feat = feat[:92]\n",
        "            sps_features.append(feat)\n",
        "\n",
        "        sps_features = np.array(sps_features)\n",
        "        node_features = torch.tensor(sps_features).type(\n",
        "            torch.get_default_dtype()\n",
        "        )\n",
        "\n",
        "        edge_index = torch.cat((u.unsqueeze(0), v.unsqueeze(0)), dim=0).long()\n",
        "        g = Data(x=node_features, edge_index=edge_index, edge_attr=r)\n",
        "\n",
        "        if compute_line_graph:\n",
        "            linegraph_trans = LineGraph(force_directed=True)\n",
        "            g_new = Data()\n",
        "            g_new.x, g_new.edge_index, g_new.edge_attr = g.x, g.edge_index, g.edge_attr\n",
        "            lg = linegraph_trans(g)\n",
        "            lg.edge_attr = pyg_compute_bond_cosines(lg)\n",
        "            return g_new, lg\n",
        "        else:\n",
        "            return g\n",
        "\n",
        "def pyg_compute_bond_cosines(lg):\n",
        "    \"\"\"Compute bond angle cosines from bond displacement vectors.\"\"\"\n",
        "    # line graph edge: (a, b), (b, c)\n",
        "    # `a -> b -> c`\n",
        "    # use law of cosines to compute angles cosines\n",
        "    # negate src bond so displacements are like `a <- b -> c`\n",
        "    # cos(theta) = ba \\dot bc / (||ba|| ||bc||)\n",
        "    src, dst = lg.edge_index\n",
        "    x = lg.x\n",
        "    r1 = -x[src]\n",
        "    r2 = x[dst]\n",
        "    bond_cosine = torch.sum(r1 * r2, dim=1) / (\n",
        "        torch.norm(r1, dim=1) * torch.norm(r2, dim=1)\n",
        "    )\n",
        "    bond_cosine = torch.clamp(bond_cosine, -1, 1)\n",
        "    return bond_cosine\n",
        "\n",
        "def pyg_compute_bond_angle(lg):\n",
        "    \"\"\"Compute bond angle from bond displacement vectors.\"\"\"\n",
        "    # line graph edge: (a, b), (b, c)\n",
        "    # `a -> b -> c`\n",
        "    src, dst = lg.edge_index\n",
        "    x = lg.x\n",
        "    r1 = -x[src]\n",
        "    r2 = x[dst]\n",
        "    a = (r1 * r2).sum(dim=-1) # cos_angle * |pos_ji| * |pos_jk|\n",
        "    b = torch.cross(r1, r2).norm(dim=-1) # sin_angle * |pos_ji| * |pos_jk|\n",
        "    angle = torch.atan2(b, a)\n",
        "    return angle\n",
        "\n",
        "\n",
        "\n",
        "class PygStandardize(torch.nn.Module):\n",
        "    \"\"\"Standardize atom_features: subtract mean and divide by std.\"\"\"\n",
        "\n",
        "    def __init__(self, mean: torch.Tensor, std: torch.Tensor):\n",
        "        \"\"\"Register featurewise mean and standard deviation.\"\"\"\n",
        "        super().__init__()\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def forward(self, g: Data):\n",
        "        \"\"\"Apply standardization to atom_features.\"\"\"\n",
        "        h = g.x\n",
        "        #g.x = (h - self.mean) / self.std\n",
        "        return g\n",
        "\n",
        "\n",
        "\n",
        "def prepare_pyg_batch(\n",
        "    batch: Tuple[Data, torch.Tensor], device=None, non_blocking=False\n",
        "):\n",
        "    \"\"\"Send batched dgl crystal graph to device.\"\"\"\n",
        "    g, t = batch\n",
        "    batch = (\n",
        "        g.to(device),\n",
        "        t.to(device, non_blocking=non_blocking),\n",
        "    )\n",
        "\n",
        "    return batch\n",
        "\n",
        "def prepare_pyg_line_graph_batch_pre_train(\n",
        "    batch: Tuple[Tuple[Data, Data], dict],\n",
        "    device=None,\n",
        "    non_blocking=False,\n",
        "):\n",
        "    \"\"\"Send line graph batch to device.\n",
        "\n",
        "    Note: the batch is a nested tuple, with the graph and line graph together\n",
        "    \"\"\"\n",
        "    g, lg, return_dict = batch\n",
        "    for i, item in enumerate(return_dict):\n",
        "        return_dict[item]  = return_dict[item].to(device, non_blocking=non_blocking)\n",
        "    batch = (\n",
        "        (\n",
        "            g.to(device),\n",
        "            lg.to(device),\n",
        "        ),\n",
        "        (\n",
        "            return_dict,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "\n",
        "def prepare_pyg_line_graph_batch(\n",
        "    batch: Tuple[Tuple[Data, Data, torch.Tensor], torch.Tensor],\n",
        "    device=None,\n",
        "    non_blocking=False,\n",
        "):\n",
        "    \"\"\"Send line graph batch to device.\n",
        "\n",
        "    Note: the batch is a nested tuple, with the graph and line graph together\n",
        "    \"\"\"\n",
        "    g, lg, lattice, t = batch\n",
        "    batch = (\n",
        "        (\n",
        "            g.to(device),\n",
        "            lg.to(device),\n",
        "            lattice.to(device, non_blocking=non_blocking),\n",
        "        ),\n",
        "        t.to(device, non_blocking=non_blocking),\n",
        "    )\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "goD3B5hrEJGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afda4357-0b99-419e-e0eb-c392c701e87b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#changed part of the above code\n",
        "from jarvis.core.specie import chem_data, get_node_attributes\n",
        "sps_features = []\n",
        "        for ii, s in enumerate(atoms.elements):\n",
        "            if s == 'X':\n",
        "\n",
        "                feat = [0.0] * 92\n",
        "            else:\n",
        "\n",
        "                feat = list(get_node_attributes(s, atom_features=atom_features))\n",
        "\n",
        "                if len(feat) < 92:\n",
        "                    feat += [0.0] * (92 - len(feat))\n",
        "                elif len(feat) > 92:\n",
        "                    feat = feat[:92]\n",
        "            sps_features.append(feat)\n",
        "\n",
        "        sps_features = np.array(sps_features)\n",
        "        node_features = torch.tensor(sps_features).type(\n",
        "            torch.get_default_dtype()\n",
        "        )"
      ],
      "metadata": {
        "id": "32rV_N392-JZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "fcd9608c-844d-444b-946e-9e9a86b82e07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-9-d7744a92ea18>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-d7744a92ea18>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    for ii, s in enumerate(atoms.elements):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data.py"
      ],
      "metadata": {
        "id": "utfEqIeFXSC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
        "\n",
        "import imp\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "# from typing import Dict, List, Optional, Set, Tuple\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from jarvis.core.atoms import Atoms\n",
        "from matformer.graphs import PygGraph, PygStructureDataset, chemical_symbols\n",
        "#\n",
        "from jarvis.db.figshare import data as jdata\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from jarvis.db.jsonutils import dumpjson\n",
        "\n",
        "# from sklearn.pipeline import Pipeline\n",
        "import pickle as pk\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "# use pandas progress_apply\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "def load_dataset(\n",
        "    name: str = \"dft_3d\",\n",
        "    target=None,\n",
        "    limit: Optional[int] = None,\n",
        "    classification_threshold: Optional[float] = None,\n",
        "):\n",
        "    \"\"\"Load jarvis data.\"\"\"\n",
        "    d = jdata(name)\n",
        "    data = []\n",
        "    for i in d:\n",
        "        if i[target] != \"na\" and not math.isnan(i[target]):\n",
        "            if classification_threshold is not None:\n",
        "                if i[target] <= classification_threshold:\n",
        "                    i[target] = 0\n",
        "                elif i[target] > classification_threshold:\n",
        "                    i[target] = 1\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        \"Check classification data type.\",\n",
        "                        i[target],\n",
        "                        type(i[target]),\n",
        "                    )\n",
        "            data.append(i)\n",
        "    d = data\n",
        "    if limit is not None:\n",
        "        d = d[:limit]\n",
        "    d = pd.DataFrame(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "def mean_absolute_deviation(data, axis=None):\n",
        "    \"\"\"Get Mean absolute deviation.\"\"\"\n",
        "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
        "\n",
        "\n",
        "\n",
        "def load_pyg_graphs(\n",
        "    df: pd.DataFrame,\n",
        "    name: str = \"dft_3d\",\n",
        "    neighbor_strategy: str = \"k-nearest\",\n",
        "    cutoff: float = 8,\n",
        "    max_neighbors: int = 12,\n",
        "    cachedir: Optional[Path] = None,\n",
        "    use_canonize: bool = False,\n",
        "    use_lattice: bool = False,\n",
        "    use_angle: bool = False,\n",
        "):\n",
        "    \"\"\"Construct crystal graphs.\n",
        "\n",
        "    Load only atomic number node features\n",
        "    and bond displacement vector edge features.\n",
        "\n",
        "    Resulting graphs have scheme e.g.\n",
        "    ```\n",
        "    Graph(num_nodes=12, num_edges=156,\n",
        "          ndata_schemes={'atom_features': Scheme(shape=(1,)}\n",
        "          edata_schemes={'r': Scheme(shape=(3,)})\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def atoms_to_graph(atoms):\n",
        "        \"\"\"Convert structure dict to DGLGraph.\"\"\"\n",
        "        structure = Atoms.from_dict(atoms)\n",
        "        return PygGraph.atom_dgl_multigraph(\n",
        "            structure,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            cutoff=cutoff,\n",
        "            atom_features=\"atomic_number\",\n",
        "            max_neighbors=max_neighbors,\n",
        "            compute_line_graph=False,\n",
        "            use_canonize=use_canonize,\n",
        "            use_lattice=use_lattice,\n",
        "            use_angle=use_angle,\n",
        "        )\n",
        "    #pandarallel.initialize(progress_bar=True)\n",
        "    pandarallel.initialize(nb_workers=32)\n",
        "    graphs = df[\"atoms\"].parallel_apply(atoms_to_graph).values\n",
        "\n",
        "    return graphs\n",
        "\n",
        "\n",
        "def get_id_train_val_test(\n",
        "    total_size=1000,\n",
        "    split_seed=123,\n",
        "    train_ratio=None,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    n_train=None,\n",
        "    n_test=None,\n",
        "    n_val=None,\n",
        "    keep_data_order=False,\n",
        "):\n",
        "    \"\"\"Get train, val, test IDs.\"\"\"\n",
        "    if (\n",
        "        train_ratio is None\n",
        "        and val_ratio is not None\n",
        "        and test_ratio is not None\n",
        "    ):\n",
        "        if train_ratio is None:\n",
        "            assert val_ratio + test_ratio < 1\n",
        "            train_ratio = 1 - val_ratio - test_ratio\n",
        "            print(\"Using rest of the dataset except the test and val sets.\")\n",
        "        else:\n",
        "            assert train_ratio + val_ratio + test_ratio <= 1\n",
        "    # indices = list(range(total_size))\n",
        "    if n_train is None:\n",
        "        n_train = int(train_ratio * total_size)\n",
        "    if n_test is None:\n",
        "        n_test = int(test_ratio * total_size)\n",
        "    if n_val is None:\n",
        "        n_val = int(val_ratio * total_size)\n",
        "    ids = list(np.arange(total_size))\n",
        "    if not keep_data_order:\n",
        "        random.seed(split_seed)\n",
        "        random.shuffle(ids)\n",
        "    if n_train + n_val + n_test > total_size:\n",
        "        raise ValueError(\n",
        "            \"Check total number of samples.\",\n",
        "            n_train + n_val + n_test,\n",
        "            \">\",\n",
        "            total_size,\n",
        "        )\n",
        "\n",
        "    id_train = ids[:n_train]\n",
        "    id_val = ids[-(n_val + n_test) : -n_test]  # noqa:E203\n",
        "    id_test = ids[-n_test:]\n",
        "    return id_train, id_val, id_test\n",
        "\n",
        "\n",
        "def get_torch_dataset(\n",
        "    dataset=[],\n",
        "    id_tag=\"jid\",\n",
        "    target=\"\",\n",
        "    neighbor_strategy=\"\",\n",
        "    atom_features=\"\",\n",
        "    use_canonize=\"\",\n",
        "    name=\"\",\n",
        "    line_graph=\"\",\n",
        "    cutoff=8.0,\n",
        "    max_neighbors=12,\n",
        "    classification=False,\n",
        "    output_dir=\".\",\n",
        "    tmp_name=\"dataset\",\n",
        "):\n",
        "    \"\"\"Get Torch Dataset.\"\"\"\n",
        "    df = pd.DataFrame(dataset)\n",
        "    # print(\"df\", df)\n",
        "    vals = df[target].values\n",
        "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
        "        val_list = [vals[i].item() for i in range(len(vals))]\n",
        "        vals = val_list\n",
        "    print(\"data range\", np.max(vals), np.min(vals))\n",
        "    print(\"data mean and std\", np.mean(vals), np.std(vals))\n",
        "    f = open(os.path.join(output_dir, tmp_name + \"_data_range\"), \"w\")\n",
        "    line = \"Max=\" + str(np.max(vals)) + \"\\n\"\n",
        "    f.write(line)\n",
        "    line = \"Min=\" + str(np.min(vals)) + \"\\n\"\n",
        "    f.write(line)\n",
        "    f.close()\n",
        "\n",
        "    graphs = load_graphs(\n",
        "        df,\n",
        "        name=name,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "    )\n",
        "\n",
        "    data = StructureDataset(\n",
        "        df,\n",
        "        graphs,\n",
        "        target=target,\n",
        "        atom_features=atom_features,\n",
        "        line_graph=line_graph,\n",
        "        id_tag=id_tag,\n",
        "        classification=classification,\n",
        "    )\n",
        "    return data\n",
        "\n",
        "def get_pyg_dataset(\n",
        "    dataset=[],\n",
        "    masks = None,\n",
        "    targets_mlm=None,\n",
        "    id_tag=\"jid\",\n",
        "    target=\"\",\n",
        "    neighbor_strategy=\"\",\n",
        "    atom_features=\"\",\n",
        "    use_canonize=\"\",\n",
        "    name=\"\",\n",
        "    line_graph=\"\",\n",
        "    cutoff=8.0,\n",
        "    max_neighbors=12,\n",
        "    classification=False,\n",
        "    output_dir=\".\",\n",
        "    tmp_name=\"dataset\",\n",
        "    use_lattice=False,\n",
        "    use_angle=False,\n",
        "    data_from='Jarvis',\n",
        "    use_save=False,\n",
        "    mean_train=None,\n",
        "    std_train=None,\n",
        "    now=False, # for test\n",
        "    pre_train=False,\n",
        "    targets_lattice= None,\n",
        "    targets_position = None,\n",
        "):\n",
        "    \"\"\"Get pyg Dataset.\"\"\"\n",
        "    df = pd.DataFrame(dataset)\n",
        "    # print(\"df\", df)\n",
        "    # neighbor_strategy = \"pairwise-k-nearest\"\n",
        "\n",
        "    vals = df[target].values\n",
        "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
        "        val_list = [vals[i].item() for i in range(len(vals))]\n",
        "        vals = val_list\n",
        "    output_dir = \"./saved_data/\" + tmp_name + \"test_graph_angle.pkl\" # for fast test use\n",
        "    print(\"data range\", np.max(vals), np.min(vals))\n",
        "    print(output_dir)\n",
        "    if now:\n",
        "        if not os.path.exists(output_dir):\n",
        "            graphs = load_pyg_graphs(\n",
        "                df,\n",
        "                name=name,\n",
        "                neighbor_strategy=neighbor_strategy,\n",
        "                use_canonize=use_canonize,\n",
        "                cutoff=cutoff,\n",
        "                max_neighbors=max_neighbors,\n",
        "                use_lattice=use_lattice,\n",
        "                use_angle=use_angle,\n",
        "            )\n",
        "            with open(output_dir, 'wb') as pf:\n",
        "                pk.dump(graphs, pf)\n",
        "            print('save graphs to ', output_dir)\n",
        "        else:\n",
        "            print('loading graphs from ', output_dir)\n",
        "            with open(output_dir, 'rb') as pf:\n",
        "                graphs = pk.load(pf)\n",
        "    else:\n",
        "        print('graphs not saved')\n",
        "        graphs = load_pyg_graphs(\n",
        "            df,\n",
        "            name=name,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            use_canonize=use_canonize,\n",
        "            cutoff=cutoff,\n",
        "            max_neighbors=max_neighbors,\n",
        "            use_lattice=use_lattice,\n",
        "            use_angle=use_angle,\n",
        "        )\n",
        "    if mean_train == None:\n",
        "        mean_train = np.mean(vals)\n",
        "        std_train = np.std(vals)\n",
        "        data = PygStructureDataset(\n",
        "            df,\n",
        "            graphs,\n",
        "            masks=masks,\n",
        "            targets_mlm=targets_mlm,\n",
        "            targets_lattice=targets_lattice,\n",
        "            targets_position=targets_position,\n",
        "            target=target,\n",
        "            atom_features=atom_features,\n",
        "            line_graph=line_graph,\n",
        "            id_tag=id_tag,\n",
        "            classification=classification,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            mean_train=mean_train,\n",
        "            std_train=std_train,\n",
        "            pre_train=pre_train\n",
        "        )\n",
        "    else:\n",
        "        data = PygStructureDataset(\n",
        "            df,\n",
        "            graphs,\n",
        "            masks=masks,\n",
        "            targets_mlm=targets_mlm,\n",
        "            targets_lattice=targets_lattice,\n",
        "            targets_position=targets_position,\n",
        "            target=target,\n",
        "            atom_features=atom_features,\n",
        "            line_graph=line_graph,\n",
        "            id_tag=id_tag,\n",
        "            classification=classification,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            mean_train=mean_train,\n",
        "            std_train=std_train,\n",
        "            pre_train=pre_train\n",
        "        )\n",
        "    return data, mean_train, std_train\n",
        "\n",
        "\n",
        "def get_train_val_loaders(\n",
        "    dataset: str = \"dft_3d\",\n",
        "    dataset_array=[],\n",
        "    target: str = \"formation_energy_peratom\",\n",
        "    atom_features: str = \"cgcnn\",\n",
        "    neighbor_strategy: str = \"k-nearest\",\n",
        "    n_train=None,\n",
        "    n_val=None,\n",
        "    n_test=None,\n",
        "    train_ratio=None,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    batch_size: int = 5,\n",
        "    standardize: bool = False,\n",
        "    line_graph: bool = True,\n",
        "    split_seed: int = 123,\n",
        "    workers: int = 0,\n",
        "    pin_memory: bool = True,\n",
        "    save_dataloader: bool = False,\n",
        "    filename: str = \"sample\",\n",
        "    id_tag: str = \"jid\",\n",
        "    use_canonize: bool = False,\n",
        "    cutoff: float = 8.0,\n",
        "    max_neighbors: int = 12,\n",
        "    classification_threshold: Optional[float] = None,\n",
        "    target_multiplication_factor: Optional[float] = None,\n",
        "    standard_scalar_and_pca=False,\n",
        "    keep_data_order=False,\n",
        "    output_features=1,\n",
        "    output_dir=None,\n",
        "    matrix_input=False,\n",
        "    pyg_input=False,\n",
        "    use_lattice=False,\n",
        "    use_angle=False,\n",
        "    use_save=True,\n",
        "    mp_id_list=None,\n",
        "    pre_train=False,\n",
        "    mask_ratio=None,\n",
        "    lattice_noise=None,\n",
        "    position_noise=None,\n",
        "):\n",
        "    \"\"\"Help function to set up JARVIS train and val dataloaders.\"\"\"\n",
        "    # data loading\n",
        "    mean_train=None\n",
        "    std_train=None\n",
        "    assert (matrix_input and pyg_input) == False\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    train_sample = os.path.join(output_dir, filename + \"_train.data\")\n",
        "    val_sample = os.path.join(output_dir, filename + \"_val.data\")\n",
        "    test_sample = os.path.join(output_dir, filename + \"_test.data\")\n",
        "\n",
        "    if (\n",
        "        os.path.exists(train_sample)\n",
        "        and os.path.exists(val_sample)\n",
        "        and os.path.exists(test_sample)\n",
        "        and save_dataloader\n",
        "    ):\n",
        "        print(\"Loading from saved file...\")\n",
        "        print(\"Make sure all the DataLoader params are same.\")\n",
        "        print(\"This module is made for debugging only.\")\n",
        "        train_loader = torch.load(train_sample)\n",
        "        val_loader = torch.load(val_sample)\n",
        "        test_loader = torch.load(test_sample)\n",
        "        if train_loader.pin_memory != pin_memory:\n",
        "            train_loader.pin_memory = pin_memory\n",
        "        if test_loader.pin_memory != pin_memory:\n",
        "            test_loader.pin_memory = pin_memory\n",
        "        if val_loader.pin_memory != pin_memory:\n",
        "            val_loader.pin_memory = pin_memory\n",
        "        if train_loader.num_workers != workers:\n",
        "            train_loader.num_workers = workers\n",
        "        if test_loader.num_workers != workers:\n",
        "            test_loader.num_workers = workers\n",
        "        if val_loader.num_workers != workers:\n",
        "            val_loader.num_workers = workers\n",
        "        print(\"train\", len(train_loader.dataset))\n",
        "        print(\"val\", len(val_loader.dataset))\n",
        "        print(\"test\", len(test_loader.dataset))\n",
        "        return (\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            test_loader,\n",
        "            train_loader.dataset.prepare_batch,\n",
        "            None,\n",
        "            None\n",
        "        )\n",
        "    else:\n",
        "        if not dataset_array:\n",
        "            d = jdata(dataset)\n",
        "        else:\n",
        "            d = dataset_array\n",
        "            # for ii, i in enumerate(pc_y):\n",
        "            #    d[ii][target] = pc_y[ii].tolist()\n",
        "\n",
        "        dat = []\n",
        "        if classification_threshold is not None:\n",
        "            print(\n",
        "                \"Using \",\n",
        "                classification_threshold,\n",
        "                \" for classifying \",\n",
        "                target,\n",
        "                \" data.\",\n",
        "            )\n",
        "            print(\"Converting target data into 1 and 0.\")\n",
        "        all_targets = []\n",
        "\n",
        "        # TODO:make an all key in qm9_dgl\n",
        "        if dataset == \"qm9_dgl\" and target == \"all\":\n",
        "            print(\"Making all qm9_dgl\")\n",
        "            tmp = []\n",
        "            for ii in d:\n",
        "                ii[\"all\"] = [\n",
        "                    ii[\"mu\"],\n",
        "                    ii[\"alpha\"],\n",
        "                    ii[\"homo\"],\n",
        "                    ii[\"lumo\"],\n",
        "                    ii[\"gap\"],\n",
        "                    ii[\"r2\"],\n",
        "                    ii[\"zpve\"],\n",
        "                    ii[\"U0\"],\n",
        "                    ii[\"U\"],\n",
        "                    ii[\"H\"],\n",
        "                    ii[\"G\"],\n",
        "                    ii[\"Cv\"],\n",
        "                ]\n",
        "                tmp.append(ii)\n",
        "            print(\"Made all qm9_dgl\")\n",
        "            d = tmp\n",
        "        all_targets = []\n",
        "        masks = []\n",
        "        position_gt = []\n",
        "        lattice_gt = []\n",
        "\n",
        "        for i in d:\n",
        "            if pre_train:\n",
        "                if mask_ratio is not None:\n",
        "                    if len(i[\"atoms\"][\"elements\"]) < 2:\n",
        "                        continue\n",
        "                    num_to_predict = max(1, math.ceil(len(i[\"atoms\"][\"elements\"])*mask_ratio))\n",
        "                    sampled_indices = random.sample(range(len(i[\"atoms\"][\"elements\"])), num_to_predict)\n",
        "                    atoms = i[\"atoms\"][\"elements\"]\n",
        "                    mask = torch.zeros((len(atoms)))\n",
        "                    z = []\n",
        "                    for ele in i[\"atoms\"][\"elements\"]:\n",
        "                        z.append(chemical_symbols.index(ele))\n",
        "                    for sampled_index in sampled_indices:\n",
        "                        i[\"atoms\"][\"elements\"][sampled_index] = \"X\"\n",
        "                        mask[sampled_index] = 1\n",
        "                    all_targets.append(torch.tensor(np.array(z)))\n",
        "                    masks.append(mask)\n",
        "                if position_noise is not None:\n",
        "                    position_gt.append(torch.tensor(np.array(i['atoms']['coords'])))\n",
        "                    position_noised = []\n",
        "                    for position in i['atoms']['coords']:\n",
        "                        position_noised.append(((np.array(position)+np.random.rand(3)*position_noise)%1.0).tolist())\n",
        "                    i['atoms']['coords'] = position_noised\n",
        "                if lattice_noise is not None:\n",
        "                    lattice_gt.append(torch.tensor(np.array(i['atoms']['lattice_mat'])))\n",
        "                    lattice_noised = []\n",
        "                    for lat_vec in i['atoms']['lattice_mat']:\n",
        "                        lattice_noised.append(((np.array(lat_vec)+np.random.rand(3)*lattice_noise)).tolist())\n",
        "                    i['atoms']['lattice_mat'] =  lattice_noised\n",
        "                dat.append(i)\n",
        "\n",
        "            elif isinstance(i[target], list):  # multioutput target\n",
        "                all_targets.append(torch.tensor(i[target]))\n",
        "                dat.append(i)\n",
        "\n",
        "            elif (\n",
        "                i[target] is not None\n",
        "                and i[target] != \"na\"\n",
        "                and not math.isnan(i[target])\n",
        "            ):\n",
        "                if target_multiplication_factor is not None:\n",
        "                    i[target] = i[target] * target_multiplication_factor\n",
        "                if classification_threshold is not None:\n",
        "                    if i[target] <= classification_threshold:\n",
        "                        i[target] = 0\n",
        "                    elif i[target] > classification_threshold:\n",
        "                        i[target] = 1\n",
        "                    else:\n",
        "                        raise ValueError(\n",
        "                            \"Check classification data type.\",\n",
        "                            i[target],\n",
        "                            type(i[target]),\n",
        "                        )\n",
        "                dat.append(i)\n",
        "                all_targets.append(i[target])\n",
        "\n",
        "\n",
        "    if mp_id_list is not None:\n",
        "        if mp_id_list == 'bulk':\n",
        "            print('using mp bulk dataset')\n",
        "            with open('./data/bulk_megnet_train.pkl', 'rb') as f:\n",
        "                dataset_train = pk.load(f)\n",
        "            with open('./data/bulk_megnet_val.pkl', 'rb') as f:\n",
        "                dataset_val = pk.load(f)\n",
        "            with open('./data/bulk_megnet_test.pkl', 'rb') as f:\n",
        "                dataset_test = pk.load(f)\n",
        "\n",
        "        if mp_id_list == 'shear':\n",
        "            print('using mp shear dataset')\n",
        "            with open('./data/shear_megnet_train.pkl', 'rb') as f:\n",
        "                dataset_train = pk.load(f)\n",
        "            with open('./data/shear_megnet_val.pkl', 'rb') as f:\n",
        "                dataset_val = pk.load(f)\n",
        "            with open('./data/shear_megnet_test.pkl', 'rb') as f:\n",
        "                dataset_test = pk.load(f)\n",
        "\n",
        "    else:\n",
        "        id_train, id_val, id_test = get_id_train_val_test(\n",
        "            total_size=len(dat),\n",
        "            split_seed=split_seed,\n",
        "            train_ratio=train_ratio,\n",
        "            val_ratio=val_ratio,\n",
        "            test_ratio=test_ratio,\n",
        "            n_train=n_train,\n",
        "            n_test=n_test,\n",
        "            n_val=n_val,\n",
        "            keep_data_order=keep_data_order,\n",
        "        )\n",
        "        ids_train_val_test = {}\n",
        "        ids_train_val_test[\"id_train\"] = [dat[i][id_tag] for i in id_train]\n",
        "        ids_train_val_test[\"id_val\"] = [dat[i][id_tag] for i in id_val]\n",
        "        ids_train_val_test[\"id_test\"] = [dat[i][id_tag] for i in id_test]\n",
        "        dumpjson(\n",
        "            data=ids_train_val_test,\n",
        "            filename=os.path.join(output_dir, \"ids_train_val_test.json\"),\n",
        "        )\n",
        "        dataset_train = [dat[x] for x in id_train]\n",
        "        dataset_val = [dat[x] for x in id_val]\n",
        "        dataset_test = [dat[x] for x in id_test]\n",
        "    if pre_train:\n",
        "        if mask_ratio is not None:\n",
        "            masks_train = [masks[x] for x in id_train]\n",
        "            masks_val = [masks[x] for x in id_val]\n",
        "            masks_test = [masks[x] for x in id_test]\n",
        "            targets_train = [all_targets[x] for x in id_train]\n",
        "            targets_val = [all_targets[x] for x in id_val]\n",
        "            targets_test = [all_targets[x] for x in id_test]\n",
        "        else:\n",
        "            targets_train = None\n",
        "            targets_val = None\n",
        "            targets_test = None\n",
        "            masks_train = None\n",
        "            masks_val = None\n",
        "            masks_test = None\n",
        "\n",
        "        if position_noise is not None:\n",
        "            position_gt_train = [position_gt[x] for x in id_train]\n",
        "            position_gt_val = [position_gt[x] for x in id_val]\n",
        "            position_gt_test = [position_gt[x] for x in id_test]\n",
        "        else:\n",
        "            position_gt_train = None\n",
        "            position_gt_val = None\n",
        "            position_gt_test = None\n",
        "\n",
        "        if lattice_noise is not None:\n",
        "            lattice_gt_train = [lattice_gt[x] for x in id_train]\n",
        "            lattice_gt_val = [lattice_gt[x] for x in id_val]\n",
        "            lattice_gt_test = [lattice_gt[x] for x in id_test]\n",
        "        else:\n",
        "            lattice_gt_train = None\n",
        "            lattice_gt_val = None\n",
        "            lattice_gt_test = None\n",
        "\n",
        "\n",
        "    if pre_train:\n",
        "        pass\n",
        "    elif standard_scalar_and_pca:\n",
        "        y_data = [i[target] for i in dataset_train]\n",
        "        # pipe = Pipeline([('scale', StandardScaler())])\n",
        "        if not isinstance(y_data[0], list):\n",
        "            print(\"Running StandardScalar\")\n",
        "            y_data = np.array(y_data).reshape(-1, 1)\n",
        "        sc = StandardScaler()\n",
        "\n",
        "        sc.fit(y_data)\n",
        "        print(\"Mean\", sc.mean_)\n",
        "        print(\"Variance\", sc.var_)\n",
        "        try:\n",
        "            print(\"New max\", max(y_data))\n",
        "            print(\"New min\", min(y_data))\n",
        "        except Exception as exp:\n",
        "            print(exp)\n",
        "            pass\n",
        "\n",
        "        pk.dump(sc, open(os.path.join(output_dir, \"sc.pkl\"), \"wb\"))\n",
        "\n",
        "    if classification_threshold is None:\n",
        "        try:\n",
        "            from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "            print(\"MAX val:\", max(all_targets))\n",
        "            print(\"MIN val:\", min(all_targets))\n",
        "            print(\"MAD:\", mean_absolute_deviation(all_targets))\n",
        "            try:\n",
        "                f = open(os.path.join(output_dir, \"mad\"), \"w\")\n",
        "                line = \"MAX val:\" + str(max(all_targets)) + \"\\n\"\n",
        "                line += \"MIN val:\" + str(min(all_targets)) + \"\\n\"\n",
        "                line += (\n",
        "                    \"MAD val:\"\n",
        "                    + str(mean_absolute_deviation(all_targets))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "                f.write(line)\n",
        "                f.close()\n",
        "            except Exception as exp:\n",
        "                print(\"Cannot write mad\", exp)\n",
        "                pass\n",
        "            # Random model precited value\n",
        "            x_bar = np.mean(np.array([i[target] for i in dataset_train]))\n",
        "            baseline_mae = mean_absolute_error(\n",
        "                np.array([i[target] for i in dataset_test]),\n",
        "                np.array([x_bar for i in dataset_test]),\n",
        "            )\n",
        "            print(\"Baseline MAE:\", baseline_mae)\n",
        "        except Exception as exp:\n",
        "            print(\"Data error\", exp)\n",
        "            pass\n",
        "\n",
        "    train_data, mean_train, std_train = get_pyg_dataset(\n",
        "        dataset=dataset_train,\n",
        "        masks=masks_train,\n",
        "        targets_mlm= targets_train,\n",
        "        targets_position = position_gt_train,\n",
        "        targets_lattice = lattice_gt_train,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"train_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "        pre_train=pre_train,\n",
        "    )\n",
        "    val_data,_,_ = get_pyg_dataset(\n",
        "        dataset=dataset_val,\n",
        "        masks=masks_val,\n",
        "        targets_mlm= targets_val,\n",
        "        targets_position = position_gt_val,\n",
        "        targets_lattice = lattice_gt_val,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"val_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "        mean_train=mean_train,\n",
        "        std_train=std_train,\n",
        "        pre_train=pre_train,\n",
        "    )\n",
        "    test_data,_,_ = get_pyg_dataset(\n",
        "        dataset=dataset_test,\n",
        "        masks = masks_test,\n",
        "        targets_mlm= targets_test,\n",
        "        targets_position = position_gt_test,\n",
        "        targets_lattice = lattice_gt_test,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"test_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "        mean_train=mean_train,\n",
        "        std_train=std_train,\n",
        "        pre_train=pre_train\n",
        "    )\n",
        "\n",
        "    collate_fn = train_data.collate\n",
        "    if pre_train and line_graph:\n",
        "        collate_fn = train_data.collate_line_graph_pretrain\n",
        "    elif line_graph:\n",
        "        collate_fn = train_data.collate_line_graph\n",
        "\n",
        "    # use a regular pytorch dataloader\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=False,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=False,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=False,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "    if save_dataloader:\n",
        "        torch.save(train_loader, train_sample)\n",
        "        torch.save(val_loader, val_sample)\n",
        "        torch.save(test_loader, test_sample)\n",
        "\n",
        "    print(\"n_train:\", len(train_loader.dataset))\n",
        "    print(\"n_val:\", len(val_loader.dataset))\n",
        "    print(\"n_test:\", len(test_loader.dataset))\n",
        "    return (\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        test_loader,\n",
        "        train_loader.dataset.prepare_batch,\n",
        "        mean_train,\n",
        "        std_train,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "gQo7X2vSXRN-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_accelerator\n"
      ],
      "metadata": {
        "id": "xTLF71ntgZHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
        "\n",
        "import imp\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "import gc\n",
        "# from typing import Dict, List, Optional, Set, Tuple\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from jarvis.core.atoms import Atoms\n",
        "from matformer.graphs import PygGraph, PygStructureDataset, chemical_symbols\n",
        "#\n",
        "from jarvis.db.figshare import data as jdata\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from jarvis.db.jsonutils import dumpjson\n",
        "\n",
        "# from sklearn.pipeline import Pipeline\n",
        "import pickle as pk\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pandarallel import pandarallel\n",
        "import time\n",
        "# use pandas progress_apply\n",
        "tqdm.pandas()\n",
        "\n",
        "class ConstructTarget():\n",
        "    def __init__(self, mask_ratio=None, position_noise=None, lattice_noise=None):\n",
        "        super().__init__()\n",
        "        self._mask_ratio = mask_ratio\n",
        "        self._position_noise = position_noise\n",
        "        self._lattice_noise = lattice_noise\n",
        "    def forward(self, i):\n",
        "        pd_dict = {}\n",
        "        use_this = True\n",
        "        if self._mask_ratio is not None:\n",
        "            if len(i[\"elements\"]) < 2:\n",
        "                use_this = False\n",
        "                mask = None\n",
        "                target = None\n",
        "            else:\n",
        "                num_to_predict = max(1, math.ceil(len(i[\"elements\"])*self._mask_ratio))\n",
        "                sampled_indices = random.sample(range(len(i[\"elements\"])), num_to_predict)\n",
        "                atoms = i[\"elements\"]\n",
        "                mask = torch.zeros((len(atoms)))\n",
        "                z = []\n",
        "                for ele in i[\"elements\"]:\n",
        "                    z.append(chemical_symbols.index(ele))\n",
        "                for sampled_index in sampled_indices:\n",
        "                    i[\"elements\"][sampled_index] = \"X\"\n",
        "                    mask[sampled_index] = 1\n",
        "                target = torch.tensor(np.array(z))\n",
        "            pd_dict['mask']  = mask\n",
        "            pd_dict['target_all']  = target\n",
        "        if self._position_noise is not None:\n",
        "            pd_dict['position_gt'] = torch.tensor(np.array(i['coords']))\n",
        "            position_noised = []\n",
        "            for position in i['coords']:\n",
        "                position_noised.append(((np.array(position)+np.random.rand(3)*self._position_noise)%1.0).tolist())\n",
        "            i['coords'] = position_noised\n",
        "        if self._lattice_noise is not None:\n",
        "            pd_dict['lattice_gt'] = torch.tensor(np.array(i['lattice_mat']))\n",
        "            lattice_noised = []\n",
        "            for lat_vec in i['lattice_mat']:\n",
        "                lattice_noised.append(((np.array(lat_vec)+np.random.rand(3)*self._lattice_noise)).tolist())\n",
        "            i['lattice_mat'] =  lattice_noised\n",
        "        pd_dict['atoms'] = i\n",
        "        pd_dict['use_this']  = use_this\n",
        "        #print(len(pd_dict['mask']), len(pd_dict['atoms_noised'][\"atoms\"][\"elements\"]))\n",
        "        return pd.Series(pd_dict)\n",
        "\n",
        "def load_dataset(\n",
        "    name: str = \"dft_3d\",\n",
        "    target=None,\n",
        "    limit: Optional[int] = None,\n",
        "    classification_threshold: Optional[float] = None,\n",
        "):\n",
        "    \"\"\"Load jarvis data.\"\"\"\n",
        "    d = jdata(name)\n",
        "    data = []\n",
        "    for i in d:\n",
        "        if i[target] != \"na\" and not math.isnan(i[target]):\n",
        "            if classification_threshold is not None:\n",
        "                if i[target] <= classification_threshold:\n",
        "                    i[target] = 0\n",
        "                elif i[target] > classification_threshold:\n",
        "                    i[target] = 1\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        \"Check classification data type.\",\n",
        "                        i[target],\n",
        "                        type(i[target]),\n",
        "                    )\n",
        "            data.append(i)\n",
        "    d = data\n",
        "    if limit is not None:\n",
        "        d = d[:limit]\n",
        "    d = pd.DataFrame(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "def mean_absolute_deviation(data, axis=None):\n",
        "    \"\"\"Get Mean absolute deviation.\"\"\"\n",
        "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
        "\n",
        "\n",
        "def load_pyg_graphs(\n",
        "    df: pd.DataFrame,\n",
        "    name: str = \"dft_3d\",\n",
        "    neighbor_strategy: str = \"k-nearest\",\n",
        "    cutoff: float = 8,\n",
        "    max_neighbors: int = 12,\n",
        "    cachedir: Optional[Path] = None,\n",
        "    use_canonize: bool = False,\n",
        "    use_lattice: bool = False,\n",
        "    use_angle: bool = False,\n",
        "):\n",
        "    \"\"\"Construct crystal graphs.\n",
        "\n",
        "    Load only atomic number node features\n",
        "    and bond displacement vector edge features.\n",
        "\n",
        "    Resulting graphs have scheme e.g.\n",
        "    ```\n",
        "    Graph(num_nodes=12, num_edges=156,\n",
        "          ndata_schemes={'atom_features': Scheme(shape=(1,)}\n",
        "          edata_schemes={'r': Scheme(shape=(3,)})\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def atoms_to_graph(atoms):\n",
        "        \"\"\"Convert structure dict to DGLGraph.\"\"\"\n",
        "        structure = Atoms.from_dict(atoms)\n",
        "        return PygGraph.atom_dgl_multigraph(\n",
        "            structure,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            cutoff=cutoff,\n",
        "            atom_features=\"atomic_number\",\n",
        "            max_neighbors=max_neighbors,\n",
        "            compute_line_graph=False,\n",
        "            use_canonize=use_canonize,\n",
        "            use_lattice=use_lattice,\n",
        "            use_angle=use_angle,\n",
        "        )\n",
        "    #pandarallel.initialize(progress_bar=True)\n",
        "    pandarallel.initialize()\n",
        "    graphs = df[\"atoms\"].parallel_apply(atoms_to_graph).values\n",
        "\n",
        "    return graphs\n",
        "\n",
        "\n",
        "def get_id_train_val_test(\n",
        "    total_size=1000,\n",
        "    split_seed=123,\n",
        "    train_ratio=None,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    n_train=None,\n",
        "    n_test=None,\n",
        "    n_val=None,\n",
        "    keep_data_order=False,\n",
        "):\n",
        "    \"\"\"Get train, val, test IDs.\"\"\"\n",
        "    if (\n",
        "        train_ratio is None\n",
        "        and val_ratio is not None\n",
        "        and test_ratio is not None\n",
        "    ):\n",
        "        if train_ratio is None:\n",
        "            assert val_ratio + test_ratio < 1\n",
        "            train_ratio = 1 - val_ratio - test_ratio\n",
        "            print(\"Using rest of the dataset except the test and val sets.\")\n",
        "        else:\n",
        "            assert train_ratio + val_ratio + test_ratio <= 1\n",
        "    # indices = list(range(total_size))\n",
        "    if n_train is None:\n",
        "        n_train = int(train_ratio * total_size)\n",
        "    if n_test is None:\n",
        "        n_test = int(test_ratio * total_size)\n",
        "    if n_val is None:\n",
        "        n_val = int(val_ratio * total_size)\n",
        "    ids = list(np.arange(total_size))\n",
        "    if not keep_data_order:\n",
        "        random.seed(split_seed)\n",
        "        random.shuffle(ids)\n",
        "    if n_train + n_val + n_test > total_size:\n",
        "        raise ValueError(\n",
        "            \"Check total number of samples.\",\n",
        "            n_train + n_val + n_test,\n",
        "            \">\",\n",
        "            total_size,\n",
        "        )\n",
        "\n",
        "    id_train = ids[:n_train]\n",
        "    id_val = ids[-(n_val + n_test) : -n_test]  # noqa:E203\n",
        "    id_test = ids[-n_test:]\n",
        "    return id_train, id_val, id_test\n",
        "\n",
        "\n",
        "def get_torch_dataset(\n",
        "    dataset=[],\n",
        "    id_tag=\"jid\",\n",
        "    target=\"\",\n",
        "    neighbor_strategy=\"\",\n",
        "    atom_features=\"\",\n",
        "    use_canonize=\"\",\n",
        "    name=\"\",\n",
        "    line_graph=\"\",\n",
        "    cutoff=8.0,\n",
        "    max_neighbors=12,\n",
        "    classification=False,\n",
        "    output_dir=\".\",\n",
        "    tmp_name=\"dataset\",\n",
        "):\n",
        "    \"\"\"Get Torch Dataset.\"\"\"\n",
        "    df = pd.DataFrame(dataset)\n",
        "    # print(\"df\", df)\n",
        "    vals = df[target].values\n",
        "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
        "        val_list = [vals[i].item() for i in range(len(vals))]\n",
        "        vals = val_list\n",
        "    print(\"data range\", np.max(vals), np.min(vals))\n",
        "    print(\"data mean and std\", np.mean(vals), np.std(vals))\n",
        "    f = open(os.path.join(output_dir, tmp_name + \"_data_range\"), \"w\")\n",
        "    line = \"Max=\" + str(np.max(vals)) + \"\\n\"\n",
        "    f.write(line)\n",
        "    line = \"Min=\" + str(np.min(vals)) + \"\\n\"\n",
        "    f.write(line)\n",
        "    f.close()\n",
        "\n",
        "    graphs = load_graphs(\n",
        "        df,\n",
        "        name=name,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "    )\n",
        "\n",
        "    data = StructureDataset(\n",
        "        df,\n",
        "        graphs,\n",
        "        target=target,\n",
        "        atom_features=atom_features,\n",
        "        line_graph=line_graph,\n",
        "        id_tag=id_tag,\n",
        "        classification=classification,\n",
        "    )\n",
        "    return data\n",
        "\n",
        "def get_pyg_dataset(\n",
        "    dataset=[],\n",
        "    masks = None,\n",
        "    targets_mlm=None,\n",
        "    id_tag=\"jid\",\n",
        "    target=\"\",\n",
        "    neighbor_strategy=\"\",\n",
        "    atom_features=\"\",\n",
        "    use_canonize=\"\",\n",
        "    name=\"\",\n",
        "    line_graph=\"\",\n",
        "    cutoff=8.0,\n",
        "    max_neighbors=12,\n",
        "    classification=False,\n",
        "    output_dir=\".\",\n",
        "    tmp_name=\"dataset\",\n",
        "    use_lattice=False,\n",
        "    use_angle=False,\n",
        "    data_from='Jarvis',\n",
        "    use_save=False,\n",
        "    mean_train=None,\n",
        "    std_train=None,\n",
        "    now=False, # for test\n",
        "    pre_train=False,\n",
        "    targets_lattice= None,\n",
        "    targets_position = None,\n",
        "):\n",
        "    \"\"\"Get pyg Dataset.\"\"\"\n",
        "    df = pd.DataFrame(dataset)\n",
        "    # print(\"df\", df)\n",
        "    # neighbor_strategy = \"pairwise-k-nearest\"\n",
        "\n",
        "    vals = df[target].values\n",
        "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
        "        val_list = [vals[i].item() for i in range(len(vals))]\n",
        "        vals = val_list\n",
        "    output_dir = \"./saved_data/\" + tmp_name + \"test_graph_angle.pkl\" # for fast test use\n",
        "    print(\"data range\", np.max(vals), np.min(vals))\n",
        "    print(output_dir)\n",
        "    if now:\n",
        "        if not os.path.exists(output_dir):\n",
        "            graphs = load_pyg_graphs(\n",
        "                df,\n",
        "                name=name,\n",
        "                neighbor_strategy=neighbor_strategy,\n",
        "                use_canonize=use_canonize,\n",
        "                cutoff=cutoff,\n",
        "                max_neighbors=max_neighbors,\n",
        "                use_lattice=use_lattice,\n",
        "                use_angle=use_angle,\n",
        "            )\n",
        "            with open(output_dir, 'wb') as pf:\n",
        "                pk.dump(graphs, pf)\n",
        "            print('save graphs to ', output_dir)\n",
        "        else:\n",
        "            print('loading graphs from ', output_dir)\n",
        "            with open(output_dir, 'rb') as pf:\n",
        "                graphs = pk.load(pf)\n",
        "    else:\n",
        "        print('graphs not saved')\n",
        "        graphs = load_pyg_graphs(\n",
        "            df,\n",
        "            name=name,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            use_canonize=use_canonize,\n",
        "            cutoff=cutoff,\n",
        "            max_neighbors=max_neighbors,\n",
        "            use_lattice=use_lattice,\n",
        "            use_angle=use_angle,\n",
        "        )\n",
        "    if mean_train == None:\n",
        "        mean_train = np.mean(vals)\n",
        "        std_train = np.std(vals)\n",
        "        data = PygStructureDataset(\n",
        "            df,\n",
        "            graphs,\n",
        "            masks=masks,\n",
        "            targets_mlm=targets_mlm,\n",
        "            targets_lattice=targets_lattice,\n",
        "            targets_position=targets_position,\n",
        "            target=target,\n",
        "            atom_features=atom_features,\n",
        "            line_graph=line_graph,\n",
        "            id_tag=id_tag,\n",
        "            classification=classification,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            mean_train=mean_train,\n",
        "            std_train=std_train,\n",
        "            pre_train=pre_train\n",
        "        )\n",
        "    else:\n",
        "        data = PygStructureDataset(\n",
        "            df,\n",
        "            graphs,\n",
        "            masks=masks,\n",
        "            targets_mlm=targets_mlm,\n",
        "            targets_lattice=targets_lattice,\n",
        "            targets_position=targets_position,\n",
        "            target=target,\n",
        "            atom_features=atom_features,\n",
        "            line_graph=line_graph,\n",
        "            id_tag=id_tag,\n",
        "            classification=classification,\n",
        "            neighbor_strategy=neighbor_strategy,\n",
        "            mean_train=mean_train,\n",
        "            std_train=std_train,\n",
        "            pre_train=pre_train\n",
        "        )\n",
        "    return data, mean_train, std_train\n",
        "\n",
        "\n",
        "def get_train_val_loaders(\n",
        "    dataset: str = \"dft_3d\",\n",
        "    dataset_array=[],\n",
        "    target: str = \"formation_energy_peratom\",\n",
        "    atom_features: str = \"cgcnn\",\n",
        "    neighbor_strategy: str = \"k-nearest\",\n",
        "    n_train=None,\n",
        "    n_val=None,\n",
        "    n_test=None,\n",
        "    train_ratio=None,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    batch_size: int = 5,\n",
        "    standardize: bool = False,\n",
        "    line_graph: bool = True,\n",
        "    split_seed: int = 123,\n",
        "    workers: int = 0,\n",
        "    pin_memory: bool = True,\n",
        "    save_dataloader: bool = False,\n",
        "    filename: str = \"sample\",\n",
        "    id_tag: str = \"jid\",\n",
        "    use_canonize: bool = False,\n",
        "    cutoff: float = 8.0,\n",
        "    max_neighbors: int = 12,\n",
        "    classification_threshold: Optional[float] = None,\n",
        "    target_multiplication_factor: Optional[float] = None,\n",
        "    standard_scalar_and_pca=False,\n",
        "    keep_data_order=False,\n",
        "    output_features=1,\n",
        "    output_dir=None,\n",
        "    matrix_input=False,\n",
        "    pyg_input=False,\n",
        "    use_lattice=False,\n",
        "    use_angle=False,\n",
        "    use_save=True,\n",
        "    mp_id_list=None,\n",
        "    pre_train=False,\n",
        "    mask_ratio=None,\n",
        "    lattice_noise=None,\n",
        "    position_noise=None,\n",
        "):\n",
        "    \"\"\"Help function to set up JARVIS train and val dataloaders.\"\"\"\n",
        "    # data loading\n",
        "    mean_train=None\n",
        "    std_train=None\n",
        "    assert (matrix_input and pyg_input) == False\n",
        "    #print(\"*******get_train_val_loaders***********\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    train_sample = os.path.join(output_dir, filename + \"_train.data\")\n",
        "    val_sample = os.path.join(output_dir, filename + \"_val.data\")\n",
        "    test_sample = os.path.join(output_dir, filename + \"_test.data\")\n",
        "\n",
        "    if (\n",
        "        os.path.exists(train_sample)\n",
        "        and os.path.exists(val_sample)\n",
        "        and os.path.exists(test_sample)\n",
        "        and save_dataloader\n",
        "    ):\n",
        "        print(\"Loading from saved file...\")\n",
        "        print(\"Make sure all the DataLoader params are same.\")\n",
        "        print(\"This module is made for debugging only.\")\n",
        "        train_loader = torch.load(train_sample)\n",
        "        val_loader = torch.load(val_sample)\n",
        "        test_loader = torch.load(test_sample)\n",
        "        if train_loader.pin_memory != pin_memory:\n",
        "            train_loader.pin_memory = pin_memory\n",
        "        if test_loader.pin_memory != pin_memory:\n",
        "            test_loader.pin_memory = pin_memory\n",
        "        if val_loader.pin_memory != pin_memory:\n",
        "            val_loader.pin_memory = pin_memory\n",
        "        if train_loader.num_workers != workers:\n",
        "            train_loader.num_workers = workers\n",
        "        if test_loader.num_workers != workers:\n",
        "            test_loader.num_workers = workers\n",
        "        if val_loader.num_workers != workers:\n",
        "            val_loader.num_workers = workers\n",
        "        print(\"train\", len(train_loader.dataset))\n",
        "        print(\"val\", len(val_loader.dataset))\n",
        "        print(\"test\", len(test_loader.dataset))\n",
        "        return (\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            test_loader,\n",
        "            train_loader.dataset.prepare_batch,\n",
        "            None,\n",
        "            None\n",
        "        )\n",
        "    else:\n",
        "        if not dataset_array:\n",
        "            d = jdata(dataset)\n",
        "        else:\n",
        "            d = dataset_array\n",
        "            # for ii, i in enumerate(pc_y):\n",
        "            #    d[ii][target] = pc_y[ii].tolist()\n",
        "\n",
        "        dat = []\n",
        "        if classification_threshold is not None:\n",
        "            print(\n",
        "                \"Using \",\n",
        "                classification_threshold,\n",
        "                \" for classifying \",\n",
        "                target,\n",
        "                \" data.\",\n",
        "            )\n",
        "            print(\"Converting target data into 1 and 0.\")\n",
        "        all_targets = []\n",
        "\n",
        "        # TODO:make an all key in qm9_dgl\n",
        "        if dataset == \"qm9_dgl\" and target == \"all\":\n",
        "            print(\"Making all qm9_dgl\")\n",
        "            tmp = []\n",
        "            for ii in d:\n",
        "                ii[\"all\"] = [\n",
        "                    ii[\"mu\"],\n",
        "                    ii[\"alpha\"],\n",
        "                    ii[\"homo\"],\n",
        "                    ii[\"lumo\"],\n",
        "                    ii[\"gap\"],\n",
        "                    ii[\"r2\"],\n",
        "                    ii[\"zpve\"],\n",
        "                    ii[\"U0\"],\n",
        "                    ii[\"U\"],\n",
        "                    ii[\"H\"],\n",
        "                    ii[\"G\"],\n",
        "                    ii[\"Cv\"],\n",
        "                ]\n",
        "                tmp.append(ii)\n",
        "            print(\"Made all qm9_dgl\")\n",
        "            d = tmp\n",
        "        all_targets = []\n",
        "        masks = []\n",
        "        position_gt = []\n",
        "        lattice_gt = []\n",
        "        construct = ConstructTarget(mask_ratio=mask_ratio, position_noise=position_noise, lattice_noise=lattice_noise)\n",
        "        print(\"*******Construct targets***********\")\n",
        "        tc1 = time.time()\n",
        "        if pre_train:\n",
        "            #pandarallel.initialize(progress_bar=True)\n",
        "            pandarallel.initialize()\n",
        "            df = pd.DataFrame(d)\n",
        "            #print(df['atoms'][0]['elements'])\n",
        "            #print(df.head(2))\n",
        "            data_frame_temp = pd.DataFrame(df['atoms'].parallel_apply(construct.forward))\n",
        "            data_frame_temp['jid'] = df['jid']\n",
        "            data_frame_temp['target'] = df['target']\n",
        "            #print(data_frame_temp.head(2))\n",
        "            data_frame = data_frame_temp[data_frame_temp['use_this']==True].reindex()\n",
        "            #for i in range(10):\n",
        "            #    print(len(data_frame['mask'][i]), len(data_frame['atoms'][i][\"elements\"]))\n",
        "            #print(data_frame.head(2))\n",
        "            dat = data_frame.to_dict('records')\n",
        "            #dat = list(data_frame['atoms_noised'].values)\n",
        "            del data_frame_temp\n",
        "            gc.collect()\n",
        "        tc2 = time.time()\n",
        "        print(f\"*******Construct targets done{tc2-tc1}s***********\")\n",
        "        '''\n",
        "        for i in tqdm(d):\n",
        "            if pre_train:\n",
        "                if mask_ratio is not None:\n",
        "                    if len(i[\"atoms\"][\"elements\"]) < 2:\n",
        "                        continue\n",
        "                    num_to_predict = max(1, math.ceil(len(i[\"atoms\"][\"elements\"])*mask_ratio))\n",
        "                    sampled_indices = random.sample(range(len(i[\"atoms\"][\"elements\"])), num_to_predict)\n",
        "                    atoms = i[\"atoms\"][\"elements\"]\n",
        "                    mask = torch.zeros((len(atoms)))\n",
        "                    z = []\n",
        "                    for ele in i[\"atoms\"][\"elements\"]:\n",
        "                        z.append(chemical_symbols.index(ele))\n",
        "                    for sampled_index in sampled_indices:\n",
        "                        i[\"atoms\"][\"elements\"][sampled_index] = \"X\"\n",
        "                        mask[sampled_index] = 1\n",
        "                    all_targets.append(torch.tensor(np.array(z)))\n",
        "                    masks.append(mask)\n",
        "                if position_noise is not None:\n",
        "                    position_gt.append(torch.tensor(np.array(i['atoms']['coords'])))\n",
        "                    position_noised = []\n",
        "                    for position in i['atoms']['coords']:\n",
        "                        position_noised.append(((np.array(position)+np.random.rand(3)*position_noise)%1.0).tolist())\n",
        "                    i['atoms']['coords'] = position_noised\n",
        "                if lattice_noise is not None:\n",
        "                    lattice_gt.append(torch.tensor(np.array(i['atoms']['lattice_mat'])))\n",
        "                    lattice_noised = []\n",
        "                    for lat_vec in i['atoms']['lattice_mat']:\n",
        "                        lattice_noised.append(((np.array(lat_vec)+np.random.rand(3)*lattice_noise)).tolist())\n",
        "                    i['atoms']['lattice_mat'] =  lattice_noised\n",
        "                dat.append(i)\n",
        "            elif isinstance(i[target], list):  # multioutput target\n",
        "                all_targets.append(torch.tensor(i[target]))\n",
        "                dat.append(i)\n",
        "\n",
        "            elif (\n",
        "                i[target] is not None\n",
        "                and i[target] != \"na\"\n",
        "                and not math.isnan(i[target])\n",
        "            ):\n",
        "                if target_multiplication_factor is not None:\n",
        "                    i[target] = i[target] * target_multiplication_factor\n",
        "                if classification_threshold is not None:\n",
        "                    if i[target] <= classification_threshold:\n",
        "                        i[target] = 0\n",
        "                    elif i[target] > classification_threshold:\n",
        "                        i[target] = 1\n",
        "                    else:\n",
        "                        raise ValueError(\n",
        "                            \"Check classification data type.\",\n",
        "                            i[target],\n",
        "                            type(i[target]),\n",
        "                        )\n",
        "                dat.append(i)\n",
        "                all_targets.append(i[target])\n",
        "        '''\n",
        "\n",
        "    if mp_id_list is not None:\n",
        "        if mp_id_list == 'bulk':\n",
        "            print('using mp bulk dataset')\n",
        "            with open('./data/bulk_megnet_train.pkl', 'rb') as f:\n",
        "                dataset_train = pk.load(f)\n",
        "            with open('./data/bulk_megnet_val.pkl', 'rb') as f:\n",
        "                dataset_val = pk.load(f)\n",
        "            with open('./data/bulk_megnet_test.pkl', 'rb') as f:\n",
        "                dataset_test = pk.load(f)\n",
        "\n",
        "        if mp_id_list == 'shear':\n",
        "            print('using mp shear dataset')\n",
        "            with open('./data/shear_megnet_train.pkl', 'rb') as f:\n",
        "                dataset_train = pk.load(f)\n",
        "            with open('./data/shear_megnet_val.pkl', 'rb') as f:\n",
        "                dataset_val = pk.load(f)\n",
        "            with open('./data/shear_megnet_test.pkl', 'rb') as f:\n",
        "                dataset_test = pk.load(f)\n",
        "\n",
        "    else:\n",
        "        id_train, id_val, id_test = get_id_train_val_test(\n",
        "            total_size=len(dat),\n",
        "            split_seed=split_seed,\n",
        "            train_ratio=train_ratio,\n",
        "            val_ratio=val_ratio,\n",
        "            test_ratio=test_ratio,\n",
        "            n_train=n_train,\n",
        "            n_test=n_test,\n",
        "            n_val=n_val,\n",
        "            keep_data_order=keep_data_order,\n",
        "        )\n",
        "        ids_train_val_test = {}\n",
        "        ids_train_val_test[\"id_train\"] = [dat[i][id_tag] for i in id_train]\n",
        "        ids_train_val_test[\"id_val\"] = [dat[i][id_tag] for i in id_val]\n",
        "        ids_train_val_test[\"id_test\"] = [dat[i][id_tag] for i in id_test]\n",
        "        dumpjson(\n",
        "            data=ids_train_val_test,\n",
        "            filename=os.path.join(output_dir, \"ids_train_val_test.json\"),\n",
        "        )\n",
        "        dataset_train = [dat[x] for x in id_train]\n",
        "        dataset_val = [dat[x] for x in id_val]\n",
        "        dataset_test = [dat[x] for x in id_test]\n",
        "    if pre_train:\n",
        "        if mask_ratio is not None:\n",
        "            print(\"Get masks\")\n",
        "            masks = data_frame['mask'].values\n",
        "            print(\"Get atom types\")\n",
        "            all_targets = data_frame['target_all'].values\n",
        "            masks_train = masks[id_train]\n",
        "            masks_val = masks[id_val]\n",
        "            masks_test = masks[id_test]\n",
        "            targets_train = all_targets[id_train]\n",
        "            targets_val = all_targets[id_val]\n",
        "            targets_test = all_targets[id_test]\n",
        "        else:\n",
        "            targets_train = None\n",
        "            targets_val = None\n",
        "            targets_test = None\n",
        "            masks_train = None\n",
        "            masks_val = None\n",
        "            masks_test = None\n",
        "\n",
        "        if position_noise is not None:\n",
        "            print(\"Get position GT\")\n",
        "            position_gt = data_frame['position_gt'].values\n",
        "            position_gt_train = position_gt[id_train]\n",
        "            position_gt_val = position_gt[id_val]\n",
        "            position_gt_test = position_gt[id_test]\n",
        "        else:\n",
        "            position_gt_train = None\n",
        "            position_gt_val = None\n",
        "            position_gt_test = None\n",
        "\n",
        "        if lattice_noise is not None:\n",
        "            print(\"Get lattice gt\")\n",
        "            lattice_gt = data_frame['lattice_gt'].values\n",
        "            lattice_gt_train = lattice_gt[id_train]\n",
        "            lattice_gt_val = lattice_gt[id_val]\n",
        "            lattice_gt_test = lattice_gt[id_test]\n",
        "        else:\n",
        "            lattice_gt_train = None\n",
        "            lattice_gt_val = None\n",
        "            lattice_gt_test = None\n",
        "        '''\n",
        "        if mask_ratio is not None:\n",
        "            print(\"Get masks\")\n",
        "            masks = list(data_frame['mask'].values)\n",
        "            print(\"Get atom types\")\n",
        "            all_targets = list(data_frame['target'].values)\n",
        "            masks_train = [masks[x] for x in id_train]\n",
        "            masks_val = [masks[x] for x in id_val]\n",
        "            masks_test = [masks[x] for x in id_test]\n",
        "            targets_train = [all_targets[x] for x in id_train]\n",
        "            targets_val = [all_targets[x] for x in id_val]\n",
        "            targets_test = [all_targets[x] for x in id_test]\n",
        "        else:\n",
        "            targets_train = None\n",
        "            targets_val = None\n",
        "            targets_test = None\n",
        "            masks_train = None\n",
        "            masks_val = None\n",
        "            masks_test = None\n",
        "\n",
        "        if position_noise is not None:\n",
        "            print(\"Get position GT\")\n",
        "            position_gt = list(data_frame['position_gt'].values)\n",
        "            position_gt_train = [position_gt[x] for x in id_train]\n",
        "            position_gt_val = [position_gt[x] for x in id_val]\n",
        "            position_gt_test = [position_gt[x] for x in id_test]\n",
        "        else:\n",
        "            position_gt_train = None\n",
        "            position_gt_val = None\n",
        "            position_gt_test = None\n",
        "\n",
        "        if lattice_noise is not None:\n",
        "            print(\"Get lattice gt\")\n",
        "            lattice_gt = list(data_frame['lattice_gt'].values)\n",
        "            lattice_gt_train = [lattice_gt[x] for x in id_train]\n",
        "            lattice_gt_val = [lattice_gt[x] for x in id_val]\n",
        "            lattice_gt_test = [lattice_gt[x] for x in id_test]\n",
        "        else:\n",
        "            lattice_gt_train = None\n",
        "            lattice_gt_val = None\n",
        "            lattice_gt_test = None\n",
        "        '''\n",
        "\n",
        "    if pre_train:\n",
        "        pass\n",
        "    elif standard_scalar_and_pca:\n",
        "        y_data = [i[target] for i in dataset_train]\n",
        "        # pipe = Pipeline([('scale', StandardScaler())])\n",
        "        if not isinstance(y_data[0], list):\n",
        "            print(\"Running StandardScalar\")\n",
        "            y_data = np.array(y_data).reshape(-1, 1)\n",
        "        sc = StandardScaler()\n",
        "\n",
        "        sc.fit(y_data)\n",
        "        print(\"Mean\", sc.mean_)\n",
        "        print(\"Variance\", sc.var_)\n",
        "        try:\n",
        "            print(\"New max\", max(y_data))\n",
        "            print(\"New min\", min(y_data))\n",
        "        except Exception as exp:\n",
        "            print(exp)\n",
        "            pass\n",
        "\n",
        "        pk.dump(sc, open(os.path.join(output_dir, \"sc.pkl\"), \"wb\"))\n",
        "\n",
        "    if classification_threshold is None:\n",
        "        try:\n",
        "            from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "            print(\"MAX val:\", max(all_targets))\n",
        "            print(\"MIN val:\", min(all_targets))\n",
        "            print(\"MAD:\", mean_absolute_deviation(all_targets))\n",
        "            try:\n",
        "                f = open(os.path.join(output_dir, \"mad\"), \"w\")\n",
        "                line = \"MAX val:\" + str(max(all_targets)) + \"\\n\"\n",
        "                line += \"MIN val:\" + str(min(all_targets)) + \"\\n\"\n",
        "                line += (\n",
        "                    \"MAD val:\"\n",
        "                    + str(mean_absolute_deviation(all_targets))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "                f.write(line)\n",
        "                f.close()\n",
        "            except Exception as exp:\n",
        "                print(\"Cannot write mad\", exp)\n",
        "                pass\n",
        "            # Random model precited value\n",
        "            x_bar = np.mean(np.array([i[target] for i in dataset_train]))\n",
        "            baseline_mae = mean_absolute_error(\n",
        "                np.array([i[target] for i in dataset_test]),\n",
        "                np.array([x_bar for i in dataset_test]),\n",
        "            )\n",
        "            print(\"Baseline MAE:\", baseline_mae)\n",
        "        except Exception as exp:\n",
        "            print(\"Data error\", exp)\n",
        "            pass\n",
        "\n",
        "    train_data, mean_train, std_train = get_pyg_dataset(\n",
        "        dataset=dataset_train,\n",
        "        masks=masks_train,\n",
        "        targets_mlm= targets_train,\n",
        "        targets_position = position_gt_train,\n",
        "        targets_lattice = lattice_gt_train,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"train_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "        pre_train=pre_train,\n",
        "    )\n",
        "    val_data,_,_ = get_pyg_dataset(\n",
        "        dataset=dataset_val,\n",
        "        masks=masks_val,\n",
        "        targets_mlm= targets_val,\n",
        "        targets_position = position_gt_val,\n",
        "        targets_lattice = lattice_gt_val,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"val_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "        mean_train=mean_train,\n",
        "        std_train=std_train,\n",
        "        pre_train=pre_train,\n",
        "    )\n",
        "    test_data,_,_ = get_pyg_dataset(\n",
        "        dataset=dataset_test,\n",
        "        masks = masks_test,\n",
        "        targets_mlm= targets_test,\n",
        "        targets_position = position_gt_test,\n",
        "        targets_lattice = lattice_gt_test,\n",
        "        id_tag=id_tag,\n",
        "        atom_features=atom_features,\n",
        "        target=target,\n",
        "        neighbor_strategy=neighbor_strategy,\n",
        "        use_canonize=use_canonize,\n",
        "        name=dataset,\n",
        "        line_graph=line_graph,\n",
        "        cutoff=cutoff,\n",
        "        max_neighbors=max_neighbors,\n",
        "        classification=classification_threshold is not None,\n",
        "        output_dir=output_dir,\n",
        "        tmp_name=\"test_data\",\n",
        "        use_lattice=use_lattice,\n",
        "        use_angle=use_angle,\n",
        "        use_save=False,\n",
        "        mean_train=mean_train,\n",
        "        std_train=std_train,\n",
        "        pre_train=pre_train\n",
        "    )\n",
        "\n",
        "    collate_fn = train_data.collate\n",
        "    if pre_train and line_graph:\n",
        "        collate_fn = train_data.collate_line_graph_pretrain\n",
        "    elif line_graph:\n",
        "        collate_fn = train_data.collate_line_graph\n",
        "    return (train_data, val_data, test_data, collate_fn, pin_memory)\n",
        "    # use a regular pytorch dataloader\n",
        "\n"
      ],
      "metadata": {
        "id": "BV3FEf1FgNWE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pyg_att.-py\n"
      ],
      "metadata": {
        "id": "HHXmPeolLqfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
        "\n",
        "from typing import Tuple, Optional, List\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pydantic.typing import Literal\n",
        "from torch import nn\n",
        "from matformer.models.utils import RBFExpansion\n",
        "from matformer.features import angle_emb_mp\n",
        "from torch_scatter import scatter\n",
        "from matformer.models.transformer import MatformerConv\n",
        "from pydantic import BaseSettings as PydanticBaseSettings\n",
        "\n",
        "\n",
        "class BaseSettings(PydanticBaseSettings):\n",
        "    \"\"\"Add configuration to default Pydantic BaseSettings.\"\"\"\n",
        "    class Config:\n",
        "        extra = \"forbid\"\n",
        "        use_enum_values = True\n",
        "        env_prefix = \"jv_\"\n",
        "\n",
        "\n",
        "class MatformerConfig(BaseSettings):\n",
        "    \"\"\"Hyperparameter schema for Matformer model.\"\"\"\n",
        "    name: Literal[\"matformer\"]\n",
        "    conv_layers: int = 5\n",
        "    edge_layers: int = 0\n",
        "    atom_input_features: int = 92\n",
        "    edge_features: int = 128\n",
        "    triplet_input_features: int = 40\n",
        "    node_features: int = 128\n",
        "    fc_layers: int = 1\n",
        "    fc_features: int = 128\n",
        "    output_features: int = 1\n",
        "    node_layer_head: int = 4\n",
        "    edge_layer_head: int = 4\n",
        "    nn_based: bool = False\n",
        "    link: Literal[\"identity\", \"log\", \"logit\"] = \"identity\"\n",
        "    zero_inflated: bool = False\n",
        "    use_angle: bool = False\n",
        "    angle_lattice: bool = False\n",
        "    classification: bool = False\n",
        "    pre_train: bool = False\n",
        "    position_noise: Optional[float] = None\n",
        "    lattice_noise: Optional[float] = None\n",
        "    mask_ratio: Optional[float] = None\n",
        "\n",
        "    class Config:\n",
        "        env_prefix = \"jv_model\"\n",
        "\n",
        "\n",
        "class Matformer(nn.Module):\n",
        "    \"\"\"Matformer: A transformer-based model for materials property prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, config: MatformerConfig = MatformerConfig(name=\"matformer\")):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.classification = config.classification\n",
        "        self.use_angle = config.use_angle\n",
        "        self.pre_train = config.pre_train\n",
        "        self.zero_inflated = config.zero_inflated\n",
        "        self.mask_ratio = config.mask_ratio is not None\n",
        "        self.position_noise = config.position_noise is not None\n",
        "        self.lattice_noise = config.lattice_noise is not None\n",
        "\n",
        "        # Atom embedding layer\n",
        "        self.atom_embedding = nn.Linear(\n",
        "            config.atom_input_features, config.node_features\n",
        "        )\n",
        "\n",
        "        # Radial basis function expansion for edge features\n",
        "        self.rbf = nn.Sequential(\n",
        "            RBFExpansion(vmin=0, vmax=8.0, bins=config.edge_features),\n",
        "            nn.Linear(config.edge_features, config.node_features),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(config.node_features, config.node_features),\n",
        "        )\n",
        "\n",
        "        # Attention layers\n",
        "        self.att_layers = nn.ModuleList([\n",
        "            MatformerConv(\n",
        "                in_channels=config.node_features,\n",
        "                out_channels=config.node_features,\n",
        "                heads=config.node_layer_head,\n",
        "                edge_dim=config.node_features\n",
        "            ) for _ in range(config.conv_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(config.node_features, config.fc_features),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "        self.fc_out = nn.Linear(config.fc_features, config.output_features)\n",
        "\n",
        "        # Link function\n",
        "        self.link_name = config.link\n",
        "        if config.link == \"identity\":\n",
        "            self.link = lambda x: x\n",
        "        elif config.link == \"log\":\n",
        "            self.link = torch.exp\n",
        "            if not self.zero_inflated:\n",
        "                self.fc_out.bias.data = torch.tensor(np.log(0.7), dtype=torch.float)\n",
        "        elif config.link == \"logit\":\n",
        "            self.link = torch.sigmoid\n",
        "\n",
        "        # Classification-specific layers\n",
        "        if self.classification:\n",
        "            self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        # Pretraining-specific layers\n",
        "        if self.mask_ratio:\n",
        "            self.mlm_pred = nn.Linear(config.node_features, 119)\n",
        "            self.softmax_mlm = nn.LogSoftmax(dim=-1)\n",
        "        if self.position_noise:\n",
        "            self.position_mlp = nn.Linear(config.node_features, 3)\n",
        "        if self.lattice_noise:\n",
        "            self.lattice_mlp = nn.Linear(config.node_features, 9)\n",
        "\n",
        "    def forward(self, data_tuple: Tuple) -> torch.Tensor:\n",
        "\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "\n",
        "        \"\"\"\n",
        "        Forward pass of the Matformer model.\n",
        "\n",
        "        Args:\n",
        "            data_tuple: Tuple containing:\n",
        "                - data: PyG Data object with node features and edge indices\n",
        "                - ldata: Line graph data (unused in current implementation)\n",
        "                - lattice: Lattice information (unused in current implementation)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Model predictions\n",
        "        \"\"\"\n",
        "        data, _, _ = data_tuple  # Unpack the data tuple\n",
        "\n",
        "        # Atom feature embedding\n",
        "        x = self.atom_embedding(data.x)\n",
        "        atom_feature_avg = torch.mean(x, dim=0, keepdim=True)\n",
        "        x_list.append(atom_feature_avg)\n",
        "        print(\"\\n[Atom Features] shape:\", x.shape)\n",
        "        print(\"[Atom Features] sample:\\n\", x[:5])\n",
        "\n",
        "        # Edge feature processing\n",
        "        bondlength = torch.norm(data.edge_attr, dim=1)\n",
        "        print(\"\\n[Bond Lengths] shape:\", bondlength.shape)\n",
        "        print(\"[Bond Lengths] sample:\\n\", bondlength[:10])\n",
        "        y = self.rbf(bondlength)\n",
        "        bond_feature_avg = torch.mean(y, dim=0, keepdim=True)\n",
        "        y_list.append(bond_feature_avg)\n",
        "        print(\"\\n[Bond Features] shape:\", y.shape)\n",
        "        print(\"[Bond Features] sample:\\n\", y[:5])\n",
        "\n",
        "        # Attention layers\n",
        "        for layer in self.att_layers:\n",
        "            x = layer(x, data.edge_index, y)\n",
        "\n",
        "        # Graph-level readout\n",
        "        features = scatter(x, data.batch, dim=0, reduce=\"mean\")\n",
        "        features = self.fc(features)\n",
        "        out = self.fc_out(features)\n",
        "\n",
        "        # Apply link function if specified\n",
        "        if hasattr(self, 'link') and self.link:\n",
        "            out = self.link(out)\n",
        "\n",
        "        # Classification-specific processing\n",
        "        if self.classification:\n",
        "            out = self.softmax(out)\n",
        "\n",
        "        return out.squeeze(-1),x_list,y_list  # Remove last dimension if output_features=1\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    config = MatformerConfig(\n",
        "        name=\"matformer\",\n",
        "        atom_input_features=92,\n",
        "        node_features=128,\n",
        "        output_features=1\n",
        "    )\n",
        "    model = Matformer(config)\n",
        "    print(f\"Model architecture:\\n{model}\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "377aGCHBNl6f",
        "outputId": "d6307fdd-2268-4195-d027-2487384a6d89"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model architecture:\n",
            "Matformer(\n",
            "  (atom_embedding): Linear(in_features=92, out_features=128, bias=True)\n",
            "  (rbf): Sequential(\n",
            "    (0): RBFExpansion()\n",
            "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (2): Softplus(beta=1, threshold=20)\n",
            "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (att_layers): ModuleList(\n",
            "    (0-4): 5 x MatformerConv(128, 128, heads=4)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (1): SiLU()\n",
            "  )\n",
            "  (fc_out): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 2,782,849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    config = MatformerConfig(\n",
        "        name=\"matformer\",\n",
        "        atom_input_features=92,\n",
        "        node_features=128,\n",
        "        output_features=1\n",
        "    )\n",
        "    model = Matformer(config)\n",
        "    print(f\"Model architecture:\\n{model}\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Create dummy input data\n",
        "    from torch_geometric.data import Data\n",
        "\n",
        "    # Example with 4 atoms and 4 bonds\n",
        "    num_atoms = 4\n",
        "    num_bonds = 4\n",
        "\n",
        "    # Atom features (92-dimensional)\n",
        "    x = torch.randn(num_atoms, config.atom_input_features)\n",
        "\n",
        "    # Edge indices (bonds between atoms)\n",
        "    edge_index = torch.tensor([[0, 1, 2, 3],  # Source nodes\n",
        "                              [1, 2, 3, 0]], # Target nodes\n",
        "                             dtype=torch.long)\n",
        "\n",
        "    # Edge attributes (3D vectors representing bond vectors)\n",
        "    edge_attr = torch.randn(num_bonds, 3)\n",
        "\n",
        "    # Batch indices (all atoms belong to same graph)\n",
        "    batch = torch.zeros(num_atoms, dtype=torch.long)\n",
        "\n",
        "    # Create PyG Data object\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, batch=batch)\n",
        "\n",
        "    # Create the input tuple expected by forward()\n",
        "    data_tuple = (data, None, None)  # (graph_data, line_graph_data, lattice_data)\n",
        "\n",
        "    # Call the forward pass\n",
        "    output, atom_features, bond_features = model.forward(data_tuple)\n",
        "\n",
        "    # Print the collected features\n",
        "    print(\"\\n=== Collected Features ===\")\n",
        "    print(\"\\nAtom features (averaged):\")\n",
        "    print(atom_features[0].shape)  # Should be [1, 128]\n",
        "    print(atom_features[0])\n",
        "\n",
        "    print(\"\\nBond features (averaged):\")\n",
        "    print(bond_features[0].shape)  # Should be [1, 128]\n",
        "    print(bond_features[0])\n",
        "\n",
        "    # Note: If you want to access the actual bond lengths, you'll need to modify\n",
        "    # the forward() method to return them as well (add bondlength to return tuple)"
      ],
      "metadata": {
        "id": "6mMo0Ihksu3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Matformer(config)\n",
        "\n",
        "\n",
        "out, atom_embeds, bond_embeds = model((data, None, None))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WzJfuBGQy4i",
        "outputId": "af489ab4-11e8-47e3-ef5f-44757b871ccd"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Atom Features] shape: torch.Size([4, 128])\n",
            "[Atom Features] sample:\n",
            " tensor([[-1.7811e-02, -3.8489e-01,  1.1573e-01, -4.1183e-01, -7.2481e-01,\n",
            "          9.5015e-01,  9.2779e-01, -4.3211e-01,  1.0670e+00,  8.5227e-02,\n",
            "          4.7383e-01,  6.7563e-01, -3.6556e-01, -5.6806e-01, -3.2215e-01,\n",
            "         -6.2905e-01,  2.7742e-01,  1.0455e+00, -6.3027e-01, -6.9554e-01,\n",
            "          2.2431e-01, -4.4384e-01, -8.4471e-01, -4.6448e-02, -4.9980e-01,\n",
            "         -7.5410e-01, -8.0779e-01,  2.7518e-01,  2.3975e-01,  4.2234e-01,\n",
            "         -6.3257e-01,  3.1074e-01, -5.0124e-01,  1.0280e+00,  3.1776e-01,\n",
            "         -5.5841e-01,  2.5053e-01,  2.9950e-01,  2.9211e-01,  6.9621e-01,\n",
            "         -1.5181e-01, -1.2056e-01,  6.8484e-02,  3.4270e-01,  6.7564e-03,\n",
            "         -9.4143e-01,  3.3730e-02,  9.1563e-02, -1.9037e-01,  5.0819e-02,\n",
            "          4.8274e-01, -8.1192e-01,  5.8583e-02, -1.2357e-01,  2.4800e-01,\n",
            "          7.0231e-01,  3.4856e-01,  3.8016e-01,  8.9545e-01, -7.0316e-01,\n",
            "          6.5440e-01,  5.8041e-02,  3.9808e-01, -1.2673e-01,  5.4321e-01,\n",
            "          3.9510e-01,  3.9886e-01, -2.1616e-01,  9.0450e-01, -1.1524e-01,\n",
            "         -8.8906e-01,  6.7944e-01, -1.3307e+00,  1.5925e-01,  1.8503e-01,\n",
            "          7.7277e-01,  6.7627e-03, -8.2848e-01, -2.6155e-01, -9.2414e-01,\n",
            "         -2.3684e-01,  6.6070e-01, -8.4646e-02,  6.5099e-01, -8.1823e-01,\n",
            "          1.4536e+00, -5.8755e-01,  2.5518e-01,  2.8957e-02,  7.8542e-01,\n",
            "         -3.0185e-01, -1.0001e+00, -3.2696e-01, -7.6581e-02,  2.7971e-01,\n",
            "         -5.6642e-04, -2.3394e-01,  5.5253e-01, -6.3530e-01, -5.0809e-02,\n",
            "          9.5803e-01, -1.5882e-01,  1.8422e-01,  1.0388e-01,  7.4698e-01,\n",
            "          1.4138e-01,  6.9815e-01, -9.1192e-01, -5.8207e-03,  6.3985e-01,\n",
            "          1.1842e-01, -1.1897e+00, -1.2098e+00,  5.8046e-01, -8.3094e-02,\n",
            "          3.1911e-02, -1.5269e-01,  2.5226e-01,  1.2478e-01,  1.4225e-01,\n",
            "          1.1263e-01, -8.2434e-01, -3.2030e-01, -2.1589e-01,  1.3571e-01,\n",
            "          4.5558e-01, -5.3495e-01, -5.3803e-01],\n",
            "        [-3.7826e-01,  1.1831e+00, -1.7621e-01, -9.1149e-01, -5.9853e-02,\n",
            "          3.9016e-01, -5.2281e-01,  9.7581e-01,  2.6778e-01, -9.2082e-01,\n",
            "         -1.9101e-01, -9.3779e-01,  1.1875e-01,  3.3862e-02, -4.1098e-01,\n",
            "          2.3380e-01,  2.1097e-01,  4.3464e-01,  4.4484e-01, -1.4107e-01,\n",
            "          3.6156e-02,  3.3488e-01, -6.5435e-01,  2.3706e-01,  5.1450e-01,\n",
            "          4.0327e-01,  1.3523e-01,  1.4594e-01,  4.5011e-01, -7.9182e-02,\n",
            "         -5.9205e-01, -9.8635e-01,  2.6674e-02,  8.3215e-01,  1.5326e-01,\n",
            "          1.8034e-01, -4.4651e-01,  3.4009e-01,  4.7743e-01, -5.0475e-01,\n",
            "          6.7076e-01,  6.9610e-02,  3.2597e-01,  7.2095e-01, -6.8743e-01,\n",
            "          6.7966e-02, -4.6561e-01,  7.6962e-01,  6.9978e-01, -1.0315e+00,\n",
            "         -1.2531e+00,  9.7740e-01, -5.5078e-01,  5.4230e-02,  7.4028e-02,\n",
            "         -5.3922e-01,  2.6848e-01,  3.4670e-02,  1.1180e-01, -8.4354e-01,\n",
            "          1.6723e-01,  9.3918e-01,  1.4638e+00,  3.9309e-01, -9.7917e-02,\n",
            "          1.9256e-01, -2.8091e-01, -2.7360e-01,  6.2239e-01,  2.0680e-02,\n",
            "          1.2533e+00,  2.6375e-01, -2.1270e-01,  1.1288e-01,  9.9101e-02,\n",
            "          1.7938e-01, -2.8406e-02, -3.1164e-02,  8.4812e-01, -3.1802e-01,\n",
            "         -3.0150e-02, -8.5208e-02, -1.2135e+00, -4.2839e-01,  2.0888e-01,\n",
            "          2.0409e-01, -2.1438e-01, -1.0357e+00,  1.1093e-01,  6.3601e-01,\n",
            "         -9.2764e-01, -6.0733e-01,  4.5770e-01, -2.1051e-01,  8.6447e-01,\n",
            "          3.2812e-01, -5.7475e-02, -4.7251e-01,  1.0289e-01,  4.7436e-01,\n",
            "          8.5288e-01,  7.1638e-01,  8.3845e-01, -9.5240e-02, -1.1539e-01,\n",
            "         -3.1935e-01, -3.1739e-01,  3.5933e-01, -8.9292e-01, -4.9948e-01,\n",
            "          5.1100e-01,  2.9633e-01,  8.3644e-01,  5.0573e-01,  2.7558e-01,\n",
            "          2.4469e-01,  1.1155e-01,  1.9333e-02, -4.1956e-01,  7.8895e-01,\n",
            "          3.8037e-01, -6.7992e-01,  1.7198e-01, -5.5500e-01, -1.1062e-01,\n",
            "         -1.1000e-01,  3.4122e-01,  3.6094e-01],\n",
            "        [ 1.0078e+00, -5.9989e-01,  5.2160e-01, -7.3691e-01,  9.9967e-02,\n",
            "          8.8172e-01, -1.8069e-01,  1.7070e+00, -8.2054e-01, -4.9406e-01,\n",
            "         -1.0025e+00,  9.1441e-02, -3.2538e-01, -7.3386e-01, -1.4106e-01,\n",
            "          5.1237e-01,  6.6251e-01,  1.4596e-01, -3.1223e-01, -4.8901e-01,\n",
            "          1.5524e+00,  2.5025e-02, -6.7742e-01,  9.6954e-02, -2.3687e-01,\n",
            "         -1.0533e-02, -9.3091e-02, -1.0091e+00,  7.9224e-02, -1.2081e+00,\n",
            "         -7.2311e-01, -2.3855e-01,  2.4870e-01,  1.7948e-02, -1.1800e+00,\n",
            "          2.1038e-01,  4.4479e-01, -9.5802e-02, -2.4238e-01,  1.9932e-01,\n",
            "         -4.9386e-02, -5.3942e-01,  4.9608e-01,  1.7122e-01, -1.5647e-02,\n",
            "         -3.9970e-01, -2.4229e-01,  1.0047e+00,  2.4476e-03, -2.6460e-01,\n",
            "         -8.4813e-01,  3.7302e-01,  5.2759e-01, -2.0257e-04, -1.3888e-01,\n",
            "          3.8152e-01, -7.2494e-01,  2.7429e-01,  6.6624e-01,  5.8503e-01,\n",
            "          6.0559e-02,  2.5982e-01, -3.1485e-01, -3.8239e-01, -3.1697e-02,\n",
            "         -5.8637e-02,  2.5013e-01,  4.8643e-01, -2.7920e-01, -6.5619e-01,\n",
            "         -5.8393e-01, -3.4598e-01,  1.3954e+00,  1.3475e-01,  1.1027e-01,\n",
            "         -2.9882e-01,  7.6542e-01, -3.1577e-01,  2.8423e-01,  3.1079e-01,\n",
            "         -2.9112e-01, -1.1099e-01,  4.0974e-01,  5.5959e-01,  8.2252e-01,\n",
            "         -4.6592e-01, -1.6425e-02, -1.9596e-02,  1.9784e-01,  2.1973e-01,\n",
            "         -1.6046e-01,  1.9744e-01,  1.0906e-01,  8.8748e-01,  1.0659e-01,\n",
            "          1.1285e+00, -2.4626e-01,  1.9817e-01,  7.6196e-01,  1.4777e+00,\n",
            "          2.1367e+00,  1.2812e-01, -3.8961e-01,  3.5111e-01, -6.3347e-01,\n",
            "          4.3368e-01,  3.6534e-01, -6.8437e-01,  5.6762e-02,  1.4898e+00,\n",
            "          6.0898e-01,  7.8106e-01, -5.1574e-01,  3.2559e-01,  5.7541e-01,\n",
            "          9.5975e-01,  1.6632e-01,  1.2885e-01, -2.4038e-01, -4.3374e-01,\n",
            "          4.6531e-01, -2.9975e-01, -1.5574e-01, -1.2712e+00,  6.6431e-01,\n",
            "         -4.7641e-01,  8.2608e-01,  4.9319e-01],\n",
            "        [-6.2367e-01, -8.5303e-01, -8.5168e-01, -1.2195e-02,  4.6809e-01,\n",
            "         -1.0326e+00,  8.9826e-01, -9.3594e-01, -9.1072e-02,  6.0564e-01,\n",
            "          5.5002e-01, -1.0623e+00,  2.4246e-01, -1.0604e-01, -3.0257e-01,\n",
            "          4.6188e-01,  1.1132e-01, -7.5511e-01, -1.1638e-01,  6.4128e-01,\n",
            "          8.0887e-01, -8.0387e-01,  7.4651e-01, -5.7325e-02,  1.8854e-01,\n",
            "          6.4901e-01,  2.4481e-01, -8.8945e-01,  4.0033e-01, -1.1307e-01,\n",
            "          1.2326e+00,  7.8011e-01, -1.4930e-01, -5.7619e-01, -7.6873e-02,\n",
            "         -2.2867e-01,  4.5740e-01, -3.6328e-02,  5.1711e-01,  7.7624e-02,\n",
            "         -5.5336e-01, -5.0871e-01, -2.3931e-02, -9.8501e-01, -6.9562e-01,\n",
            "          4.1322e-01,  6.2932e-01, -1.5107e-01,  4.5086e-01, -1.3574e-01,\n",
            "          3.4946e-01,  4.4986e-01,  5.9054e-02, -8.6492e-01, -1.0848e-03,\n",
            "         -5.3425e-02,  6.4687e-01,  2.5420e-01,  8.1288e-01,  6.0533e-01,\n",
            "         -1.8218e-01, -3.4330e-01,  1.8293e-01,  3.4460e-02,  4.9369e-01,\n",
            "         -7.5712e-01, -2.8005e-01, -2.2908e-01, -9.4634e-01,  2.3094e-01,\n",
            "         -6.2116e-01,  3.8696e-01, -2.9305e-01, -1.6336e-01,  2.1073e-02,\n",
            "         -1.7751e-01, -4.1930e-01, -9.4219e-01,  5.6478e-01, -2.7191e-01,\n",
            "          2.4823e-02, -2.7363e-03,  5.9411e-01, -5.3738e-01,  1.2975e-01,\n",
            "          3.9206e-03,  1.1413e-02,  2.4190e-01,  2.9338e-01, -4.0633e-01,\n",
            "         -2.6949e-01,  7.3019e-01, -7.4221e-01, -5.3561e-01, -1.0203e+00,\n",
            "         -3.5697e-01, -8.3660e-02, -4.2870e-02,  2.0363e-01, -4.5066e-02,\n",
            "         -1.5690e-02, -1.7644e-01, -5.3293e-01,  1.4128e-01, -9.7165e-01,\n",
            "          7.5408e-01, -5.3118e-01,  1.1352e-01,  2.5012e-01,  7.5241e-02,\n",
            "          1.9109e-02,  5.7471e-01, -7.2505e-01, -3.7978e-02,  4.8892e-01,\n",
            "         -1.3938e-01,  1.0733e-01,  1.1677e-01, -3.9037e-01, -3.0092e-01,\n",
            "         -7.0268e-01, -1.9104e-01,  5.7772e-01,  6.9519e-01,  2.4337e-01,\n",
            "          8.0726e-01,  2.0843e-01,  3.0598e-01]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "[Bond Lengths] shape: torch.Size([4])\n",
            "[Bond Lengths] sample:\n",
            " tensor([1.7784, 1.7305, 2.1667, 2.2350])\n",
            "\n",
            "[Bond Features] shape: torch.Size([4, 128])\n",
            "[Bond Features] sample:\n",
            " tensor([[-0.1037, -0.2098,  0.4544, -0.3249,  0.0989, -0.1828, -0.7469, -0.5102,\n",
            "          0.1934,  0.5643, -0.1739, -0.0464, -0.2061,  0.0431,  0.5262, -0.8114,\n",
            "         -0.4184,  0.4978, -0.8409, -0.0988, -0.0826,  0.2036, -0.3274, -0.4515,\n",
            "         -0.4987, -0.1213, -0.2677,  0.4817,  0.4258, -0.5225, -0.5969,  0.7061,\n",
            "          0.2694,  0.1328, -0.0571, -0.0291, -0.4394, -0.1201,  0.3847, -0.1736,\n",
            "          0.6422,  0.0745, -0.4439,  0.0641,  0.4716,  0.4359, -0.4535, -0.4107,\n",
            "         -0.4115, -0.0975,  0.4635, -0.3165,  0.3548,  0.0337,  0.2423, -0.0032,\n",
            "         -0.1982, -0.2542,  0.3074, -0.3575,  0.4255,  0.3565,  0.0546,  0.0283,\n",
            "         -0.2023, -0.0017,  0.1216, -0.4880,  0.3071, -0.0914, -0.6228,  0.3043,\n",
            "          0.5321, -0.3236,  0.2207,  0.2417,  0.9737, -0.3863,  0.3949,  0.2440,\n",
            "          0.2100, -0.2995, -0.9710, -0.2778,  0.1385, -0.3236,  0.0595, -0.5694,\n",
            "         -0.2653, -0.0097,  0.0731,  0.7189, -0.3923, -0.2227,  0.0748, -0.4004,\n",
            "         -0.4231,  0.3996, -0.1375, -0.8513,  0.2032,  0.3200, -0.6144, -0.1325,\n",
            "          0.0322,  0.0885,  0.1748,  0.1560, -0.4821, -0.1929, -0.5154, -0.0261,\n",
            "          0.1995, -0.7377,  0.6114,  0.2130, -0.1984,  0.1047,  0.1661, -0.1810,\n",
            "          0.1677,  0.1524, -0.2688,  0.2701,  0.5276,  0.3119, -0.1093, -0.3373],\n",
            "        [-0.1117, -0.2076,  0.4511, -0.3339,  0.1019, -0.1887, -0.7572, -0.5111,\n",
            "          0.1950,  0.5693, -0.1758, -0.0456, -0.2055,  0.0412,  0.5339, -0.8207,\n",
            "         -0.4300,  0.5027, -0.8438, -0.1007, -0.0802,  0.2113, -0.3222, -0.4492,\n",
            "         -0.5054, -0.1253, -0.2536,  0.4856,  0.4288, -0.5371, -0.5932,  0.6978,\n",
            "          0.2712,  0.1360, -0.0643, -0.0153, -0.4472, -0.1096,  0.3924, -0.1821,\n",
            "          0.6492,  0.0816, -0.4439,  0.0555,  0.4683,  0.4328, -0.4495, -0.4131,\n",
            "         -0.4081, -0.0956,  0.4578, -0.3195,  0.3574,  0.0288,  0.2475, -0.0048,\n",
            "         -0.1936, -0.2590,  0.3086, -0.3621,  0.4240,  0.3600,  0.0598,  0.0275,\n",
            "         -0.2075, -0.0031,  0.1230, -0.4871,  0.3121, -0.0980, -0.6335,  0.3110,\n",
            "          0.5344, -0.3223,  0.2157,  0.2402,  0.9799, -0.3903,  0.3887,  0.2503,\n",
            "          0.2149, -0.2874, -0.9721, -0.2901,  0.1453, -0.3319,  0.0612, -0.5582,\n",
            "         -0.2656, -0.0020,  0.0706,  0.7190, -0.4086, -0.2155,  0.0802, -0.4010,\n",
            "         -0.4237,  0.4108, -0.1384, -0.8501,  0.2026,  0.3281, -0.6086, -0.1263,\n",
            "          0.0377,  0.0911,  0.1747,  0.1517, -0.4784, -0.1886, -0.5223, -0.0314,\n",
            "          0.2006, -0.7434,  0.6085,  0.2122, -0.1987,  0.1046,  0.1689, -0.1781,\n",
            "          0.1659,  0.1488, -0.2653,  0.2854,  0.5370,  0.3097, -0.1085, -0.3343],\n",
            "        [-0.0996, -0.2559,  0.4728, -0.3126,  0.0769, -0.2311, -0.7659, -0.5282,\n",
            "          0.1841,  0.5269, -0.2058, -0.0764, -0.1795,  0.0456,  0.5130, -0.7479,\n",
            "         -0.4304,  0.4679, -0.8828, -0.0949, -0.1249,  0.2024, -0.3560, -0.5046,\n",
            "         -0.4875, -0.1741, -0.3037,  0.4124,  0.4017, -0.4988, -0.6416,  0.7080,\n",
            "          0.3027,  0.1346, -0.0247, -0.0563, -0.4847, -0.1555,  0.4488, -0.1420,\n",
            "          0.5894,  0.0241, -0.4413,  0.0858,  0.4590,  0.5139, -0.4808, -0.4579,\n",
            "         -0.4548, -0.0743,  0.5233, -0.2905,  0.3578,  0.0960,  0.2815, -0.0039,\n",
            "         -0.2358, -0.1894,  0.2908, -0.3752,  0.4565,  0.3596,  0.0949,  0.0532,\n",
            "         -0.2379,  0.0185,  0.0780, -0.5021,  0.3095, -0.0342, -0.5567,  0.3645,\n",
            "          0.5737, -0.4017,  0.2606,  0.2251,  0.9737, -0.3787,  0.4417,  0.1830,\n",
            "          0.2418, -0.3240, -0.9985, -0.2006,  0.1011, -0.2472,  0.0276, -0.5537,\n",
            "         -0.2488, -0.0379,  0.0662,  0.7089, -0.3241, -0.2707,  0.0406, -0.4232,\n",
            "         -0.4930,  0.3656, -0.1498, -0.8635,  0.2578,  0.2913, -0.6622, -0.1238,\n",
            "          0.0871,  0.0990,  0.1444,  0.2231, -0.5067, -0.1799, -0.4860, -0.0312,\n",
            "          0.2047, -0.6900,  0.6095,  0.2067, -0.2137,  0.0878,  0.1087, -0.2374,\n",
            "          0.1988,  0.1968, -0.2884,  0.2861,  0.5005,  0.4007, -0.1022, -0.3946],\n",
            "        [-0.1092, -0.2540,  0.4643, -0.3146,  0.0868, -0.2544, -0.7724, -0.5314,\n",
            "          0.1871,  0.5286, -0.2113, -0.0818, -0.1813,  0.0479,  0.5169, -0.7465,\n",
            "         -0.4500,  0.4645, -0.8952, -0.0920, -0.1246,  0.2366, -0.3461, -0.5032,\n",
            "         -0.4967, -0.1862, -0.2901,  0.4039,  0.4071, -0.5188, -0.6340,  0.6876,\n",
            "          0.3134,  0.1387, -0.0212, -0.0372, -0.4959, -0.1460,  0.4551, -0.1480,\n",
            "          0.6060,  0.0260, -0.4442,  0.0754,  0.4414,  0.5217, -0.4723, -0.4892,\n",
            "         -0.4523, -0.0714,  0.5183, -0.2839,  0.3514,  0.0889,  0.2931, -0.0018,\n",
            "         -0.2321, -0.1734,  0.2806, -0.3775,  0.4487,  0.3649,  0.1047,  0.0503,\n",
            "         -0.2561,  0.0209,  0.0860, -0.5021,  0.3000, -0.0383, -0.5593,  0.3767,\n",
            "          0.5702, -0.4251,  0.2628,  0.2113,  0.9713, -0.3785,  0.4285,  0.1869,\n",
            "          0.2509, -0.3122, -0.9936, -0.2121,  0.0899, -0.2544,  0.0352, -0.5315,\n",
            "         -0.2372, -0.0243,  0.0604,  0.7027, -0.3413, -0.2627,  0.0302, -0.4321,\n",
            "         -0.5002,  0.3749, -0.1469, -0.8722,  0.2650,  0.2932, -0.6664, -0.1237,\n",
            "          0.0885,  0.1072,  0.1429,  0.2254, -0.5107, -0.1720, -0.4930, -0.0450,\n",
            "          0.2222, -0.6980,  0.5864,  0.2122, -0.2071,  0.0714,  0.1071, -0.2510,\n",
            "          0.2012,  0.1969, -0.2851,  0.3013,  0.5164,  0.4009, -0.0979, -0.3968]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOhEdbdsZXi2",
        "outputId": "5871e538-7156-425f-e6b3-b25afc68b621"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            ">>> \n",
            "KeyboardInterrupt\n",
            ">>> ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd matformer/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PksQB63tjxJH",
        "outputId": "a9c50198-da77-40d4-d25e-47e547839062"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'matformer/'\n",
            "/content/DPF/matformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sh pretrain.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsKhoFesjpam",
        "outputId": "bceec231-2568-4320-9cbe-f1543f58f729",
        "collapsed": true
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.0+cu118\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `0`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.0+cu118\n",
            "name='matformer' conv_layers=5 edge_layers=0 atom_input_features=92 edge_features=128 triplet_input_features=40 node_features=128 fc_layers=1 fc_features=128 output_features=1 node_layer_head=4 edge_layer_head=4 nn_based=False link='identity' zero_inflated=False use_angle=False angle_lattice=False classification=False pre_train=False position_noise=None lattice_noise=None mask_ratio=None\n",
            "{'dataset': 'user_data', 'target': 'target', 'epochs': 50, 'batch_size': 256, 'weight_decay': 1e-05, 'learning_rate': 0.001, 'criterion': 'mse', 'optimizer': 'adamw', 'scheduler': 'onecycle', 'save_dataloader': False, 'pin_memory': False, 'write_predictions': True, 'num_workers': 16, 'classification_threshold': None, 'atom_features': 'atomic_number', 'pre_train': True, 'pyg_input': True, 'use_lattice': True, 'use_angle': False, 'output_dir': '/mnt/nas/share2/home/liuke/prj/log/pre_train_denoise_200/m0.3_l0.3_p0.3/', 'model': {'use_angle': False, 'name': 'matformer', 'pre_train': True, 'position_noise': 0.3, 'lattice_noise': 0.3, 'mask_ratio': 0.3, 'output_features': 119}}\n",
            "*******train_for_folder***********\n",
            "#############get data##################\n",
            "INFO: Pandarallel will run on 32 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DPF/matformer/pretrain_accelerator_reconstruct.py\", line 461, in <module>\n",
            "    train_for_folder(\n",
            "  File \"/content/DPF/matformer/pretrain_accelerator_reconstruct.py\", line 118, in train_for_folder\n",
            "    dataset, n_outputs = get_data(root_dir, file_format='poscar', debug=debug)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/DPF/matformer/acc_util.py\", line 29, in get_data\n",
            "    df = pd.read_csv(id_prop_dat, header=None)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\", line 873, in get_handle\n",
            "    handle = open(\n",
            "             ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/mnt/nas/share2/home/liuke/data/llm_pre_train_poscar/id_prop.csv'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1198, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 785, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'pretrain_accelerator_reconstruct.py', '--position_noise', '0.3', '--lattice_noise', '0.3', '--mask_ratio', '0.3', '--root_dir', 'PATH/TO/YOUR/DATA', '--config_name', 'PATH/TO/YOUR/CONFIG']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jarvis.db.figshare import data as load_jarvis_data\n",
        "dataset=load_jarvis_data(\"dft_3d\")\n",
        "print(f\"Loaded {len(dataset)} entries.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37bgdjYkqofa",
        "outputId": "2d9140b1-2903-4575-8d41-43b4867e2300"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining 3D dataset 55k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Loading the zipfile...\n",
            "Loading completed.\n",
            "Loaded 55723 entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jarvis.db.figshare import data as load_jarvis_data\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_jarvis_data(\"dft_3d\")\n",
        "print(f\"Loaded {len(dataset)} entries.\")\n",
        "\n",
        "# Extract 'jid' and 'formation_energy_peratom'\n",
        "rows = []\n",
        "for entry in dataset:\n",
        "    rows.append({\n",
        "        'id': entry['jid'],  # 'jid' is the ID\n",
        "        'formation_energy_peratom': entry['formation_energy_peratom']\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"id_prop.csv\", index=False)\n",
        "\n",
        "print(\"id_prop.csv saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV5NwH79rba5",
        "outputId": "c4162120-7a65-4735-a00e-7978e5d558df"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining 3D dataset 55k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Loading the zipfile...\n",
            "Loading completed.\n",
            "Loaded 55723 entries.\n",
            "id_prop.csv saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head id_prop.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edzQOUbptiDU",
        "outputId": "9cd84406-8114-4e98-eb1d-e8a983f03126"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id,formation_energy_peratom\n",
            "JVASP-90856,-0.42762\n",
            "JVASP-86097,-0.41596\n",
            "JVASP-64906,0.04847\n",
            "JVASP-98225,-0.4414\n",
            "JVASP-10,-0.71026\n",
            "JVASP-14014,-0.45468\n",
            "JVASP-64664,0.09202\n",
            "JVASP-22556,-2.07159\n",
            "JVASP-86726,-0.42539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"id_prop.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zanyuwsytTh2",
        "outputId": "215da85d-f70e-40b2-b076-0a8f09ffd424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0fecb810-32a3-4f65-b147-480c7fa0e159\", \"id_prop.csv\", 1163095)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4Xh7DAyBu42F",
        "outputId": "57daf503-9485-482c-b174-8616668fd4de"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/DPF/matformer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jarvis.db.figshare import data as load_jarvis_data\n",
        "from jarvis.core.atoms import Atoms\n",
        "import os\n",
        "\n",
        "def atoms_to_poscar(atoms: Atoms) -> str:\n",
        "    \"\"\"\n",
        "    Convert Jarvis Atoms object to POSCAR string format.\n",
        "    \"\"\"\n",
        "    lattice = atoms.lattice_mat\n",
        "    species = atoms.elements  # list of element symbols like ['Si', 'Si']\n",
        "    unique_species = sorted(set(species), key=species.index)  # unique preserving order\n",
        "    counts = [species.count(s) for s in unique_species]\n",
        "    coords = atoms.cart_coords  # Cartesian coordinates\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"Generated by script\")  # comment line\n",
        "    lines.append(\"1.0\")  # universal scaling factor\n",
        "\n",
        "    # lattice vectors (3 lines)\n",
        "    for v in lattice:\n",
        "        lines.append(f\"{v[0]:.16f} {v[1]:.16f} {v[2]:.16f}\")\n",
        "\n",
        "    # element symbols line\n",
        "    lines.append(\" \".join(unique_species))\n",
        "\n",
        "    # counts line\n",
        "    lines.append(\" \".join(str(c) for c in counts))\n",
        "\n",
        "    # coordinate type\n",
        "    lines.append(\"Cartesian\")\n",
        "\n",
        "    # atomic positions\n",
        "    for el in unique_species:\n",
        "        for i, sp in enumerate(species):\n",
        "            if sp == el:\n",
        "                x, y, z = coords[i]\n",
        "                lines.append(f\"{x:.16f} {y:.16f} {z:.16f}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "output_dir = \"/content/DPF/poscar_files\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "dataset = load_jarvis_data(\"dft_3d\")\n",
        "\n",
        "for entry in dataset:\n",
        "    jid = entry['jid']\n",
        "    atoms_dict = entry['atoms']\n",
        "    atoms = Atoms.from_dict(atoms_dict)\n",
        "\n",
        "    poscar_content = atoms_to_poscar(atoms)\n",
        "    poscar_path = os.path.join(output_dir, f\"{jid}.vasp\")\n",
        "\n",
        "    with open(poscar_path, \"w\") as f:\n",
        "        f.write(poscar_content)\n",
        "\n",
        "print(f\"Saved POSCAR (.vasp) files to {output_dir}\")\n"
      ],
      "metadata": {
        "id": "m_0QMwmWwzP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020dce5d-36fb-4b71-c6f8-d15963f9a0d2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining 3D dataset 55k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Loading the zipfile...\n",
            "Loading completed.\n",
            "Saved POSCAR (.vasp) files to /content/DPF/poscar_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fjyydMeDDVJk",
        "outputId": "32695556-d279-49ff-d782-609a5b2006ca"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/DPF/matformer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DPF/matformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2CZj8hD6V8",
        "outputId": "40250bf7-b8fe-44d4-e9e7-5f565106d1a5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DPF/matformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_HsnrO-zDiZ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python pretrain_accelerator_reconstruct.py --root_dir /content/DPF --output_dir /content/DPF/output --batch_size 64 --epochs 10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX0750X1z8ko",
        "outputId": "67d7b9de-7d17-486a-aad5-bb8898ad6c9f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.0+cu118\n",
            "name='matformer' conv_layers=5 edge_layers=0 atom_input_features=92 edge_features=128 triplet_input_features=40 node_features=128 fc_layers=1 fc_features=128 output_features=1 node_layer_head=4 edge_layer_head=4 nn_based=False link='identity' zero_inflated=False use_angle=False angle_lattice=False classification=False pre_train=False position_noise=None lattice_noise=None mask_ratio=None\n",
            "{'dataset': 'user_data', 'target': 'target', 'epochs': '10', 'batch_size': '64', 'weight_decay': 1e-05, 'learning_rate': 0.001, 'criterion': 'mse', 'optimizer': 'adamw', 'scheduler': 'onecycle', 'save_dataloader': False, 'pin_memory': False, 'write_predictions': True, 'num_workers': 16, 'classification_threshold': None, 'atom_features': 'atomic_number', 'pre_train': True, 'pyg_input': True, 'use_lattice': True, 'use_angle': False, 'output_dir': '/content/DPF/outputmNone_lNone_pNone/', 'model': {'use_angle': False, 'name': 'matformer', 'pre_train': True, 'position_noise': None, 'lattice_noise': None, 'mask_ratio': None, 'output_features': 119}}\n",
            "*******train_for_folder***********\n",
            "#############get data##################\n",
            "INFO: Pandarallel will run on 32 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "                 id  formation_energy_peratom\n",
            "0       JVASP-90856                  -0.42762\n",
            "1       JVASP-86097                  -0.41596\n",
            "2       JVASP-64906                   0.04847\n",
            "3       JVASP-98225                  -0.44140\n",
            "4          JVASP-10                  -0.71026\n",
            "...             ...                       ...\n",
            "55718  JVASP-123206                   0.37366\n",
            "55719  JVASP-122142                  -0.01992\n",
            "55720  JVASP-123207                   0.07529\n",
            "55721  JVASP-121802                  -2.45757\n",
            "55722  JVASP-123937                  -0.08259\n",
            "\n",
            "[55723 rows x 2 columns]\n",
            "#############get data done 67.9047954082489 s##################\n",
            "*******Construct targets***********\n",
            "INFO: Pandarallel will run on 1 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "*******Construct targets done32.18136239051819s***********\n",
            "Data error max() arg is an empty sequence\n",
            "data range 0.0 0.0\n",
            "./saved_data/train_datatest_graph_angle.pkl\n",
            "graphs not saved\n",
            "INFO: Pandarallel will run on 1 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "mean 0.000000 std 0.000000\n",
            "normalize using training mean 0.000000 and std 0.000000\n",
            "pre_train\n",
            "100% 44578/44578 [00:00<00:00, 161441.54it/s]\n",
            "data range 0.0 0.0\n",
            "./saved_data/val_datatest_graph_angle.pkl\n",
            "graphs not saved\n",
            "INFO: Pandarallel will run on 1 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "mean 0.000000 std 0.000000\n",
            "normalize using training mean 0.000000 and std 0.000000\n",
            "pre_train\n",
            "100% 5572/5572 [00:00<00:00, 121702.55it/s]\n",
            "data range 0.0 0.0\n",
            "./saved_data/test_datatest_graph_angle.pkl\n",
            "graphs not saved\n",
            "INFO: Pandarallel will run on 1 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "mean 0.000000 std 0.000000\n",
            "normalize using training mean 0.000000 and std 0.000000\n",
            "pre_train\n",
            "100% 5572/5572 [00:00<00:00, 124371.57it/s]\n",
            "## Dataset constructed ##\n",
            "## Start Broadcast ##\n",
            "## Broadcast Done ##\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "********START TRAINING***********\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DPF/matformer/pretrain_accelerator_reconstruct.py\", line 461, in <module>\n",
            "    train_for_folder(\n",
            "  File \"/content/DPF/matformer/pretrain_accelerator_reconstruct.py\", line 310, in train_for_folder\n",
            "    accelerator.backward(loss)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\", line 2473, in backward\n",
            "    loss.backward(**kwargs)\n",
            "    ^^^^^^^^^^^^^\n",
            "AttributeError: 'float' object has no attribute 'backward'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "s=pd.read_csv(\"id_prop.csv\")\n",
        "s"
      ],
      "metadata": {
        "id": "0R5ILG8_BBT3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "e946102b-5564-4ebc-f37e-30a454098f74"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id  formation_energy_peratom\n",
              "0       JVASP-90856                  -0.42762\n",
              "1       JVASP-86097                  -0.41596\n",
              "2       JVASP-64906                   0.04847\n",
              "3       JVASP-98225                  -0.44140\n",
              "4          JVASP-10                  -0.71026\n",
              "...             ...                       ...\n",
              "55718  JVASP-123206                   0.37366\n",
              "55719  JVASP-122142                  -0.01992\n",
              "55720  JVASP-123207                   0.07529\n",
              "55721  JVASP-121802                  -2.45757\n",
              "55722  JVASP-123937                  -0.08259\n",
              "\n",
              "[55723 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4427a622-edbf-4f86-a73d-43da556176a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>formation_energy_peratom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JVASP-90856</td>\n",
              "      <td>-0.42762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>JVASP-86097</td>\n",
              "      <td>-0.41596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JVASP-64906</td>\n",
              "      <td>0.04847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JVASP-98225</td>\n",
              "      <td>-0.44140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JVASP-10</td>\n",
              "      <td>-0.71026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55718</th>\n",
              "      <td>JVASP-123206</td>\n",
              "      <td>0.37366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55719</th>\n",
              "      <td>JVASP-122142</td>\n",
              "      <td>-0.01992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55720</th>\n",
              "      <td>JVASP-123207</td>\n",
              "      <td>0.07529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55721</th>\n",
              "      <td>JVASP-121802</td>\n",
              "      <td>-2.45757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55722</th>\n",
              "      <td>JVASP-123937</td>\n",
              "      <td>-0.08259</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55723 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4427a622-edbf-4f86-a73d-43da556176a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4427a622-edbf-4f86-a73d-43da556176a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4427a622-edbf-4f86-a73d-43da556176a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-92bcea85-2914-4bf5-921f-1f81114a4e01\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-92bcea85-2914-4bf5-921f-1f81114a4e01')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-92bcea85-2914-4bf5-921f-1f81114a4e01 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f77d7a5b-0b3b-4b85-89d0-cc5e95d89bdc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('s')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f77d7a5b-0b3b-4b85-89d0-cc5e95d89bdc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('s');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "s",
              "summary": "{\n  \"name\": \"s\",\n  \"rows\": 55723,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 55712,\n        \"samples\": [\n          \"JVASP-36044\",\n          \"JVASP-122933\",\n          \"JVASP-39732\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"formation_energy_peratom\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.081827245978554,\n        \"min\": -4.42304,\n        \"max\": 4.98575,\n        \"num_unique_values\": 49967,\n        \"samples\": [\n          -0.72953,\n          -2.18302,\n          -1.9283\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWb4WLh86Z8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}